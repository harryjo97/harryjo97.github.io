<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://harryjo97.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://harryjo97.github.io/" rel="alternate" type="text/html" /><updated>2021-01-18T01:54:08+09:00</updated><id>https://harryjo97.github.io/feed.xml</id><title type="html">Graph ML review</title><subtitle>about Graph ML</subtitle><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><entry><title type="html">Semi-Supervised Classification with Graph Convolutional Networks</title><link href="https://harryjo97.github.io/paper%20review/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/" rel="alternate" type="text/html" title="Semi-Supervised Classification with Graph Convolutional Networks" /><published>2021-01-13T20:00:00+09:00</published><updated>2021-01-13T20:00:00+09:00</updated><id>https://harryjo97.github.io/paper%20review/Semi-Supervised-Classification-with-Graph-Convolutional-Networks</id><content type="html" xml:base="https://harryjo97.github.io/paper%20review/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/">&lt;p&gt;Graph Convolutional Network 이해하기 : paper review&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;논문에서 해결하고자 하는 문제는 다음과 같습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Classifying nodes in a graph where labels are only available for a small subset of nodes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉 그래프의 node 들 중 label 이 주어진 node 들의 수가 적은 상황에서 출발합니다.&lt;/p&gt;

&lt;p&gt;Graph Convolution Network 를 사용하기 이전에는, 주로 explicit graph-based regularization &lt;a href=&quot;https://www.aaai.org/Papers/ICML/2003/ICML03-118.pdf&quot;&gt;(Zhu et al., 2003)&lt;/a&gt; 을 이용하여 문제에 접근하였습니다. 이 방법은 supervised loss \(\mathcal{L}_0\) 에 graph Laplacian regularization 항 \(\mathcal{L}_{reg}\) 을 더한 loss function 을 학습에 사용합니다. Neural network 와 같이 differentiable 함수 \(f\) 와 feature vector matrix  \(X\), 그리고 unnormalized graph Laplacian \(L\) 로 Laplacian regularization 을 다음과 같이 정의합니다.&lt;/p&gt;

\[\mathcal{L} = \mathcal{L}_0 + \lambda\mathcal{L}_{reg},\;\;\; \mathcal{L}_{reg} = f(X)^T\,L\,f(X)
\tag{1}\]

&lt;p&gt;\(\mathcal{L}_{reg}\) 를 자세히 들여다보면, 그래프의 adjacency matrix \(A\) 에 대해 다음을 만족합니다.&lt;/p&gt;

\[f(X)^T\;L\;f(X) = \sum_{i,j} A_{ij} \|f(X_i)-f(X_j)\|^2\]

&lt;p&gt;\(\mathcal{L}_{reg}\) 의 값이 작다는 것은 곧 인접한 두 node 의 feature 가 비슷하다는 뜻입니다. 이와 같이 explicit graph-based regularization 은 그래프의 인접한 node 들은 비슷한 feature 를 가질 것이라는 가정을 전제로 하기 때문에, 일반적인 상황에서 제약을 받습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 Explicit graph-based regularization 을 사용하지 않기 위해 그래프의 구조를 포함하는 neural network model \(f(X,A)\) 를 제시합니다.  [1] 에서 제시된 spectral convolution 과 [3] 의 truncated Chebyshev expansion 을 사용한 ChebyNet 을 발전시킨 Graph Convolutional Network (GCN) 을 통해 semi-supervised node classification 을 해결합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;fast-approximate-convolutions-on-graphs&quot;&gt;Fast Approximate Convolutions On Graphs&lt;/h2&gt;

&lt;h3 id=&quot;spectral-graph-convolution&quot;&gt;Spectral Graph Convolution&lt;/h3&gt;

&lt;p&gt;Graph signal \(x\in\mathbb{R}^N\) 와 filter \(g_{\theta}=\text{diag}(\theta)\) 에 대해 spectral convolution 은 다음과 같이 정의됩니다 [1, 2].&lt;/p&gt;

\[g_{\theta}\ast x = Ug_{\theta}U^Tx
\tag{2}\]

&lt;p&gt;여기서 \(U\)  는 normalized graph Laplacian \(L = I - D^{-1/2}AD^{-1/2}\) 의 eigenvector 로 이루어진 Fourier basis 이고, \(L=U\Lambda U^T\) 로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Filter \(g_{\theta}\) 는 다음과 같이 \(L\) 의 eigenvalue 들의 함수로 생각할 수 있습니다 [3].&lt;/p&gt;

\[g_{\theta}(\Lambda) =
\begin{bmatrix}
g_{\theta}(\lambda_0) &amp;amp; &amp;amp; &amp;amp; \\
 &amp;amp; g_{\theta}(\lambda_1) &amp;amp; &amp;amp; \\
  &amp;amp; &amp;amp; \ddots &amp;amp; \\
  &amp;amp; &amp;amp; &amp;amp; g_{\theta}(\lambda_{N-1})
\end{bmatrix}\]

&lt;p&gt;\((2)\) 을 계산하기 위해서는 \(U\) 의 matrix multiplication 을 수행해야하며, 이는 \(O(N^2)\) 으로 상당히 복잡한 연산입니다. 또한 \(U\) 를 구하기 위한 eigendecomposition 은 복잡도가 \(O(N^3)\) 이므로, node 의 개수가 수천 수만개인 그래프에 대해서 \((2)\) 를 계산하는 것은 굉장히 힘듭니다.&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해, truncated Chebyshev expansion 을 통해 \(g_{\theta}(\Lambda)\) 를 다음과 같이 근사합니다 [3, 5].&lt;/p&gt;

\[g_{\theta'}(\Lambda) \approx \sum^K_{k=0} \theta'_{k}T_k(\tilde{\Lambda})
\tag{3}\]

&lt;p&gt;여기서 \(\tilde{\Lambda} = \frac{2}{\lambda_{max}}\Lambda - I\) 로 정의하고, \(L\) 의 가장 큰 eigenvalue \(\lambda_{max}\) 를 사용해 Chebyshev expansion 을 위해 \(\Lambda\) 를  scaling 해준 것입니다.&lt;/p&gt;

&lt;p&gt;\((3)\) 의 근사를 \((2)\) 에 대입하면, \(\tilde{L} = \frac{2}{\lambda_{max}}L - I\) 에 대해 다음의 결과를 얻을 수 있습니다.&lt;/p&gt;

\[g_{\theta'}\ast x \approx \sum^K_{k=0} \theta'_kT_k(\tilde{L})x = y
\tag{4}\]

&lt;p&gt;\((4)\) 의 결과가 특별한 이유는 각 node 에 대해 localized 되어 있기 때문입니다. 우선 graph Laplacian \(L\) 은 다음과 같이 localization 특성을 가집니다 [5].&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;\(\left(L^s\right)_{ij}\) 는 그래프의 두 node \(i\) 와 \(j\) 를 연결하는 path 들 중 길이가 \(s\) 이하인 path 들의 개수와 일치한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;\((4)\) 에서 \(L\) 의 \(K\)-th power 까지만 존재하기 때문에, \(y(i)\) 는 \(i\) 의 \(K\)-th order neighborhood signal 들의 합으로 표현할 수 있습니다. 따라서 \((4)\) 의 근사는 \(K\)-localized 됨을 확인할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;layer-wise-linear-model&quot;&gt;Layer-wise Linear Model&lt;/h3&gt;

&lt;p&gt;\((4)\) 에서 \(K\) 가 클수록 더 많은 종류의 convolutional filter 를 얻을 수 있지만, 그만큼 계산이 복잡해지며 overfitting 의 가능성도 커집니다. 여러개의 convolutional layer 를 쌓아 deep model 을 만든다면, \(K\) 가 작아도 다양한 종류의 convolutional filter 를 표현할 수 있습니다. 특히 overfitting 의 가능성을 덜 수 있고, 한정된 자원에 대해서 $K$ 가 클 때보다 더 깊은 모델을 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 극단적으로 \(K=1\) 로 제한을 두었습니다. 또한 normalized graph Laplacian 의 eigenvalue 들은 \([0,2]\) 구간에 속하기 때문에 [6], \(\lambda_{max}\approx 2\) 로 근사합니다.이 경우 \((4)\) 는 다음과 같이 두 개의 parameter \(\theta'_0\) 와 \(\theta'_1\) 을 통해 표현할 수 있습니다.&lt;/p&gt;

\[g_{\theta'}\ast x \approx \theta'_0x + \theta'_1(L-I)x = \theta'_0x - \theta'_1D^{-1/2}AD^{-1/2}x\]

&lt;p&gt;더 나아가, 계산을 줄이기 위해 하나의 parameter \(\theta = \theta'_0 = -\theta'_1\) 만을 사용한다면, 다음과 같은 간단한 결과를 얻게됩니다.&lt;/p&gt;

\[g_{\theta}\ast x \approx \theta(I + D^{-1/2}AD^{-1/2})x
\tag{5}\]

&lt;p&gt;\(M = I + D^{-1/2}AD^{-1/2}\) 의 eigenvalue 는 \([0,2]\) 에 속합니다 [Appendix A]. 그렇기 때문에, \((5)\) 를 사용한 layer 를 여러개 쌓아 deep model 을 만든다면 exploding / vanishing gradient problem 과 같이 불안정한 학습이 이루어질 수 있습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 이를 해결하기 위해 renormalization trick 을 사용합니다. \(\tilde{A} = A + I\) 와 \(\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}\)  에 대해, \((5)\) 에서 \(I + D^{-1/2}AD^{-1/2}\) 대신 \(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\) 를 이용해 다음과 같이 convolutional filter 를 정의합니다 [Appendix B].&lt;/p&gt;

\[g_{\theta}\ast x \approx \theta\, \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2} x
\tag{6}\]

&lt;p&gt;\((6)\) 의 결과는 각 node 가 1차원의 feature 를 가질 때로 한정되어 있습니다. 이제 각 node 마다 \(C\) 차원의 feature vector 를 가지는 상황을 고려하겠습니다. 주어진 signal \(X\in\mathbb{R}^{N\times C}\) 와 \(F\) 개의 feature map 에 대해서 \((6)\) 을 다음과 같이 일반화할 수 있습니다 [Appendix C].&lt;/p&gt;

\[Z = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}X\Theta
\tag{7}\]

&lt;p&gt;여기서 \(\Theta\in\mathbb{R}^{C\times F}\) 는 filter의 parameter matrix 이고 \(Z\in\mathbb{R}^{N\times F}\) 가 filtering 의 결과입니다. 특히 \(\Theta\) 는 그래프의 모든 node 들에 대해 동일하게 사용되기 때문에, CNN 의 filter 와 같이 weight-sharing 의 관점에서 큰 의미가 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;\((7)\) 을 사용해 muli-layer GCN 의 layer-wise propagation rule 을 정의할 수 있습니다. \(l\) 번째 layer 와 \(l+1\) 번째 layer 의 activation 을 다음과 같이 쓰면,&lt;/p&gt;

\[H^{(l)}\in\mathbb{R}^{N\times C_l}\, , \;\; H^{(l+1)}\in\mathbb{R}^{N\times C_{l+1}}\]

&lt;p&gt;trainable weight matrix \(W^{(l)}\in\mathbb{R}^{C_l\times C_{l+1}}\) 와 activation function \(\sigma\) (e.g. ReLU, tanh) 를 사용해 다음과 같이 propagation rule 을 정의할 수 있습니다.&lt;/p&gt;

\[H^{(l+1)} = \sigma\left( \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\,H^{(l)}W^{(l)} \right)
\tag{8}\]

&lt;p&gt;\((8)\) 에서 혼동하지 말아야 점은, 각 layer 들에 대해 그래프의 구조 (node 들과 node 들의 연결 상태) 는 변하지 않고, 각 node 에 주어진 feature vector 의 dimension \(C_l\) 만 변한다는 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/gcn.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;semi-supervised-node-classification&quot;&gt;Semi-Supervised Node Classification&lt;/h2&gt;

&lt;h3 id=&quot;example--two-layer-gcn&quot;&gt;Example : Two-layer GCN&lt;/h3&gt;

&lt;p&gt;\((8)\) 의 propagation rule 을 사용해 node classification 을 위한 two-layer GCN 을 보겠습니다.. 먼저 전처리 단계에서 \(\hat{A} = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\) 를 계산하여 다음과 같이 두 개의 layer 를 가지는 model 을 만들 수 있습니다.&lt;/p&gt;

\[Z = \text{softmax}\left( \hat{A}\;\text{ReLU}\left( \hat{A}XW^{(0)} \right)W^{(1)} \right)
\tag{9}\]

&lt;p&gt;마지막 output layer 에서 activation function 으로 softmax 를 각 행 별로 적용해줍니다. 
Loss function 으로 label 이 있는 node 들에 대해서만 cross-entropy error 를 계산합니다.&lt;/p&gt;

\[\mathcal{L} = -\sum_{l\in\text{labled}}\sum^{\text{output dim}}_{f=1} Y_{lf}\ln Z_{lf}\]

&lt;p&gt;이를 통해 \((9)\) 의 weight matrix \(W^{(0)}\) 와 \(W^{(1)}\) 은 gradient descent 를 통해 업데이트 합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;experiments--results&quot;&gt;Experiments &amp;amp; Results&lt;/h2&gt;

&lt;p&gt;실험 방법 및 데이터에 관해서 더 자세한 설명은 &lt;a href=&quot;https://arxiv.org/pdf/1603.08861.pdf&quot;&gt;Yang et al., 2016&lt;/a&gt; 을 참고하기 바랍니다.&lt;/p&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;논문에서는 크게 네 가지 dataset : Citeseer, Cora, Pubmed, NELL 을 실험에 사용했습니다.&lt;/p&gt;

&lt;p&gt;이들 중 Citeseer, Cora, 그리고 Pubmed  는 citation network dataset 으로, 각 node 는 문서들이며 edge 는 citation link 를 의미합니다.  NELL 은 knowledge graph 에서 추출된 이분 그래프 dataset 으로 relation node 와 entity node 모두 사용했습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/dataset.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;h3 id=&quot;node-classification&quot;&gt;Node Classification&lt;/h3&gt;

&lt;p&gt;각 데이터셋에 대한 baseline method 들과 two-layer GCN 의 classification accuracy 는 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/result1.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;GCN 의 정확도가 다른 baseline method 들에 비해 월등히 높은 것을 볼 수 있습니다. 특히 baseline method 들 중 정확도가 가장 높은  Planetoid 와 비교해, GCN 의 수렴 속도가 훨씬 빠르다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;evaluation-of-propagation-model&quot;&gt;Evaluation of Propagation Model&lt;/h3&gt;

&lt;p&gt;위에서 제시된 다양한 propagation model 들의 performance 를 비교한 결과는 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/result2.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;\((7)\) 에서 사용한 renormalization trick 이 가장 높은 정확도를 보여줍니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&quot;a--largesest-eigenvalue-of-m&quot;&gt;A.  Largesest Eigenvalue of \(M\)&lt;/h3&gt;

&lt;p&gt;\(M = I + D^{-1/2}AD^{-1/2}\) 가 real symmetric matrix 이기 때문에, Courant-Fischer 정리에 의해 \(M\) 의 가장 큰 eigenvalue \(\mu\) 는 다음을 만족합니다.&lt;/p&gt;

\[\mu = \sup_{\|x\|=1} x^TMx\]

&lt;p&gt;\(L\) 의 정의에 의해 \(M = 2I-L\) 이며 \(L\) 은 positive semi-definite matrix 이기 때문에, \(\|x\|=1\) 를 만족하는 \(x\in\mathbb{R}^N\) 에 대해 다음이 성립합니다.&lt;/p&gt;

\[x^TMx 
=x^T(2I-L)x
= 2 - x^TLx \leq 2\]

&lt;p&gt;따라서,&lt;/p&gt;

\[\mu = \sup_{\|x\|=1} x^TMx \leq 2\]

&lt;h3 id=&quot;b-about-renormalization-trick&quot;&gt;B. About Renormalization Trick&lt;/h3&gt;

&lt;p&gt;\(I + D^{-1/2}AD^{-1/2}\) 와 \(\tilde{D}^{-1/2}\tilde{A}\,\tilde{D}^{-1/2}\) 의 matrix 를 자세히 살펴보면 다음과 같습니다.&lt;/p&gt;

\[I + D^{-1/2}AD^{-1/2} = \begin{cases}
1 &amp;amp; i=j \\
A_{ij}/\sqrt{D_{ii}D_{jj}} &amp;amp; i\neq j
\end{cases}\]

\[\tilde{D}^{-1/2}\tilde{A}\,\tilde{D}^{-1/2} = \begin{cases}
1/(D_{ii}+1) &amp;amp; i=j \\
A_{ij}/\sqrt{(D_{ii}+1)(D_{jj}+1)} &amp;amp; i\neq j
\end{cases}\]

&lt;h3 id=&quot;c-generalization-to-high-dimensional-feature-vectors&quot;&gt;C. Generalization to high dimensional feature vectors&lt;/h3&gt;

&lt;p&gt;먼저 filter 의 개수가 1개일 때를 생각하겠습니다. 각 node 가 \(C\) 차원의 feature vector 를 가질 때, 이를 signal \(X\in\mathbb{R}^{N\times C}\) 로 표현할 수 있습니다.&lt;/p&gt;

\[X = \begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
x_1 &amp;amp; \cdots &amp;amp; x_C \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix}\]

&lt;p&gt;\(X\) 의 각 column 은 특정 feature 에 대한 signal \(x_{i}\in\mathbb{R}^N\) 입니다. 각 feature 마다 convolutional filter \((6)\) 을 적용해 새로운 feature \(Z\in\mathbb{R}^N\) 를 얻어내는 과정을 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
Z 
&amp;amp;= \sum^{C}_{i=1} \hat{A}x_i\theta_i\\
&amp;amp;= \begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
\hat{A}x_1 &amp;amp; \cdots &amp;amp; \hat{A}x_C \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix}
\begin{bmatrix}
\theta_1 \\
\vdots \\
\theta_C
\end{bmatrix} \\
\\
&amp;amp;= \hat{A}\;
\begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
x_1 &amp;amp; \cdots &amp;amp;x_C \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix}
\begin{bmatrix}
\theta_1 \\
\vdots \\
\theta_C
\end{bmatrix}
= \hat{A}X\Theta
\end{align}\]

&lt;p&gt;이제 Filter 의 개수가 \(F\) 개라면, \(i\) 번째 filter 로 만들어진 새로운 feature \(Z_i = \hat{A}X\Theta_i\) 들에 대해 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\begin{align}
Z 
&amp;amp;= \begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
Z_1 &amp;amp; \cdots &amp;amp; Z_F \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix} 
= \begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
\hat{A}X\Theta_1 &amp;amp; \cdots &amp;amp; \hat{A}X\Theta_F \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix} \\
\\
&amp;amp;= \hat{A}X\begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
\Theta_1 &amp;amp; \cdots &amp;amp;\Theta_F \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix}
= \hat{A}X\Theta
\end{align}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. &lt;a href=&quot;https://arxiv.org/pdf/1312.6203.pdf&quot;&gt;Spectral networks and locally
connected networks on graphs&lt;/a&gt;. In International Conference on Learning Representations (ICLR),
2014.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M. Henaff, J. Bruna, and Y. LeCun. &lt;a href=&quot;https://arxiv.org/pdf/1506.05163.pdf&quot;&gt;Deep Convolutional Networks on Graph-Structured Data&lt;/a&gt;.
arXiv:1506.05163, 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. &lt;a href=&quot;https://arxiv.org/pdf/1606.09375.pdf&quot;&gt;Convolutional neural networks on
graphs with fast localized spectral filtering&lt;/a&gt;. In Advances in neural information processing systems
(NIPS), 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thomas N. Kipf and Max Welling. &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;Semi-supervised classification with graph convolutional networks&lt;/a&gt;.
In International Conference on Learning Representations (ICLR), 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;F. R. K. Chung. Spectral Graph Theory, volume 92. American Mathematical Society, 1997.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;paper review&quot;]" /><category term="gcn" /><summary type="html">Graph Convolutional Network 이해하기 : paper review</summary></entry><entry><title type="html">Weisfeiler-Lehman Algorithm</title><link href="https://harryjo97.github.io/theory/Weisfeiler-Lehman-Algorithm/" rel="alternate" type="text/html" title="Weisfeiler-Lehman Algorithm" /><published>2021-01-12T19:00:00+09:00</published><updated>2021-01-12T19:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Weisfeiler-Lehman-Algorithm</id><content type="html" xml:base="https://harryjo97.github.io/theory/Weisfeiler-Lehman-Algorithm/">&lt;p&gt;Weisfeiler-Lehman Algorithm&lt;/p&gt;

&lt;h2 id=&quot;graph-isomorphism&quot;&gt;Graph Isomorphism&lt;/h2&gt;

&lt;p&gt;주어진 두 그래프 \(G = (V_{G},E_{G})\) 와 \(H=(V_{H}, E_{H})\) 에 대해, 두 그래프가 isomorphic 하다는 것은 다음을 만족하는 bijection \(f:V_{G}\rightarrow V_{H}\) 가 존재한다는 뜻입니다.&lt;/p&gt;

\[u, v \text{ are adjacent in }G \iff f(u), f(v) \text{ are adjacent in }H\]

&lt;p&gt;즉 \(G\) 에서 edge 로 이웃한 모든 node 들의 쌍에 대해, \(H\) 에서 대응되는 각 node 들의 쌍 또한 edge 로 이웃해 있을 때 isomorphic 하다고 표현합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/isomorphism.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;
&lt;p&gt;위의 그림에서 보면, 각 그래프에서 같은 숫자를 가진 node 들끼리 대응 되기 때문에, 두 그래프는 isomorphic 합니다.&lt;/p&gt;

&lt;h2 id=&quot;weisfeiler-lehman-algorithm&quot;&gt;Weisfeiler-Lehman Algorithm&lt;/h2&gt;

&lt;p&gt;주어진 두 그래프가 isomorphic 한지를 확인하는 방법으로 Weisfeiler-Lehman algorithm 이 있습니다. 보통 줄여서 WL 알고리즘 혹은 WL test 라고 부릅니다.&lt;/p&gt;

&lt;p&gt;1차원의 WL 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/algorithm.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;
&lt;p&gt;1 차원 WL 알고리즘을 통해 regular graph 를 제외한 대부분의 그래프에 대한 node embedding 이 가능합니다.&lt;/p&gt;

&lt;p&gt;주의할 점은 WL 알고리즘의 결과가 다르다면 두 그래프는 확실히 isomorphic 하지 않지만, 결과가 같다고 해서 두 그래프가 isomorphic 하다고는 결론 지을 수 없습니다. Isomorphic 하지 않은 두 그래프의 WL 알고리즘의 결과는 같을 수 있기 때문에, Graph Isomorphism 에 대한 완벽한 해결법이라고는 할 수 없습니다. WL  알고리즘의 반례로는 Reference [3] 을 참고하기 바랍니다.&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;다음의 두 그래프에 대해 WL 알고리즘을 적용해보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-0.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;주어진 두 그래프에 대해 initial node coloring  \(h^{(0)}_{i}=1\) 을 주겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-1.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;각 node 에 대해 이웃한 node 들의 coloring 정보를 모읍니다. 다음과 같이 multi-set 으로 표시하겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-2.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;이 예시에서는 편의상 hash 함수로 identity 함수를 사용하겠습니다. 
다음과 같이 1 번째 iteration 의 coloring \(h^{(1)}_{i}\) 를 계산할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-3.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;다시 각 node 에 대해 이웃한 node 들의 coloring 정보를 모은 후,&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-4.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;2 번째 iteration 의 coloring \(h^{(2)}_i\) 를 계산해 줍니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-5.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;위의 과정을 반복해 3 번째 iteration 의 coloring \(h^{(3)}_i\) 를 계산해 줍니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-6.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-7.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;3 번째 iteration 의 coloring 으로 인한 node 들의 분할이 2 번째 iteration 의 분할과 동일하므로, 알고리즘을 끝냅니다. 마지막 그림에서 보다시피, 두 그래프에 대해 WL 알고리즘을 통한 node 들의 분할이 일치합니다. 두 그래프는 실제로 isomorphic 하지만, WL 알고리즘의 결과만으로는 판별할 수 없습니다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Brendan L. Douglas. &lt;a href=&quot;https://arxiv.org/pdf/1101.5211.pdf&quot;&gt;The Weisfeiler-Lehman method and graph isomorphism testing&lt;/a&gt;. arXiv preprint
arXiv:1101.5211, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David Bieber. &lt;a href=&quot;https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/&quot;&gt;The Weisfeiler-Lehman Isomorphism Test&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;J. Cai, M. Furer, and N. Immerman. An optimal lower bound on the number of variables for graph identification. Combinatorica, 12(4):389–410, 1992.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="WL algorithm" /><category term="graph isomorphism" /><summary type="html">Weisfeiler-Lehman Algorithm</summary></entry><entry><title type="html">Polynomial Approximation Using Chebyshev Expansion</title><link href="https://harryjo97.github.io/theory/Polynomial-Approximation-of-Spectral-Filtering/" rel="alternate" type="text/html" title="Polynomial Approximation Using Chebyshev Expansion" /><published>2021-01-04T22:00:00+09:00</published><updated>2021-01-04T22:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Polynomial-Approximation-of-Spectral-Filtering</id><content type="html" xml:base="https://harryjo97.github.io/theory/Polynomial-Approximation-of-Spectral-Filtering/">&lt;p&gt;Graph Convolutional Network 이해하기 : (5) Polynomial approximation using Chebyshev expansion&lt;/p&gt;

&lt;h2 id=&quot;chebyshev-polynomial&quot;&gt;Chebyshev Polynomial&lt;/h2&gt;

&lt;p&gt;Chebyshev polynomial 은 다음과 같이 점화식[^1]으로 정의됩니다.&lt;/p&gt;

\[T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)
\;\; \text{ with } \;\; T_0(x) = 1 ,\; T_1(x) = x
\tag{a}\]

&lt;p&gt;특히 Chebyshev polynomial 은 \(L^2\left( [-1,1],\, \frac{dx}{\sqrt{1-x^2}} \right)\)[^2] 의 orthogonal basis 를 이루기 때문에 \(h\in L^2\left( [-1,1],\, \frac{dx}{\sqrt{1-x^2}} \right)\) 에 대해 uniformly convergent 한 Chebyshev expansion 이 존재합니다.&lt;/p&gt;

\[h(x) = \frac{1}{2}c_0 + \sum^{\infty}_{k=1} c_kT_k(x)
\tag{b}\]

&lt;p&gt;\((b)\) 에서 Chebyshev coefficeint \(c_k\) 는 다음과 같이 계산할 수 있습니다.&lt;/p&gt;

\[c_k = \frac{2}{\pi}\int^1_{-1} \frac{T_k(x)h(x)}{\sqrt{1-x^2}}dx\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;truncated-chebyshev-expansion&quot;&gt;Truncated Chebyshev Expansion&lt;/h2&gt;

&lt;p&gt;\(g_{\theta}\) 에 대한 spectral filtering 의 결과 \(f_{out}\) 은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}

f_{out} 
= Ug_{\theta}(\Lambda)U^T\;f_{in} \tag{$1$}

\end{align}\]

&lt;p&gt;\((1)\) 을 계산하기 위해서는 Fourier basis \(U\) 필요하기 때문에, graph Laplacian \(L\) 의 eigenvector 를 모두 찾아야 합니다. \(N\) 개의 node 를 가지는 그래프에 대해서, QR decomposition 과 같은 eigenvalue decomposition 의 computational complexity 의 \(O(N^3)\) 입니다. 즉 node 가 수천개 혹은 수만개 이상인 그래프에 대해서는 직접 \((1)\) 을 계산하는 것은 굉장히 어렵습니다.&lt;/p&gt;

&lt;p&gt;따라서, 그래프의 크기가 큰 경우에는 \((1)\) 을 근사할 수 있는 효율적인 방법이 필요합니다.&lt;/p&gt;

&lt;p&gt;만약 \(g_{\theta}\) 가 order \(K\) polynomial 이라면, \(g_{\theta}(x) = \sum^K_{k=0} a_k x^k\) 에 대해 \((1)\) 을 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

\[\begin{align}
f_{out}
&amp;amp;= U\left(\sum^{N-1}_{k=0} a_k\Lambda^k \right)U^T f_{in} \\
&amp;amp;= \sum^{K}_{k=0} a_k\left( U\Lambda U^T \right)^k f_{in} 
= g_{\theta}(L)f_{in}
\tag{2}
\end{align}\]

&lt;p&gt;\((2)\) 에서 볼 수 있듯이, Fourier basis \(U\)  없이도 \((1)\) 의 결과를 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;\((1)\) 을 효율적으로 근사할 수 있는 \(g_{\theta}\) 의 polynomial approximant \(p\) 를 찾는 것입니다.&lt;/p&gt;

&lt;p&gt;만약 \(p\) 가 \(L\) 의 spectrum 에 대한 upper bound \(\lambda_{max}\)&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 에 대해 다음의 조건을 만족한다면,&lt;/p&gt;

\[\left\vert g_{\theta}(x) - p(x) \right\vert \leq B &amp;lt; \infty
\;\;\text{ for all }\;\; x\in [0,\lambda_{max}]
\tag{6}\]

&lt;p&gt;polynomial filter \(p(L)\) 의 결과 \(\tilde{f}_{out} = p(L)f_{in}\) 을 통해 다음과 같이 \(f_{out}\) 을 근사할 수 있습니다 [3].&lt;/p&gt;

\[\begin{align}
\vert f_{out}(i) - \tilde{f}_{out}(i) \vert 
&amp;amp;= \left\vert \sum_{l} g_{\theta}(\lambda_l)\hat{f}(\lambda_l)u_l(i) - \sum_{l} p(\lambda_l)\hat{f}(\lambda_l)u_l(i) \right\vert \\
\\
&amp;amp;\leq \sum_{l} \vert g_{\theta}(\lambda_l) - p(\lambda_l) \vert \left\vert \hat{f}(\lambda_l)u_l(i) \right\vert \\
&amp;amp;\leq B \left( \sum_l \left\vert \hat{f}(\lambda_l) \right\vert^2\sum_l \vert u_l(i) \vert^2 \right)^{1/2} 
= B\;\|f\| 
\tag{7}
\end{align}\]

&lt;p&gt;이 때 \(f_{out}\) 과 \(\tilde{f}_{out}\) 에 대한 오차 \((7)\) 을 줄이기 위해서는, \(g_{\theta}\) 와 \(p\) 에 대한 \(L_{\infty}\) error \((6)\) 를  최소화해야 합니다. 만약  \(p\) 가 order \(K\) polynomial 이라면, \((6)\) 은 \(p\) 가 minimax polynomial of order \(K\) 일 때 최소가 됩니다. 이 때 truncated Chebyshev expansion 을 통해 minimax polynomial 에 대한 근사가 가능합니다.&lt;/p&gt;

&lt;p&gt;저희가 찾고자 하는  \(p\) 로 \(g_{\theta}\) 의 truncated Chebyshev expansion 을 선택할 수 있습니다. 하지만 \(g_{\theta}\) 는 spectral domain 에서 정의된 함수이기 때문에, \((b)\) 를 적용하기 위해서는 domain 의 변환이 필요합니다. \(L\) 의 eigenvalue 들은 모두 \([0, \lambda_{max}]\) 구간에 속하기 때문에 \(h_{\theta}\) 를 다음과 같이 정의하면 \(g_{\theta}\) 를 \([-1,1]\) 에서 정의된 함수로 바꿀 수 있습니다.&lt;/p&gt;

\[h_{\theta}(x) = g_{\theta}\left( \frac{\lambda_{\max}}{2}(x+1) \right)\]

&lt;p&gt;\((b)\) 를 \(h_{\theta}\) 에 적용하고 order \(K\) 까지의 truncation 을 생각하면,&lt;/p&gt;

\[h_{\theta}(x) \approx \frac{1}{2}c_0 + \sum^{K}_{k=1} c_kT_k(x)\]

&lt;p&gt;\(\tilde{L} = \frac{2}{\lambda_{max}}L - I\) 에 대해 \(p\) 를 다음과 같이 정의하면,&lt;/p&gt;

\[p(\tilde{L}) = \frac{1}{2}c_0I + \sum^{\infty}_{k=1} c_kT_k(\tilde{L})
\tag{8}\]

&lt;p&gt;복잡한 eigenvector 의 계산 없이 spectral filtering 에 대한 근사가 가능합니다.&lt;/p&gt;

\[\begin{align}
Ug_{\theta}(\Lambda)U^T 
&amp;amp;= Uh_{\theta}(\tilde{\Lambda})U^T  \\
&amp;amp;\approx Up(\tilde{\Lambda})U^T = p(\tilde{L})
\end{align}\]

&lt;p&gt;정리하면, spectral filtering 의 결과 \(f_{out}\) 은 다음과 같이 근사할 수 있습니다.&lt;/p&gt;

\[f_{out} = Ug_{\theta}(\Lambda)U^Tf_{in} \approx p(\tilde{L})f_{in}
\tag{9}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;advantage-of-using-chebyshev-expansion&quot;&gt;Advantage of using Chebyshev expansion&lt;/h2&gt;

&lt;p&gt;\((9)\) 와 같이 truncated Chebyshev expansion 을 통한 spectral filtering 의 근사는 다음과 같은 두 가지 이점이 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;fast-filtering-using-recurrence-relation&quot;&gt;Fast filtering using recurrence relation&lt;/h3&gt;

&lt;p&gt;Chebyshev polynomial 의 중요한 특성은 \((a)\) 의 점화식을 통해 얻을 수 있다는 것입니다. Graph Laplacian \(L\) 에서부터 시작해 재귀적 연산으로 order \(K\) polynomial \(T_K\) 까지 구하는 computational cost 는 \(L\) 이 sparse matrix 일 때 \(O(K\vert E\vert)\) 입니다.&lt;/p&gt;

&lt;h3 id=&quot;localized-filter&quot;&gt;Localized Filter&lt;/h3&gt;

&lt;p&gt;\((5)\) 에서 보았듯이 \(g_{\theta}\) 대신 polynomial approximant 를 사용한 filter 는 vertex domain 에서 localized 되어있습니다. 이를 통해 CNN 의 중요한 특성인 locality 가 그래프에서 일반화될 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;lanczos-algorithm&quot;&gt;Lanczos Algorithm&lt;/h2&gt;

&lt;p&gt;\((1)\) 을 효율적으로 계산하는 다른 해결 방법으로는 Lanczos Algorithm 이 있습니다. &lt;a href=&quot;https://arxiv.org/pdf/1901.01484.pdf&quot;&gt;LanczosNet: Multi-Scale Deep Graph Convolutional Networks&lt;/a&gt; 의 paper review 를 통해 더 자세히 설명하겠습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. &lt;a href=&quot;https://arxiv.org/pdf/1606.09375.pdf&quot;&gt;Convolutional neural networks on
graphs with fast localized spectral filtering&lt;/a&gt;. In Advances in neural information processing systems
(NIPS), 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thomas N. Kipf and Max Welling. &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;Semi-supervised classification with graph convolutional networks&lt;/a&gt;.
In International Conference on Learning Representations (ICLR), 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Spectrum 의 upper bound \(\lambda_{max}\) 는 Arnoldi iteration 혹은 Jacobi-Davidson method 등을 사용하면 \(L\) 의 spectrum 전체를 찾는 것에 비해서 훨씬 쉽게 구할 수 있습니다. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="spectral filtering" /><category term="polynomial filter" /><summary type="html">Graph Convolutional Network 이해하기 : (5) Polynomial approximation using Chebyshev expansion</summary></entry><entry><title type="html">Localized Polynomial Filter</title><link href="https://harryjo97.github.io/theory/Localized-Polynomial-Filter/" rel="alternate" type="text/html" title="Localized Polynomial Filter" /><published>2021-01-04T20:00:00+09:00</published><updated>2021-01-04T20:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Localized-Polynomial-Filter</id><content type="html" xml:base="https://harryjo97.github.io/theory/Localized-Polynomial-Filter/">&lt;p&gt;Graph Convolutional Network 이해하기 : (4) Localized Polynomial Filter&lt;/p&gt;

&lt;h2 id=&quot;localization-of-graph-laplacian&quot;&gt;Localization of Graph Laplacian&lt;/h2&gt;

&lt;p&gt;그래프 \(G\) 의 vertex \(i\) 와 \(j\) 에 대해, \(d_G(i,j)\) 를 \(i\) 와 \(j\) 를 연결하는 모든 path 들 중 edge 들의 수가 가장 적은 path 의 길이로 정의합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lemma 1.&lt;/p&gt;

  &lt;p&gt;그래프 \(G\) 의 adjacency matrix \(A\) 에 대해 \(\tilde{A}\) 를 다음과 같이 정의하겠습니다.&lt;/p&gt;

\[\tilde{A} = \begin{cases}
A_{ij} &amp;amp;\mbox{ if } i\neq j \\
1 &amp;amp;\mbox{ if } i=j
\end{cases}\]

  &lt;p&gt;그러면 \(s&amp;gt;0\) 에 대해, \(\left( \tilde{A}^s \right)_{ij}\) 은 vertex \(i\) 와 \(j\) 를 연결하는 path 들 중 길이가  \(s\) 이하인 path 들의 수와 같습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lemma 2.&lt;/p&gt;

  &lt;p&gt;\(N\times N\) matrix \(A\) 와 \(B\) 에 대해 \(B_{mn}=0\) 이면 \(A_{mn}=0\) 을 만족한다고 가정하겠습니다. 그러면 모든 \(s&amp;gt;0\) 에 대해 \(\left( B^s \right)_{mn}=0\) 이면  \(\left( A^s \right)_{mn}=0\) 이 성립합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위의 두 lemma 를 사용하면, 다음의 \(L\) 의 localization 을 보일 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그래프 \(G\) 의 vertex \(i, \;j\) 와 \(d_G(i,j)\)  보다 작은 모든 \(s\) 에 대해 다음이 성립합니다.&lt;/p&gt;

\[\left(L^s\right)_{ij} = 0
\tag{1}\]

&lt;/blockquote&gt;

&lt;p&gt;\(N\times N\) matrix \(B\) 를 다음과 같이 정의하겠습니다.&lt;/p&gt;

\[B_{ij} = \begin{cases}
1 &amp;amp;\mbox{ if } L_{ij}\neq 0 \\
0 &amp;amp;\mbox{ if } L_{ij}= 0
\end{cases}\]

&lt;p&gt;\(L\) 의 정의에 의해 \(B\) 는 그래프 \(G\) 에 대한 adjacency matrix \(A\) 를 변형한 \(\tilde{A}\) 와 같습니다. 그러므로, Lemma 1 에 의해 \(\left( B^s \right)_{ij}=0\) 입니다. Lemma 2 를 사용하면 \(\left( B^s \right)_{ij}=0\) 이므로 \(\left( L^s \right)_{ij}=0\) 임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;localized-polynomial-filter&quot;&gt;Localized Polynomial Filter&lt;/h2&gt;

&lt;p&gt;\(g_{\theta}\) 에 대한 spectral filtering 의 결과 \(f_{out}\) 은 다음과 같습니다.&lt;/p&gt;

\[f_{out} = Ug_{\theta}(\Lambda)U^T\;f_{in} \tag{2}\]

&lt;p&gt;만약 \(g_{\theta}\) 가 order \(K\) polynomial 이라면, \(g_{\theta}(x) = \sum^K_{k=0} a_k x^k\) 를 \((1)\) 에 넣어 정리할 수 있습니다.&lt;/p&gt;

\[\begin{align}
f_{out}
&amp;amp;= U\left(\sum^{N-1}_{k=0} a_k\Lambda^k \right)U^T f_{in} \\
&amp;amp;= \sum^{K}_{k=0} a_k\left( U\Lambda U^T \right)^k f_{in} 
= g_{\theta}(L)f_{in}
\tag{3}
\end{align}\]

&lt;p&gt;\((3)\) 에서 볼 수 있듯이, graph Laplacian \(L\) 의 eigenvector 를 계산하지 않고도 spectral filtering 의 결과를 구할 수 있습니다. 특히 vertex \(i\) 에 대해서 보면,&lt;/p&gt;

\[f_{out}(i) 
= (g_{\theta}(L) f_{in})(i) 
= \sum^{K}_{k=0}\sum^{N}_{j=1} a_{k} \left(L^k\right)_{ij} f_{in}(j) 
\tag{$4$}\]

&lt;p&gt;graph Laplacian 의 localization \((1)\) 을 사용하면, \((4)\) 를  vertex \(i\) 의 \(K\) - hop local neighborhood \(N(i,K)\) 에 대해 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
f_{out}(i) 
&amp;amp;= \sum^{N}_{j=1} \sum^{K}_{k=d_G(i,j)} a_k\left(L^k\right)_{ij} f_{in}(j) \\
\\
&amp;amp;= \sum_{j\in N(i,K)} b_{ij} f_{in}(j)
\end{align}
\tag{5}\]

&lt;p&gt;즉 \(f_{out}(i)\) 는 \(i\) 의 K - localized neighborhood 의 vertices \(j\) 에 대해 \(f_{in}(j)\) 들의 합으로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;따라서, spectral filter \(g_{\theta}\) 가 order \(K\) polynomial 이라면 filter 가 vertex domain 에서 \(K\) - localized 된다는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;D. Shuman, S. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. &lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6494675&quot;&gt;The Emerging Field of Signal
Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and other Irregular Domains&lt;/a&gt;. &lt;em&gt;IEEE Signal Processing Magazine&lt;/em&gt;, 30(3):83–98, 2013.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="spectral filtering" /><category term="polynomial filter" /><summary type="html">Graph Convolutional Network 이해하기 : (4) Localized Polynomial Filter</summary></entry><entry><title type="html">Graph Convolution and Spectral Filtering</title><link href="https://harryjo97.github.io/theory/Graph-Convoloution-and-Spectral-Filtering/" rel="alternate" type="text/html" title="Graph Convolution and Spectral Filtering" /><published>2021-01-03T17:00:00+09:00</published><updated>2021-01-03T17:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Graph-Convoloution-and-Spectral-Filtering</id><content type="html" xml:base="https://harryjo97.github.io/theory/Graph-Convoloution-and-Spectral-Filtering/">&lt;p&gt;Graph Convolutional Network 이해하기 :  (3) Graph convolution 과 spectral filtering&lt;/p&gt;

&lt;h2 id=&quot;why-do-we-need-graph-convolution&quot;&gt;Why do we need Graph Convolution?&lt;/h2&gt;

&lt;p&gt;Fourier transform 을 통해 그래프에서 convolution 을 정의한 이유는, 바로 CNN 을 그래프에 적용하기 위해서입니다. CNN 은 ML 의 여러 분야에서 뛰어난 성과를 거두었습니다. 특히, CNN 은 large-scale high dimensional 데이터로부터 local structure 를 학습하여 의미있는 패턴을 잘 찾아냅니다. 이 때 local feature 들은 convolutional filter 로 표현되며, filter 는 translation-invariant 이기 때문에 공간적인 위치나 데이터의 크기에 상관없이 같은 feature 를 뽑아낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;하지만, 그래프와 같이 irregular (non-Euclidean) domain 에서는 직접 convolution 을 정의할 수 없습니다. 기존의 convolution 의 정의는 discrete 한 그래프에서는 의미를 갖지 못합니다. 따라서, Graph Convolutional Network 를 위해서는 그래프에서 정의되는 convolution operator 가 새로 필요합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;graph-convolution&quot;&gt;Graph Convolution&lt;/h2&gt;

&lt;p&gt;Vertex domain 에서 직접 convolution operator 를 정의할 수 없기 때문에, Fourier transform 을 이용하여 Fourier domain 에서 convolution operator 를 정의합니다.&lt;/p&gt;

&lt;p&gt;기존의 convolution 과 같이 graph convolution 또한 Fourier transform 에 대해 다음의 조건을 만족해야 합니다. (Convolution theorem).&lt;/p&gt;

\[\widehat{g\ast f}(l) = \hat{g}(l)\hat{f}(l)
\tag{1}\]

&lt;p&gt;즉 vertex  domain 에서의 convolution 과 Fourier domain 에서의 multiplication 이 일치하도록 만들고 싶습니다. \((1)\) 에 대해 inverse Fourier transform 을 적용하면, 다음의 결과를 얻게 됩니다.&lt;/p&gt;

\[g\ast f = \sum^{N-1}_{l=0} \hat{g}(l) \hat{f}(l)u_l
\tag{2}\]

&lt;p&gt;따라서, vertex domain 에서 정의된 두 graph signal \(f\) 와 \(g\) 에 대해 convolution operator \(\ast\) 는 \((2)\) 과 같이 정의합니다. 이는 기존의 convolution 에서 complex exponential \(\left\{e^{2\pi i\xi t}\right\}_{\xi\in\mathbb{R}}\) 대신 graph Laplacian eigenvector \(\{u_l\}^{N-1}_{l=0}\)  을 사용했다고 이해할 수 있습니다. \((2)\) 는 Hadamard product \(\odot\) 와  \(\{u_l\}^{N-1}_{l=0}\) 을 column vector 로 가지는 Fourier basis \(U\) 를 사용해, 다음과 같은 형태로 표현할 수 있습니다.&lt;/p&gt;

\[g \ast f = U((U^Tg) \odot (U^Tf))
\tag{3}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;spectral-filtering-of-graph-signal&quot;&gt;Spectral Filtering of Graph Signal&lt;/h2&gt;

&lt;p&gt;위에서 정의한 graph convolution 을 사용해, 다음과 같이 graph signal \(f_{in}\) 의 \(g\) 에 대한 filtering 을 정의할 수 있습니다.&lt;/p&gt;

\[f_{out} = g\ast f_{in}\]

&lt;p&gt;\((1)\) 을 사용하면 filtering 은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}

f_{out} 
&amp;amp;= \sum^{N-1}_{l=0} \hat{f}_{out}(l)u_l \\
&amp;amp;= 
\begin{bmatrix}
\big| &amp;amp; \big| &amp;amp;  &amp;amp; \big| \\
u_0 &amp;amp; u_1 &amp;amp; \cdots &amp;amp; u_{N-1} \\
\big| &amp;amp; \big| &amp;amp;  &amp;amp; \big|
\end{bmatrix}
\begin{bmatrix}
\hat{f}_{out}(0) \\
\vdots \\
\hat{f}_{out}({N-1})
\end{bmatrix} \\
\\
&amp;amp;= U
\begin{bmatrix}
\hat{g}(0)\hat{f}_{in}(0) \\
\vdots \\
\hat{g}({N-1})\hat{f}_{in}({N-1})
\end{bmatrix} \\
\\
&amp;amp;= U
\begin{bmatrix}
\hat{g}(0) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; \hat{g}(1) &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp;  &amp;amp; \ddots &amp;amp; \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \hat{g}({N-1})
\end{bmatrix}
\begin{bmatrix}
\hat{f}_{in}(0) \\
\vdots \\
\hat{f}_{in}({N-1})
\end{bmatrix} \\
\\
&amp;amp;= U\,\text{diag}(\hat{g})\,U^T f_{in}

\tag{4}

\end{align}\]

&lt;p&gt;\((4)\) 를 통해 convolution operator 는 Fourier domain 에서 diagonalize 되는 operator 로 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Reference 의 [1, 2] 에서는 \(\text{diag}(\hat{g})\) 를 함수가 아닌, filter 의 parameter 로 해석했습니다. 이를 spectral construction 이라 부르며, vertex domain 에서 localized filter 를 사용하는spatial construction 에 비해 parameter 의 수를 \(N^2\) 에서 \(N\) 으로 줄였다는데 의의가 있습니다. 하지만, Fourier basis \(U\) 를 사용하기 위해서는 computational cost 가 높은 eigenvalue decomposition 을 수행해야 하기 때문에, 효율적인 방법이 아닙니다.&lt;/p&gt;

&lt;p&gt;이런 문제를 해결하기 위해 [3] 에서는 \(g_{\theta}\) 로 \(L\) 의 eigenvalue 들에 대한 polynomial 을 사용했습니다. 이 경우,  \(L = U\Lambda U^T\) 를 만족하기 때문에, \(U\) 대신 \(L\) 로써 \((5)\) 를 표현할 수 있고, eigen decomposition 을 하지 않아도 되기 때문에 굉장히 효율적입니다.&lt;/p&gt;

&lt;p&gt;따라서 GCN 에서의 spectral convolution 은 \(\hat{g}\) 을 \(L\) 의 eigenvalue 에 대한 함수 \(g_{\theta}\) 로 생각하고, \(\text{diag}(\hat{g})\) 대신 다음의 \(g_\theta(\Lambda)\) 를 사용하여 \((4)\) 를 표현합니다.&lt;/p&gt;

\[g_{\theta}(\Lambda) =
\begin{bmatrix}
g_{\theta}(\lambda_0) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; g_{\theta}(\lambda_1) &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp;  &amp;amp; \ddots &amp;amp; \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; g_{\theta}(\lambda_{N-1})
\end{bmatrix}\]

&lt;p&gt;따라서, \(g_{\theta}\) 에 대한 filtering 의 결과는 다음과 같습니다.&lt;/p&gt;

\[f_{out} = Ug_{\theta}(\Lambda)U^T f_{in}
\tag{5}\]

&lt;p&gt;\((5)\) 의 spectral filtering 은 다음과 같이 Fourier domain 에서 \(g_{\theta}\) 에 대한 filtering 으로 이해할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Convolution-and-Spectral-Filtering/filtering.PNG&quot; style=&quot;max-width: 80%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. &lt;a href=&quot;https://arxiv.org/pdf/1312.6203.pdf&quot;&gt;Spectral networks and locally
connected networks on graphs&lt;/a&gt;. In International Conference on Learning Representations (ICLR),
2014.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M. Henaff, J. Bruna, and Y. LeCun. &lt;a href=&quot;https://arxiv.org/pdf/1506.05163.pdf&quot;&gt;Deep Convolutional Networks on Graph-Structured Data&lt;/a&gt;.
arXiv:1506.05163, 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. &lt;a href=&quot;https://arxiv.org/pdf/1606.09375.pdf&quot;&gt;Convolutional neural networks on
graphs with fast localized spectral filtering&lt;/a&gt;. In Advances in neural information processing systems
(NIPS), 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thomas N. Kipf and Max Welling. &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;Semi-supervised classification with graph convolutional networks&lt;/a&gt;.
In International Conference on Learning Representations (ICLR), 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="graph convolution" /><category term="spectral filtering" /><summary type="html">Graph Convolutional Network 이해하기 : (3) Graph convolution 과 spectral filtering</summary></entry><entry><title type="html">Graph Fourier Transform</title><link href="https://harryjo97.github.io/theory/Graph-Fourier-Transform/" rel="alternate" type="text/html" title="Graph Fourier Transform" /><published>2021-01-01T19:00:00+09:00</published><updated>2021-01-01T19:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Graph-Fourier-Transform</id><content type="html" xml:base="https://harryjo97.github.io/theory/Graph-Fourier-Transform/">&lt;p&gt;Graph Convolutional Network 이해하기 : (2) Graph Fourier transform&lt;/p&gt;

&lt;h2 id=&quot;classical-fourier-transform&quot;&gt;Classical Fourier Transform&lt;/h2&gt;

&lt;h3 id=&quot;fourier-transform&quot;&gt;Fourier transform&lt;/h3&gt;

&lt;p&gt;Integrable function \(f : \mathbb{R} \rightarrow \mathbb{C}\) 의 Fourier transform 은 다음과 같이 정의합니다.&lt;/p&gt;

\[\hat{f}(\xi) = \langle f, e^{2\pi i\xi t} \rangle = \int_{\mathbb{R}} f(t) e^{-2\pi i\xi t}dt \tag{$1$}\]

&lt;p&gt;즉 \(\hat{f}(\xi)\) 은 \(f(t)\) 의 \(e^{2\pi i \xi t}\) 성분 (frequency) 의 크기 (amplitude) 를 의미합니다. \((1)\) 을 살펴보면, Fourier transform 은 time domain \(t\in\mathbb{R}\) 에서 정의된 함수 \(f(t)\) 를 frequency domain \(\xi\in\mathbb{C}\) 에서 정의된 함수 \(\hat{f}(\xi)\) 로 변환시켜준다는 것을 알 수 있습니다. 다음의 그림을 통해 time domain 으로부터 frequency domain 으로의 Fourier transform 을 이해할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Fourier-Transform/fourier.jpg&quot; style=&quot;max-width: 80%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;inverse-fourier-transform&quot;&gt;Inverse Fourier transform&lt;/h3&gt;

&lt;p&gt;\((1)\) 에서 정의된 Fourier transform 의 역과정인 inverse Fourier transform 은 다음과 같습니다.&lt;/p&gt;

\[f(x) = \int_{\mathbb{R}} \hat{f}(\xi)e^{2\pi i\xi t}d\xi \tag{$2$}\]

&lt;p&gt;Invere Fourier transform 은 Fourier transform 과 반대로 frequency domain 에서 정의된 함수 \(\hat{f}\) 을 time domain 에서의 함수 \(f\) 로 변환시켜줍니다. \((2)\) 는 주어진 \(\hat{f}\) 으로부터 원래의 함수 \(f\) 를 복원하는 과정이며, 각 \(e^{2\pi i \xi t}\) 성분의 amplitude \(\hat{f}(\xi)\) 이 주어졌을 때 원래의 함수 \(f\) 를 각 성분들의 합으로 표현하는 변환입니다.&lt;/p&gt;

&lt;h3 id=&quot;laplacian-operator&quot;&gt;Laplacian operator&lt;/h3&gt;

&lt;p&gt;\((1)\) 과 \((2)\) 에서 등장하는 complex exponentials \(\left\{ e^{2\pi i \xi t} \right\}_{\xi\in\mathbb{C}}\) 는 1 차원 Laplacian operator \(\Delta\) 의 eigenfunction 입니다.&lt;/p&gt;

\[\Delta(e^{2\pi i \xi t}) = \frac{\partial^2}{\partial t^2}e^{2\pi i \xi t} 
= -(2\pi\xi)^2 e^{2\pi i \xi t}\]

&lt;p&gt;즉 Fourier transform 은 Laplacian operator \(\Delta\) 의 eigenfunction 들의 합으로 분해하는 변환으로 생각 할 수 있습니다. [3]&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;graph-fourier-transform&quot;&gt;Graph Fourier Transform&lt;/h2&gt;

&lt;h3 id=&quot;graph-fourier-transform-1&quot;&gt;Graph Fourier transform&lt;/h3&gt;

&lt;p&gt;그래프에서의 graph Laplacian \(L\) 은 Euclidean space 의 Laplacian operator \(\Delta\) 와 같은 역할을 합니다. 특히, 1 차원 Laplacian operator \(\Delta\) 에 대해 complex exponential 들은 eigenfunction 이며, 그래프에서는 \(L\) 의 eigenvector 들이 그 역할을 대신합니다.&lt;/p&gt;

&lt;p&gt;이 때 graph Laplacian 의 eigenvector 들로 \(\mathbb{R}^N\) 의 orthonormal basis 를 구성할 수 있기 때문에, orthonormal eigenvector \(\left\{ u_l \right\}^{N-1}_{l=0}\) 을 사용해 \((2)\) 와 같이 graph Fourier transform 을 정의할 수 있습니다. Euclidean space 에서 compex exponential 들이 Fourier basis 를 이루듯이, 그래프에서는 eigenvector \(\left\{ u_l \right\}^{N-1}_{l=0}\) 들이 Fourier basis 가 됩니다.&lt;/p&gt;

&lt;p&gt;Graph signal \(f \in\mathbb{R}^{N}\) 에 대한 Fourier transform 은  \(\left\{ u_l \right\}^{N-1}_{l=0}\) 성분들의 합으로 분해하는 변환으로 이해할 수 있습니다.  \(u_l\) 성분의 크기 (amplitude) 는 inner product 를 사용해 다음과 같이 계산할 수 있습니다.&lt;/p&gt;

\[\hat{f}(l) = \langle u_l, f\rangle =  \sum^N_{i=1} u^{T}_l(i)f(i) 
\tag{3}\]

&lt;p&gt;\(\hat{f}\) 을 다음과 같이 \(\mathbb{R}^N\) 의 vector 로 생각하겠습니다.&lt;/p&gt;

\[\hat{f} 
= \begin{bmatrix}
\hat{f}(0) \\
\vdots \\
\hat{f}({N-1})
\end{bmatrix}\]

&lt;p&gt;\(L\) 의 eigenvector \(\left\{ u_l \right\}^{N-1}_{l=0}\) 를 column 으로 가지는 행렬 \(U\) 에 대해&lt;/p&gt;

\[U = \begin{bmatrix}
\bigg| &amp;amp; \bigg| &amp;amp; &amp;amp; \bigg| \\
u_0 &amp;amp; u_1 &amp;amp; \cdots &amp;amp; u_{N-1} \\
\bigg| &amp;amp; \bigg| &amp;amp; &amp;amp; \bigg|
\end{bmatrix}\]

&lt;p&gt;\((3)\) 를 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\hat{f} 
= \begin{bmatrix}
\hat{f}(0) \\
\vdots \\
\hat{f}({N-1})
\end{bmatrix}
= \begin{bmatrix}
- &amp;amp; u_0^{T} &amp;amp; - \\

&amp;amp; \vdots &amp;amp; \\
- &amp;amp; u_{N-1}^{T} &amp;amp; -
\end{bmatrix}
\begin{bmatrix}
f(1) \\
\vdots \\
f(N)
\end{bmatrix}
= U^{T}f\]

&lt;p&gt;따라서, graph signal \(f\) 에 대한 graph Fourier transform 은 Fourier basis \(U\) 를 사용해 다음과 같이 쓸 수 있습니다.
\(\hat{f} = U^T f
\tag{4}\)&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;inverse-graph-fourier-transform&quot;&gt;Inverse graph Fourier transform&lt;/h3&gt;

&lt;p&gt;Inverse Fourier transform 과 마찬가지로, inverse graph Fourier transform 은 graph Fourier transform 의 역과정입니다. 그렇기 때문에, \((5)\) 에서 정의된 graph Fourier transform 을 되돌리는 과정은 다음과 같습니다.&lt;/p&gt;

\[f \overset{\mathrm{(a)}}{=} UU^Tf = U\hat{f}
\tag{5}\]

&lt;p&gt;위의 식에서 Fourier basis 가 orthonormal 하기 때문에 \(UU^T = I\) 이므로 \((a)\) 가 성립합니다.&lt;/p&gt;

&lt;p&gt;\((6)\) 의 결과는 \(\hat{f}\) 의 의미를 통해서 유도할 수 있습니다. \(\left\{ u_l \right\}^{N-1}_{l=0}\) 은 \(\mathbb{R}^N\) 의 orthonormal basis 를 이루기 때문에, Fourier transform 의 결과인 \(\hat{f}\) 으로 부터 원래의 \(f\) 를 얻어내기 위해서는 각 성분들을 모두 더해주면 됩니다. 이 때, \(f\) 의 \(u_l\) 성분의 크기는 \(\hat{f}(l)\) 이기 때문에, 다음의 등식이 성립합니다.&lt;/p&gt;

\[f(i) = \sum^{N-1}_{l=0} \hat{f}(l)u_l(i) 
\tag{6}\]

&lt;p&gt;\((6)\) 를 \(\mathbb{R}^N\) 의 vector 로 표현하면, \((5)\) 의 결과와 일치합니다.&lt;/p&gt;

\[f 
= \begin{bmatrix}
f(1) \\
\vdots \\
f(N)
\end{bmatrix}
= \begin{bmatrix}
\bigg| &amp;amp; \bigg| &amp;amp; &amp;amp; \bigg| \\
u_0 &amp;amp; u_1 &amp;amp; \cdots &amp;amp; u_{N-1} \\
\bigg| &amp;amp; \bigg| &amp;amp; &amp;amp; \bigg|
\end{bmatrix} 
\begin{bmatrix}
\hat{f}(0) \\
\vdots \\
\hat{f}({N-1})
\end{bmatrix}
= U\hat{f}\]

&lt;h3 id=&quot;parseval-relation&quot;&gt;Parseval relation&lt;/h3&gt;

&lt;p&gt;Classical Fourier transform 에서와 마찬가지로, graph Fourier transform 은 Parseval relation 을 만족합니다.&lt;/p&gt;

\[\begin{align}
	\langle f, g\rangle 
	&amp;amp;= \langle \sum^{N-1}_{l=0}\hat{f}(l)u_l, \sum^{N-1}_{l'=0}\hat{g}({l'})u_{l'} \rangle \\
	&amp;amp;= \sum_{l, l'} \hat{f}(l)\hat{g}({l'})\langle u_l, u_{l'} \rangle \\
	&amp;amp;= \sum_{l} \hat{f}(l)\hat{g}(l) = \langle \hat{f}, \hat{g} \rangle
\end{align}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;David K. Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory. Applied and Computational Harmonic Analysis&lt;/a&gt;, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. &lt;a href=&quot;https://arxiv.org/pdf/1606.09375.pdf&quot;&gt;Convolutional neural networks on
graphs with fast localized spectral filtering&lt;/a&gt;. In Advances in neural information processing systems
(NIPS), 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Terence Tao, &lt;a href=&quot;https://www.math.ucla.edu/~tao/preprints/fourier.pdf&quot;&gt;Fourier Trasnform&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="graph fourier transform" /><summary type="html">Graph Convolutional Network 이해하기 : (2) Graph Fourier transform</summary></entry><entry><title type="html">Graph Laplacian</title><link href="https://harryjo97.github.io/theory/Graph-Laplacian/" rel="alternate" type="text/html" title="Graph Laplacian" /><published>2020-12-30T22:00:00+09:00</published><updated>2020-12-30T22:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Graph-Laplacian</id><content type="html" xml:base="https://harryjo97.github.io/theory/Graph-Laplacian/">&lt;p&gt;Graph Convolutional Network 이해하기 : (1) Graph Laplacian&lt;/p&gt;

&lt;h2 id=&quot;basic-definition&quot;&gt;Basic definition&lt;/h2&gt;

&lt;h3 id=&quot;weighted-adjacency-matrix&quot;&gt;Weighted adjacency Matrix&lt;/h3&gt;

&lt;p&gt;주어진 undirected weighted graph \(G = (V,E,W)\) 는 vertices 의 집합 \(V\), edges 의 집합 \(E\), 그리고 weighted adjacency matrix \(W\) 로 이루어집니다. 이 포스트에서는 \(\vert V\vert= N &amp;lt; \infty\) 을 가정합니다. 편의상 \(V = \{1,2,\cdots,N\}\) 으로 나타내겠습니다. 여기서 vertices 의 ordering 은 임의로 주어진 것이며, 의미를 가지지 않습니다.&lt;/p&gt;

&lt;p&gt;\(E\) 의 원소 \(e = (i,j)\) 는 vertex \(i\) 와 \(j\) 를 연결하는 edge 를 나타냅니다. 또한  \(W_{ij}\) 는 edge \(e = (i,j)\) 의 weight 을 의미하며 만약 \(i\) 와 \(j\)를 연결하는 edge 가 없다면 \(W_{ij}=0\) 이고 edge 가 있는 경우 \(W_{ij}&amp;gt;0\) 입니다. 이 때 \(W\) 는 모든 vertex pair 마다 정의되고 그래프가 undirected 이므로, \(W\) 는 \(N\times N\) real symmetric matrix 입니다.&lt;/p&gt;

&lt;p&gt;Weighted adjacency matrix 의 한 예로 adjacency matrix 가 있습니다. Adjacency matrix 는 \(i\) 와 \(j\) 를 연결하는 edge 가 있다면 \(W_{ij} = 1\), 없다면 \(W_{ij}=0\) 으로 정의합니다. 다음의 그림은 그래프 (labeled) 에 대한 adjacency matrix 의 예시입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Laplacian/adjacency.gif&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;h3 id=&quot;degree-matrix&quot;&gt;Degree matrix&lt;/h3&gt;

&lt;p&gt;주어진 weighted adjacency matrix \(W\) 에 대해 degree matrix  \(D\) 는 다음을 만족하는 diagonal matrix 로 정의합니다.
\(D_{ii} = \sum^{N}_{j=1} W_{ij}
\tag{1}\)&lt;/p&gt;

&lt;p&gt;쉽게 말해, \(D_{ii}\) 는 vertex \(i\) 를 끝점으로 가지는 edge 들의 weight 를 모두 더한 값과 같습니다. 특히 adjacency matrix 에 대해서는 \((1)\) 의 합이 각 vertex 의 degree 를 의미하기 때문에 degree matrix 라는 명칭이 붙었습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;unnormalized-graph-laplacian&quot;&gt;Unnormalized Graph Laplacian&lt;/h2&gt;

&lt;p&gt;주어진 undirected weighted graph \(G = (V,E,W)\) 에 대해  unnormalized graph Laplacian 은 다음과 같이 정의됩니다.&lt;/p&gt;

\[L = D-W\]

&lt;p&gt;여기서 \(D\) 는 앞서 정의한 degree matrix 입니다. Unnormalized graph Laplacain 은 combinatorial Laplacian 이라고도 불립니다.&lt;/p&gt;

&lt;p&gt;다음은 예시 그래프에 대한 adjacency matrix, degree matrix, 그리고 graph Laplacian matrix 를 보여줍니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Laplacian/graph-eg.jpg&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Unnormalized graph Laplacian 은 다음과 같은 특징을 가집니다.&lt;/p&gt;

&lt;h3 id=&quot;real-symmetric&quot;&gt;Real symmetric&lt;/h3&gt;

&lt;p&gt;Undirected weighted graph \(G\) 에 대해 \(W\) 와 \(D\) 는 \(N\times N\) real symmetric matrix 이므로, \(L=D-W\) 또한  \(N\times N\) real symmetric matrix 입니다.&lt;/p&gt;

&lt;h3 id=&quot;positive-semi-definite&quot;&gt;Positive semi-definite&lt;/h3&gt;

&lt;p&gt;임의의 \(x\in\mathbb{R}^N\) 에 대해,&lt;/p&gt;

\[\begin{align}
x^TLx 
&amp;amp;= x^TDx - x^TWx = \sum_{i} D_{ii}x_i^2 - \sum_{i,j}x_iW_{ij}x_j \\
&amp;amp;= \frac{1}{2} \left( 2\sum_{i} D_{ii}x_i^2 - 2\sum_{i,j} W_{ij}x_ix_j \right) \\
&amp;amp;\overset{\mathrm{(a)}}{=} \frac{1}{2}\left( 2\sum_{i}\left\{\sum_{j}W_{ij}\right\} x_i^2 - 2\sum_{i,j} W_{ij}x_ix_j \right) \\
&amp;amp;\overset{\mathrm{(b)}}{=} \frac{1}{2}\left( \sum_{i,j}W_{ij}x_i^2 + \sum_{i,j}W_{ij}x_j^2  - 2\sum_{i,j} W_{ij}x_ix_j \right) \\
&amp;amp;= \frac{1}{2}\sum_{i,j} W_{ij}(x_i-x_j)^2 \geq 0 \tag{2}
\end{align}\]

&lt;p&gt;유도 과정에서 \((a)\) 는 degree matrix 의 정의를 사용하였고, \((b)\) 는 \(W\) 가 symmetric 이기 때문에 성립합니다. 따라서 \(L\) 은 positive semi-definite matrix 입니다.&lt;/p&gt;

&lt;h3 id=&quot;non-negative-eigenvalue&quot;&gt;Non-negative eigenvalue&lt;/h3&gt;

&lt;p&gt;\(L\) 이 positive semi-definite matrix 이므로, \(L\) 은 non-negative real eigenvalue 들을 가집니다.&lt;/p&gt;

&lt;h3 id=&quot;orthonormal-basis&quot;&gt;Orthonormal basis&lt;/h3&gt;

&lt;p&gt;\(L\) 은 real symmetric matrix 이기 때문에 다음의 lemma 를 이용하면, \(L\) 의 eigenvector 들로 \(\mathbb{R}^N\) 의 orthonormal basis 를 만들 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;__Lemma : __ Real symmetric matrix 는 diagonalizable 합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;multiplicity-of-zero&quot;&gt;Multiplicity of zero&lt;/h3&gt;

&lt;p&gt;\(u_0 = \frac{1}{\sqrt{N}}\begin{bmatrix} 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix}^T\) 에 대해 다음을 만족하기 때문에,&lt;/p&gt;

\[Lu_0 = (D-W)u_0 = \mathbf{0}\]

&lt;p&gt;\(L\) 은 0 을 eigenvalue 로 가지며 \(u_0\) 는 \(L\) 의 eigenvector 입니다. 이 때, 주어진 그래프와 상관 없이 \(L\) 은 항상 0 을 eigenvalue 로 가지고 \(u_0 = \frac{1}{\sqrt{N}}\begin{bmatrix} 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix}^T\) 또한 항상 eigenvector 가 됩니다.&lt;/p&gt;

&lt;p&gt;0 의 eigenvector \(u\) 에 대해 \((2)\) 의 결과를 이용하면, 다음과 같습니다.
\(0 = u^TLu = \frac{1}{2}\sum_{i,j}W_{ij}(u(i) - u(j))^2\)&lt;/p&gt;

&lt;p&gt;따라서 \(W_{ij}\neq 0\) 인 모든 vertices \(i\) 와 \(j\) , 즉 edge로 연결된 \(i\) 와 \(j\) 에 대해 \(u(i) = u(j)\) 를 만족합니다. \((\ast)\)&lt;/p&gt;

&lt;p&gt;\(k\) 개의 connected components 를 가지는 그래프 \(G\) 의 graph Laplacian \(L\) 은&lt;/p&gt;

\[L = \begin{bmatrix}
L_1 &amp;amp; &amp;amp; &amp;amp; \\
 &amp;amp; L_2 &amp;amp; &amp;amp; \\
 &amp;amp; &amp;amp; \ddots &amp;amp; \\
 &amp;amp; &amp;amp; &amp;amp; L_k
\end{bmatrix}\]

&lt;p&gt;sub-Laplacian \(L_i\) 들로 이루어진 block matrix 로 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;각각의 sub-Laplacian 들은 모두 0 을 eigenvalue 로 가지고, eigenvector \(u\) 는 \((\ast)\) 로 인해 \(u_0\) 로 유일하게 결정되기 때문에 각각의 sub-Laplacian 들은 정확히 한 개의 eigenvalue 0 을 가집니다.  \(k\) 개의 connected components 를 가지는 그래프에 대해서 \(L\) 은 정확히 \(k\) 개의 eigenvalue 0 들을 가집니다. 따라서 graph Laplacian \(L\) 의 eigenvalue 0 의 multiplicity 는 주어진 그래프 \(G\) 의 connected components 의 개수와 일치합니다 [3].&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;주어진 그래프를 connected 라고 가정하면 \(L\) 의 eigenvalue 들을 다음과 같이 나열할 수 있습니다.&lt;/p&gt;

\[0 = \lambda_0 &amp;lt; \lambda_1 \leq \cdots \leq \lambda_{N-1}\]

&lt;p&gt;\(L\) 의 eigenvalue 들, 즉 spectrum 을 다루는 분야가 바로 spectral graph theory 입니다. Spectral graph theory 에 포함된 Fideler vector, Cheegar Constant, Laplacian embedding, NCut 등에 대해서는, 기회가 생기면 다른 포스트를 통해 자세히 설명하겠습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;other-graph-laplacians&quot;&gt;Other Graph Laplacians&lt;/h2&gt;

&lt;h3 id=&quot;normalized-graph-laplacian&quot;&gt;Normalized graph Laplacian&lt;/h3&gt;

&lt;p&gt;Normalized graph Laplacian \(L^{norm}\) 은 다음과 같이 정의합니다.&lt;/p&gt;

\[L^{norm} = D^{-1/2}\;L\;D^{-1/2} = I -  D^{-1/2}\;W\;D^{-1/2}\]

&lt;p&gt;\(L\) 과 같이 symmetric positive semi-definite matrix 입니다. 따라서  위에서 설명한 eigenvalue 에 대한 성질이 동일하게 적용됩니다.  하지만 \(L\) 과 \(L^{norm}\) 은 similar matrices 가 아니기 때문에 다른 eigenvector 를 가집니다. 특히 \(L\) 의 eigenvalue 0 에 대한 eigenvector \(u_0\) 는 그래프에 상관 없이 일정하지만, \(L^{norm}\) 의 경우 그래프에 따라 변합니다. Normalized graph Laplacian 의 특징으로는, eigenvalue 들이 \([0,2]\) 구간에 속한다는 것입니다. 그래프 \(G\) 가 bipartite graph 일 때만 \(L^{norm}\) 의 가장 큰 eigenvalue 가 2가 됩니다 [3].&lt;/p&gt;

&lt;h3 id=&quot;random-walk-graph-laplacian&quot;&gt;Random walk graph Laplacian&lt;/h3&gt;

&lt;p&gt;Random walk graph Laplacian \(L^{rw}\) 는 다음과 같이 정의합니다.&lt;/p&gt;

\[L^{rw} = D^{-1}L = I - D^{-1}W\]

&lt;p&gt;여기서 \(D^{-1}W\) 는 random walk matrix 로 그래프 \(G\) 에서의 Markov random walk 를 나타내어 줍니다. \(L^{rw}\) 는 symmetric 이 보장되지 않습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;D. Shuman, S. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. &lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6494675&quot;&gt;The Emerging Field of Signal
   Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and other Irregular Domains&lt;/a&gt;. &lt;em&gt;IEEE Signal Processing Magazine&lt;/em&gt;, 30(3):83–98, 2013.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;F. R. K. Chung. Spectral Graph Theory, volume 92. American Mathematical Society, 1997.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt; &lt;/p&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="graph laplacian" /><summary type="html">Graph Convolutional Network 이해하기 : (1) Graph Laplacian</summary></entry></feed>