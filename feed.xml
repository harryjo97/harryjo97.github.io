<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://harryjo97.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://harryjo97.github.io/" rel="alternate" type="text/html" /><updated>2021-02-19T15:02:39+09:00</updated><id>https://harryjo97.github.io/feed.xml</id><title type="html">Graph ML review</title><subtitle>about Graph ML</subtitle><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><entry><title type="html">Composition-based Multi-Relational Graph Convolutional Networks</title><link href="https://harryjo97.github.io/paper%20review/Composition-based-Multi-Relational-Graph-Convolutional-Networks/" rel="alternate" type="text/html" title="Composition-based Multi-Relational Graph Convolutional Networks" /><published>2021-02-17T12:00:00+09:00</published><updated>2021-02-17T12:00:00+09:00</updated><id>https://harryjo97.github.io/paper%20review/Composition-based-Multi-Relational-Graph-Convolutional-Networks</id><content type="html" xml:base="https://harryjo97.github.io/paper%20review/Composition-based-Multi-Relational-Graph-Convolutional-Networks/">&lt;p&gt;[paper review] CompGCN 이해하기&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;대부분의 Graph Convolutional Networks (GCNs) 는 simple, undirectred graph 에 대해서 연구가 이루어졌습니다. 더 일반적인 형태의 그래프는 edge 에 feature 와 direction 이 존재하는 multi-relational graph 이며, 예로 knowledge graph 가 있습니다. GCN 을 통해 multi-relational graph 를 분석하는 기존의 방법은 node representation 만을 학습할 수 있고, over-parametrization 의 문제가 있습니다. 특히 node 뿐만 아니라 node 사이 relation 에 대한 학습이 필요한 link prediction 의 경우 GCN 을 직접 사용하는데 어려움이 있습니다. 논문에서는 이런 문제점을 해결하기 위해 GCN 을 통해 node 의 representation 뿐만 아니라, relation 의 representation 을 같이 학습하는 CompGCN 모델을 제시합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;knowledge-graph&quot;&gt;Knowledge Graph&lt;/h3&gt;

&lt;p&gt;Knowledge graph 는 multi-relational graph 의 한 종류로 Freebase, WordNet 과 같은 knowledge base 를 그래프로 나타낸 것입니다. Entity 들을 node 로, entity 들 사이의 relation 을 edge 로 표현합니다. Node 들의 집합 \(\mathcal{V}\) 와 relation 들의 집합 \(\mathcal{R}\) 에 대해, edge 는 node \(u,v\in\mathcal{V}\)사이의 relation \(r\in\mathcal{R}\) 로 이루어진 triplet \((u,r,v)\) 로 정의됩니다. 이러한 edge 들의 집합으로 knowledge graph \(\mathcal{G} = \{(u,r,v)\} \subset \mathcal{V}\times\mathcal{R}\times\mathcal{V}\) 를 정의합니다.&lt;/p&gt;

&lt;p&gt;Knowledge graph 는 수많은 triplet 들로 이루어져 있고, 많은 수의 triplet 들이 incomplete 합니다. 이를테면, 주어진 두 entity \(u\) 와 \(v\) 사이의 relation 이 알려지지 않은 triplet \((u,?,v)\), 혹은 node \(u\) 와 연결된 relation \(r\) 이 주어졌지만 tail entity 를 모르는 triplet \((u,r,?)\) 과 같은 케이스가 있습니다. 이런 imcomplete 한 triplet 을 완성하기 위해서는 link prediction 이 필요합니다. Link prediction 은 각 entity 와 relation 들에 대한 low-dimensional representation 을 찾고, 올바른 triplet 에 더 높은 score 를 부여하는 score function 을 통해 이루어집니다. 대표적인 모델로 TransE, DistMult, RotatE 등이 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;gcn-on-multi-relational-graphs&quot;&gt;GCN on Multi-Relational Graphs&lt;/h3&gt;

&lt;p&gt;Simple undirected graph 에 대한 GCN 의 layer-wise propagation rule 은 다음과 같이 표현할 수 있습니다 [3].&lt;/p&gt;

\[H^{(k+1)} = f\left( \hat{A}H^{(k)}W^{(k)} \right)
\tag{1}\]

&lt;p&gt;하지만 multi-relational graph 에도 \((1)\) 을 적용한다면, relation 에 대한 정보를 사용하지 않기 때문에 제대로된 representation 을 찾을 수 없습니다. 따라서 \((1)\) 의 \(W^{(k)}\) 대신 relation specific weight \(W^{(k)}_r\) 를 사용하여, 다음과 같이 새로운 propagation rule 을 정의할 수 있습니다.&lt;/p&gt;

\[H^{(k+1)} = f\left( \hat{A}H^{(k)}W^{(k)}_r \right)
\tag{2}\]

&lt;p&gt;\((2)\) 를 사용할 경우 relation 의 수가 많아질수록 over-parametrization 이 쉽게 일어날 수 있다는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;compgcn-details&quot;&gt;CompGCN Details&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Directed-GCN [5] 과 같이 relation 이 단방향이 아닌 양방향으로 정의될 수 있도록, edge 와 relation 들의 집합 \(\mathcal{E}\) 과 \(\mathcal{R}\) 을 확장합니다. 기존의 edge \((u,r,v)\in\mathcal{E}\) 에 대해, relation \(r\) 의 방향을 뒤집은 inverse \(r^{-1}\) 를 이용한 inverse edge \((v,r^{-1},u)\) 와 self-loop \(\top\) 을 이용한 self edge \((u,\top,u)\) 을 추가해 새로운 edge 들의 집합 \(\mathcal{E}'\) 을 다음과 같이 정의합니다.&lt;/p&gt;

\[\mathcal{E}' = \mathcal{E}\,\cup\{(v,r^{-1},u):(u,r,v)\in\mathcal{E}\,\}\,\cup\{(u,\top,u):u\in V\}\]

&lt;p&gt;또한 기존의 relation \(r\in\mathcal{R}\) 에 대해 inverse relation \(r^{-1}\) 과 self-loop \(\top\) 을 추가한 새로운 relation 들의 집합 \(\mathcal{R}'\) 을 다음과 같이 정의합니다.&lt;/p&gt;

\[\mathcal{R}' = \mathcal{R}\,\cup\{r^{-1}:r\in\mathcal{R}\}\,\cup\{\top\}\]

&lt;p&gt;Self-loop \(\top\) 을 추가해주는 이유는 GCN [3] 에서와 같이 embedding 을 update 해줄 때 주변 node 들의 embedding 뿐만 아니라, 자가 자신의 embedding 에 대한 정보를 사용해주기 위해서입니다 [4, 5].&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;CompGCN 은 각 node \(u\in\mathcal{V}\) 에 대한 node embedding \(h_u\in\mathbb{R}^d\) 와 각 relation \(r\in\mathcal{R}'\) 에 대한 relation embedding \(h_r\in\mathbb{R}^d\) 을 학습합니다. 이 때 node embedding 과 relation embedding 의 차원이 같도록 설정해줍니다. 다음의 그림은 추가된 inverse relation 들과 node embedding, relation embedding 을 보여줍니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/graph.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
 &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Composition Operation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GCN 에 relation embedding 을 적용하기 위해, entity-relation composition operation \(\phi:\mathbb{R}^d\times \mathbb{R}^d\rightarrow\mathbb{R}^d\) 을 사용합니다. 이는 entity 의 embedding \(h_v\) 와 relation 의 embedding \(h_r\) 을 통해 새로운 embedding \(\phi(h_v,h_r)\) 을 만들어주는 operation 입니다.&lt;/p&gt;

&lt;p&gt;논문에서는 composition operation \(\phi\) 를 다음의 세 가지 non-parametrized operation 에 한정시킵니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;subtraction : \(\phi(h_v,h_r) = h_v - h_r\)&lt;/li&gt;
  &lt;li&gt;multiplication : \(\phi(h_v,h_r) = h_v\ast h_r\)&lt;/li&gt;
  &lt;li&gt;circular-correlation : \(\phi(h_v,h_r) = h_v\star h_r\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;각각의 operation 들은 TransE, DistMult, HolE 모델에서 영감을 받았다고 설명하지만, 실험 결과를 보면 operation 들에 특별한 의미가 있는 것 같지는 않습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Node Embedding Update&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;\((2)\) 를 통한 node embedding 의 update 는 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[h_u \leftarrow f\left( \sum_{(u,r,v)\in\mathcal{E}'}W_rh_v \right)
\tag{3}\]

&lt;p&gt;\((3)\) 은 \(u\) 를 head entity 로 가지는 모든 edge \((u,r,v)\) 들에 대해, relation \(r\) 과 tail entity \(v\) 의 정보를 통해 \(u\) 의 새로운 embedding 을 update 하는 과정으로 이해할 수 있습니다. 이 때 over-parametrization 의 문제점을 가지는 relation specific weight \(W_r\) 을 사용하지 않기 위해, composition operation \(\phi(h_v,h_r)\) 을 통해 relation 에 대한 정보를 담아냅니다.&lt;/p&gt;

&lt;p&gt;Composition operation 을 사용한다면, 모든 node 들에 대해 공통적인 weight \(W\) 를 사용해 \((3)\) 을 다음과 같이 바꿀 수 있습니다.
\(h_u \leftarrow f\left( \sum_{(u,r,v)\in\mathcal{E}'}W\phi(h_v,h_r) \right)
\tag{4}\)&lt;/p&gt;

&lt;p&gt;더 나아가 \(\mathcal{E}'\) 에서 기존의 edge 와 새로 추가된 inverse edge, self edge 들을 구분하기 위해, \((4)\) 의 \(W\) 대신 direction specific weight \(W_{\text{dir}(r)}\) [5] 를 사용하여 새로운 update rule 을 정의해줍니다.&lt;/p&gt;

\[h_u \leftarrow f\left( \sum_{(u,r,v)\in\mathcal{E}'}W_{\text{dir}(r)}\,\phi(h_v,h_r) \right)
\tag{5}\]

&lt;p&gt;이 때 direction specific weight \(W_{\text{dir}(r)}\) 는 다음과 같이 구분할 수 있습니다.&lt;/p&gt;

\[W_{\text{dir}(r)} = 
\begin{cases}
W_O, &amp;amp; r\in\mathcal{R} \\
W_I, &amp;amp; r\in\mathcal{R}_{inv} \\
W_S, &amp;amp; r=\top
\end{cases}\]

&lt;p&gt;\((5)\) 의 과정은 다음의 그림을 통해 이해할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/update.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
 &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Relation Embedding Update&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;\((5)\) 를 통해 node embedding 을 update 한 후, 학습 가능한 행렬 \(W_{\text{rel}}\) 을 통해 다음과 같이 relation embedding 을 update 해줍니다.&lt;/p&gt;

\[h_r \leftarrow W_{\text{rel}}h_r
\tag{6}\]

&lt;blockquote&gt;
  &lt;p&gt;Basis Formulation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;또한 relation 의 수가 증가함에 따라 CompGCN 모델이 필요 이상으로 복잡해지는 것을 막기 위해, [4] 의 basis formulation 을 응용합니다. 각 relation 들의 initial representation 을 다음과 같이 학습 가능한 basis vector \(\{v_1,\cdots,v_{\mathcal{B}}\}\) 들의 linear combination 으로 표현합니다.&lt;/p&gt;

\[h_r^{(0)} = \sum^{\mathcal{B}}_{b=1} \alpha_{br}v_b 
\tag{7}\]

&lt;p&gt;여기서 \(\alpha_{br}\in\mathbb{R}\) 은 relation 과 basis vector 에 의존하는, 학습 가능한 scalar 입니다.&lt;/p&gt;

&lt;p&gt;\((7)\) 을 통해 서로 다른 relation 들의 embedding 을 공통의 basis vector 들로 표현할 수 있습니다. 이를 weight sharing 관점에서 볼 때 수가 적은 (rare) relation 들과 수가 많은 (frequent) relation 들이 wegiht 을 공유하기 때문에, rare relation 들에 대해 overfitting 이 일어나는 것을 방지할 수 있습니다 [4]. CompGCN 은 Relational-GCN [4] 과 다르게, initial representation 만을 basis vector 로 표현하며 이후의 layer 에서는 basis 를 사용하지 않습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Comparison With Other Models&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;\((5)\) 의 update rule 은 GCN [3], Relational-GCN [4], Directed-GCN [5], Weighted-GCN 모델들을 모두 일반화한 것입니다. 각각의 모델들은 다음의 표와 같이 \((5)\) 의 direction specific weight \(W_{\text{dir}(r)}\) 와 composition operation 을 특정해주어 나타낼 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/general.PNG&quot; style=&quot;max-width: 90%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;다음의 표는 각 모델들이 반영한 특징을 잘 정리해 놓았습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/other.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;experiment--results&quot;&gt;Experiment &amp;amp; Results&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;논문에서는 CompGCN 모델을 link prediction, node classification, graph classification 세 가지 task 들에 대해 performance 를 측정합니다.&lt;/p&gt;

&lt;h3 id=&quot;link-prediction&quot;&gt;Link Prediction&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Performance comparison&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;먼저 link prediction task 에 대해 5 가지 metric 으로 평가하여 모델들의 performance 를 비교합니다. FB15k-237 과 WN18RR 데이터셋에 대한 CompGCN 과 baseline 모델들의 성능을 측정한 결과는 다음의 표에 정리되어있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/link.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;대부분의 metric 에서 CompGCN 의 성능이 가장 뛰어남을 확인할 수 있습니다. 2 가지 metric 에서 CompGCN 보다 뛰어난 성능을 보이는 RotatE [2] 모델은 entity 와 relation 을 복소수 영역에서 다루며, relation 을 rotation operation 으로 해석합니다. CompGCN 또한 complex domain 에서의 rotation operation 을 적용한다면, 더 우수한 성능을 낼 수 있지 않을까 기대해봅니다.&lt;/p&gt;

&lt;p&gt;Relational-GCN 과 같은 기존의 모델 대신 CompGCN 을 사용하는 것이 얼마나 효과적인지에 대한 분석이 필요합니다. Score function \(X\) 와 entity embedding 을 위한 모델 \(M\) 그리고 CompGCN 의 경우 composition operator \(Y\) 의 다양한 조합 \(X+M(Y)\) 에 대해, FB15k-237 데이터셋에서 성능을 평가했습니다. 결과는 다음의 표에 정리되어 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/encoder.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;다양한 조합들 중, 모델 \(M\) 으로 CompGCN 을 사용했을 때 성능이 가장 좋음을 볼 수 있습니다. 대부분의 모델들은 TransE 의 score function 을 적용했을 때 성능이 눈에 띄게 떨어지지만, CompGCN 은 성능에 큰 차이가 나지 않습니다. CompGCN 의 performance 가 다른 모델들에 비해 뛰어난 이유는, node embedding 뿐만 아니라 relation embedding 을 같이 학습하기 때문이라고 추측할 수 있습니다. 다음의 그림에서 볼 수 있듯이 Relational-GCN 과 Weighted-GCN 은 entity embedding 만을 학습하지만, CompGCN 은 relation embedding 을 같이 학습하기 때문에 entity 와 relation 을 모두 고려해야하는 link prediciton task 에서 효과적입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/embedding.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;CompGCN 의 성능을 composition operation 에 따라 비교해보면, 사용한 score function 에 따라 달라지지만 대체적으로 circular-correlation 과 같이 복잡한 operation 을 사용한 경우 더 나은 performance 를 보입니다. 특히, 다양한 조합들 중 ConvE 의 score function, circular-correlation (Corr) 과 함께 CompGCN 을 사용했을 때의 성능이 가장 뛰어남을 확인할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Scalability&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CompGCN 의 scalability 를 분석하기 위해, relation 의 수와 basis vector 의 수에 따른 CompGCN 의 performance 를 비교했습니다.  FB15k-237 데이터셋에서 ConvE + CompGCN (Corr) 모델을 사용해 basis vector 의 수 \(\mathcal{B}\) 에 따른 성능을 측정하였고, 결과는 다음의 그래프와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/basis.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;\(\mathcal{B}\) 가 커질수록 CompGCN 의 performance 가 좋아지며, \(\mathcal{B}=5\) 일 때에도 충분히 뛰어난 성능을 보입니다. \(\mathcal{B}\) 가 작을수록 parameter 의 수가 줄어들기 때문에, CompGCN 은 적은 parameter 로도 충분히 multi-relational graph 의 representatin 을 학습한다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;Relational-GCN 과 자세히 비교하기 위해, \(\mathcal{B}=5\) 로 제한된 CompGCN 과 relation 의 수에 따른 성능을 측정하였습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/rgcn.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Relational-GCN 과 비교했을 때, \(\mathcal{B}=5\) 로 제한된 조건에서도 relation 의 수와 상관 없이 CompGCN 이 R-GCN 보다 더 좋은 성능을 보여줍니다.&lt;/p&gt;

&lt;h3 id=&quot;node-classification-graph-classification&quot;&gt;Node Classification, Graph Classification&lt;/h3&gt;

&lt;p&gt;마지막으로 node classification 과 graph classification task 에서의 성능을 baseline 과 비교했습니다. Node classification 에 대해서는 MUTAG 과 AM 데이터셋에서의 정확도를, graph classification 에 대해서는 MUTAG 과 PTC 데이터셋에서의 정확도를 측정했습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Composition-based-Multi-Relational-Graph-Convolutional-Networks/classification.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;두 task 모두에서 baseline 모델들보다 월등히 뛰어난 정확도를 보여줍니다. 이를 통해 CompGCN 이 node embedding 만을 학습하는 기존의 GCN 보다 효과적으로 multi-relational graph 의 representation 을 학습함을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;future-study&quot;&gt;Future Study&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;여러가지 방면에서 CompGCN 에 대한 추가적인 연구가 이루어질 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Composition operation \(\phi\) 를 non-parametrized operation 이 아닌, Neural Tensor Networks (NTN) 와ConvE 같은 parametrized operation 을 사용했을 때 CompGCN 의 성능이 개선될 것이라고 생각합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RotatE 모델과 같이 relation 의 다양한 패턴들, symmetry / antisymmetry / inversion / composition, 을 반영할 수 있도록 score function 및 composition operation 에 대해 연구가 진행 될 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CompGCN 에 대한 이론적인 연구가 필요합니다. 실험적으로 relation embedding 을 통해 multi-relational graph 의 representation 을 학습하는데 효과적이라는 것은 입증했지만, 어떤 이유로 효과적인에 대해 설명이 부족합니다. 특히 score function 과 composition operation 이 performance 에 미치는 영향에 대해 연구한다면, CompGCN 을 더 효과적으로 사용할 수 있을 것입니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha P. Talukdar. &lt;a href=&quot;https://arxiv.org/pdf/1911.03082.pdf&quot;&gt;Composition-based multi-relational graph convolutional networks&lt;/a&gt;. In ICLR, 2020.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. &lt;a href=&quot;https://arxiv.org/pdf/1902.10197.pdf&quot;&gt;Rotate: Knowledge graph embedding by relational rotation in complex space&lt;/a&gt;. arXiv preprint arXiv:1902.10197, 2019.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thomas N Kipf and Max Welling. &lt;a href=&quot;&quot;&gt;Semi-supervised classification with graph convolutional networks&lt;/a&gt;. arXiv preprint arXiv:1609.02907, 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. &lt;a href=&quot;https://arxiv.org/pdf/1703.06103.pdf&quot;&gt;Modeling relational data with graph convolutional networks&lt;/a&gt;. arXiv preprint arXiv:1703.06103, 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Diego Marcheggiani and Ivan Titov. &lt;a href=&quot;https://arxiv.org/pdf/1703.04826.pdf&quot;&gt;Encoding sentences with graph convolutional networks for
semantic role labeling&lt;/a&gt;. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1506–1515. Association for Computational Linguistics, 2017&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;paper review&quot;]" /><category term="knowledge graph embedding" /><summary type="html">[paper review] CompGCN 이해하기</summary></entry><entry><title type="html">Graph Attention Networks</title><link href="https://harryjo97.github.io/paper%20review/Graph-Attention-Networks/" rel="alternate" type="text/html" title="Graph Attention Networks" /><published>2021-02-08T20:00:00+09:00</published><updated>2021-02-08T20:00:00+09:00</updated><id>https://harryjo97.github.io/paper%20review/Graph-Attention-Networks</id><content type="html" xml:base="https://harryjo97.github.io/paper%20review/Graph-Attention-Networks/">&lt;p&gt;[paper review] : Graph Attention Networks 이해하기&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;CNN 은 image classification, semantic segmentation, machine translation 등 다양한 분야에 적용되어 좋은 성능을 보여주었습니다. CNN 의 핵심인 convolution 은 주어진 data 의 구조가 grid-like (Euclidean domain) 특성을 가질 때 정의됩니다. 3D mesh, social network, biological network 등과 같이 irregular 한 그래프 data 에 대해 convolution operator 를 일반화시키기 위한 다양한 연구가 이루어지고 있습니다.&lt;/p&gt;

&lt;p&gt;Graph domain 에서 convolution 을 일반화시키는 연구는 크게 두 가지로 나눌 수 있습니다. 첫번째 방법은 spectral approach 로, graph Laplacian 을 통해 Fourier domain 에서 convolution 을 정의합니다. 이 때 eigendecomposition 과 같은 복잡한 행렬 연산과 non-spatially localized filter 의 문제를 해결하기 위해, ChebNet 은 Chebyshev expansion 을 사용했고, 더 나아가 [2] 에서는 Chebyshev expansion 을 각 node 의 1-step neighborhood 에 한정시켜 node classification, link prediction, graph classification 등 다양한 분야에서 좋은 성능을 보여주는 GCN 을 제시했습니다. 하지만 spectral approach 의 가장 큰 문제점은 바로 그래프의 전체 구조에 의존한다는 것입니다. Input graph 에 따라 graph Laplacian 이 변하기 때문에, inductive learning 에 직접 적용될 수 없습니다.&lt;/p&gt;

&lt;p&gt;두번째 방법은, non-spectral approach 입니다. 공간적으로 가까운 neighbor 를 통해 그래프에서 직접 convolution 을 정의합니다. Graph Laplacian 을 사용하지 않기 때문에 spectral approach 의 문제점을 피해가지만, 크기가 다른 neighborhood 들에 대해서 적용되며  CNN 의 weight-sharing 특성을 유지하는 convolution operator 를 정의하는 것은 굉장히 어렵습니다. 대표적인 모델로 MoNet 과 GraphSAGE [3] 가 있습니다.&lt;/p&gt;

&lt;p&gt;논문은 [4] 에서 제시된 attention mechanism 을 통해 non-spectral appoach 의 문제점을 해결합니다. Attention mechanism 의 장점은, 고정되지 않은 input size 에 적용될 수 있다는 점입니다. 그래프에서 크기가 다른 neighborhood 들에 대해서도 공통적으로 적용될 수 있기 때문에, attention mechansim 을 통한 non-spectral approach 는 weight-sharing 특성을 가질 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;gat-architecture&quot;&gt;GAT Architecture&lt;/h2&gt;

&lt;h3 id=&quot;graph-attentional-layer&quot;&gt;Graph Attentional Layer&lt;/h3&gt;

&lt;p&gt;들어가기 앞서, \(N\) 개의 node 를 가지는 그래프에 대해 node \(i\) 의 feature 를 vector \(h_i\in\mathbb{R}^{F}\) 로 나타내겠습니다. 여기서 \(F\) 는 node 의 input feature dimension 이며, \(F'\) 을 output 의 feature dimension 이라고 하겠습니다. 또한, node 의 ordering 에는 의미가 없으며 단순히 node 를 구분하기 위한 notation 입니다.&lt;/p&gt;

&lt;p&gt;GAT 를 이루는 graph attentional layer 는, 모든 node 들에 대해 공통된 weight matrix \(W\in\mathbb{R}^{F'\times F}\) 와 self-attention \(\mathcal{A}:\mathbb{R}^{F'}\times\mathbb{R}^{F'}\rightarrow\mathbb{R}\) 로 이루어집니다. 먼저  self-attention 을 통해 attention coefficient \(e_{ij}\) 를 다음과 같이 계산합니다.&lt;/p&gt;

\[e_{ij} = \mathcal{A}\left( Wh_i, Wh_j \right)
\tag{1}\]

&lt;p&gt;\((1)\) 의 attention coefficient \(e_{ij}\) 는 node \(i\) 에 대한 node \(j\) 의 중요도 (importance) 로 해석할 수 있습니다. 만약 모든 node 들의 쌍에 대해 attention coefficient 를 사용한다면, 그래프의 구조적인 특성을 무시하게 됩니다. GAT 의 핵심은, 바로 node \(i\) 의 neighborhood \(N_i\) 에 속하는 node \(j\) 들에 대해서만 coefficient \(e_{ij}\) 를 사용하는 것입니다. Masked attention 을 통해 그래프의 구조에 대한 정보를 살릴 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;각 node 들에 대해 \((1)\) 의 coefficient 들을 비교할 수 있도록, [4] 에서와 같이 softmax 함수를 통해 정규화를 해줍니다.&lt;/p&gt;

\[\alpha_{ij} = \text{softmax}_j(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k\in N_i}\exp(e_{ik})}
\tag{2}\]

&lt;p&gt;논문에서는 \(\mathcal{A}\) 를 weight vector \(a\in\mathbb{R}^{2F'}\) 와 LeakyReLU 를 사용해 다음과 같이 정의합니다.&lt;/p&gt;

\[\mathcal{A}\left( Wh_i, Wh_j \right)
= \text{LeakyReLU}\left( a^T\left[ Wh_i\vert\vert Wh_j \right] \right)
\tag{3}\]

&lt;p&gt;아래의 그림은 \((3)\) 의 과정을 나타냅니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Attention-Networks/attention.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;\((3)\) 을 적용해 \((2)\) 를 다시 표현하면 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

\[\alpha_{ij} = \frac{\exp\left( \text{LeakyReLU}\left( a^T\left[ Wh_i\,\Vert\, Wh_j \right] \right) \right)}{\sum_{k\in N_i}\exp\left(\text{LeakyReLU}\left( a^T\left[ Wh_i\,\Vert\, Wh_k \right] \right)\right)}
\tag{4}\]

&lt;p&gt;\((4)\) 의 정규화된 attention coefficient 를 통해 다음과 같이 node \(i\) 의 feature vector 를 update 해줍니다.&lt;/p&gt;

\[h_i \leftarrow \sigma\left( \sum_{j\in N_i}\alpha_{ij}Wh_j \right)
\tag{5}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;또한 논문에서는 self-attention 의 학습 과정을 안정화시키기 위해, transformer 에 대한 논문 “Attention is all you need” 의 방법과 같이 multi-head attention 을 사용했습니다. \(K\) 개의 independent 한 attention mechanism \((5)\) 들의 concatenation 을 통해 다음과 같이 새로운 layer-wise propagation rule 을 정의합니다.&lt;/p&gt;

\[h_i \leftarrow \Big\Vert^{K}_{k=1} \sigma\left( \sum_{j\in N_i}\alpha^{k}_{ij}W^kh_j \right)
\tag{6}\]

&lt;p&gt;\(K=3\) 일 때 independent 한 attention mechanism 들을 서로 다른 색의 화살표로 표현한다면, \((6)\) 의 식을 아래의 그림과 같이 이해할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Attention-Networks/multihead.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;마지막 layer 에서는 concatenation 대신 feature 들의 averaging 을 통해 final output 을 만들어줍니다.&lt;/p&gt;

\[h_i \leftarrow \sigma\left( \frac{1}{K}\sum^K_{k=1}\sum_{j\in N_i}\alpha^{k}_{ij}W^kh_j \right)
\tag{7}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;GAT 의 가장 큰 장점은 계산이 효율적으로 이루어질 수 있다는 것입니다. Self-attentional layer 는 모든 edge 들에 대해 병렬화가 가능하고, node 의 output feature 는 각 node 마다 병렬적으로 계산이 가능합니다. 특히 spectral approach 에서와 같은 eigendecomposition 혹은 복잡한 행렬 연산이 필요하지 않습니다. 1개의 head 에 대한 \((6)\) 의 계산 복잡도는 \(O\left(\vert V\vert FF' + \vert E\vert F'\right)\) 이며, 이는 GCN 의 복잡도와 비슷합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;gat-vs-gcn&quot;&gt;GAT vs GCN&lt;/h3&gt;

&lt;p&gt;GCN 과의 가장 큰 차이는 동일한 neighborhood 내의 node 들에 대해 다른 importance 를 부여할 수 있다는 점입니다. GCN 의 layer-wise propagation rule 은 normalization constant \(c_{ij}=\sqrt{\vert N_i\vert\vert N_j\vert}\) 를 통해 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[h_i \leftarrow \sigma\left( \sum_{j\in N_i}\frac{1}{c_{ij}}Wh_j \right)
\tag{8}\]

&lt;p&gt;\((5)\) 와 비교해보면, \(c_{ij}\) 는 값이 고정되어 있지만 \(\alpha_{ij}\) 는 weight vector \(a\in\mathbb{R}^{2F'}\) 에 따라 변할 수 있습니다. Weight 가 고정되어 있지 않기 때문에 GCN 보다 더 expressive 하고다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 \((8)\) 에서 볼 수 있듯이 GCN 은 학습 전에 그래프의 전체 구조에 대한 정보 (graph Laplacian) 를 알고 있어야합니다. GAT 의 경우 전체 구조에 대한 정보가 필요 없기 때문에 GCN 과 다르게 inductive learning 에 직접 이용될 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;gat-vs-graphsage&quot;&gt;GAT vs GraphSAGE&lt;/h3&gt;

&lt;p&gt;GraphSAGE 는 대표적인 non-spectral approach 로, GAT 와 비슷한 propagation rule 을 따릅니다. 하지만 GraphSAGE 는 GAT 와의 달리 neighborhood 중 일부만을 sample 해 사용합니다. 이는 계산량을 한정시키기 위해 선택한 방법으로, 추론 과정에서 neighborhood 중 일부의 정보만을 이용하게 됩니다. 또한 LSTM 을 aggregator 로 사용한 GraphSAGE 와 다르게, GAT 는 node 의 ordering 과 무관합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;GAT 모델을 다른 baseline 모델들과 비교하기 위해, 잘 알려진 4 가지 dataset 에서 실험을 진행했습니다. Transductive learning 의 performance 측정을 위해 Cora, Citeseer, Pubmed 세 가지의 citation network dataset 을 사용했습니다. 또한 inductive learing 의 performance 측정을 위해 protein-protein interaction (PPI) dataset 에서도 실험을 수행했습니다. 각 dataset 의 특징은 아래의 table 1 에 정리되어 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/post/Graph-Attention-Networks/dataset.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;transductive-learning&quot;&gt;Transductive Learning&lt;/h3&gt;

&lt;p&gt;Transductive learning task 의 baseline 들로는 [2] 의 실험에서 사용된 baseline 들과 함께 GCN 을 사용했습니다. GAT 와 baseline 모델들의 성능은 mean classification error 로 측정되었고, 결과는 아래의 표에 정리되어 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Attention-Networks/transductive.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;GCN 과의 비교를 통해, 같은 neighborhood 내의 node 들에 대해 다른 weight 를 부여하는 방법이 효과적임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;inductive-learning&quot;&gt;Inductive Learning&lt;/h3&gt;

&lt;p&gt;Transductive learning task 의 baseline 들로 활용된 모델들은 inductive learning 에 직접적으로 적용되기 힘들기 때문에, inductive learning task 의 baseline 에서 제외했습니다. Inductive learning task 의 baseline 들로는 GraphSAGE 의 variant 들을 선택했습니다. 특히 GraphSAGE 모델 중 성능이 좋다고 알려진 두 모델 : pool aggregator 를 사용하는 GraphSAGE-pool 과 LSTM aggregator 를 사용하는 GraphSAGE-LSTM 과 더불어 aggregator 로 GCN 또는 mean 을 사용한 GraphSAGE-GCN, GraphSAGE-mean 총 네 개의 모델을 골랐습니다.&lt;/p&gt;

&lt;p&gt;GAT 모델이 그래프 구조에 대한 정보를 이용하는지 확인하기 위해, 그래프의 구조를 전혀 이용하지 않는 multilayer perceptron (MLP) classifier 를 실험에 포함시켰습니다. 또한 GAT 모델의 특징 중 한 가지가 바로 같은 neighborhood 내의 node 들에 대해서 다른 weight 를 부여할 수 있다는 것인데, 이를 확인하기 위해 constant attention mechanism 을 사용한 Const-GAT 모델을 GAT 모델과 함께 비교했습니다.&lt;/p&gt;

&lt;p&gt;GAT 와 baseline 모델들의 성능은 micro-averaged \(F_1\) score 로 측정되었으며, 결과는 아래의 표에 정리되어 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Attention-Networks/inductive.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;GraphSAGE 의 performance 와 비교를 통해, neighborhood 의 일부만 sampling 하는 것보다 전체 neighborhood 를 이용하는 것이 효과적임을 확인할 수 있습니다. 또한 Const-GAT 모델과의 비교를 통해 다시 한번 같은 neighborhood 내의 node 들에 대해서 다른 weight 를 부여하는 것이 중요하다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 GAT 모델을 통해 학습한 feature representation 을 눈으로 확인하기 위해, data visualization 을 위해 많이 사용되는 t-SNE (stochastic neighbor embedding) 를 아래의 그림과 같이  시각화 했습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Attention-Networks/t-sne.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Node 의 색은 7개의 class 에 해당하며, 각각의 class 끼리 clustering 된 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;GAT 는 graph-structured data 에 적용될 수 있는 convolutin-style 의 neural network 로 다음과 같은 특징을 가지고 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Computationally Efficient&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;앞서 설명했듯이, 계산이 효율적으로 이루어질 수 있습니다. Self-attentional layer 는 모든 edge 들에 대해 병렬화가 가능하고, node 의 output feature 는 각 node 마다 병렬적으로 계산이 가능합니다. 특히 복잡한 행렬 연산을 사용하지 않습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Different Importance&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;같은 neighborhood 내의 node 들에 대해 다른 weight 을 부여해줄 수 있습니다. 이웃한 node 들에 대해 weight 이 고정되어 있지 않기 때문에, GCN 보다 expressive 한 특성을 가집니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Inductive Learning&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Node 들이 서로 다른 degree 를 가지는 그래프에도 적용할 수 있으며, 그래프의 전체 구조에 의존하지 않기 때문에 inductive learning 이 가능합니다.&lt;/p&gt;

&lt;p&gt;GAT 는 attention mechanism 을 사용하기 때문에, 학습된 attentional weight 를 통해 모델의 해석에 도움을 줄 수 있습니다. Attentional weight 을 이용한 model interpretability 관련 연구가 기대됩니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. &lt;a href=&quot;https://arxiv.org/pdf/1710.10903.pdf&quot;&gt;Graph attention networks&lt;/a&gt;. arXiv preprint arXiv:1710.10903, 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thomas N Kipf and Max Welling. &lt;a href=&quot;&quot;&gt;Semi-supervised classification with graph convolutional networks&lt;/a&gt;. arXiv preprint arXiv:1609.02907, 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Will Hamilton, Zhitao Ying, and Jure Leskovec. &lt;a href=&quot;https://arxiv.org/pdf/1706.02216.pdf&quot;&gt;Inductive representation learning on large graphs&lt;/a&gt;. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Neural machine translation by jointly
learning to align and translate&lt;/a&gt;. International Conference on Learning Representations (ICLR),
2015.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;paper review&quot;]" /><category term="GAT" /><summary type="html">[paper review] : Graph Attention Networks 이해하기</summary></entry><entry><title type="html">How Powerful are Graph Neural Networks?</title><link href="https://harryjo97.github.io/paper%20review/How-Powerful-are-Graph-Neural-Networks/" rel="alternate" type="text/html" title="How Powerful are Graph Neural Networks?" /><published>2021-02-02T19:00:00+09:00</published><updated>2021-02-02T19:00:00+09:00</updated><id>https://harryjo97.github.io/paper%20review/How-Powerful-are-Graph-Neural-Networks</id><content type="html" xml:base="https://harryjo97.github.io/paper%20review/How-Powerful-are-Graph-Neural-Networks/">&lt;p&gt;[Paper review] Graph Isomorphism Network 이해하기&lt;/p&gt;

&lt;h2 id=&quot;related-study&quot;&gt;Related Study&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;GNN 의 expressive power 에  대한 연구는 크게 두 가지 방향으로 이루어집니다. 첫 번째 방법은 이 논문과 같이, Weisfeiler-Lehman (WL) graph isomorphism test 를 통해 GNN 의 expressive power 에 대한 limitation 을 연구합니다 (No. 1, 2, 5). 다른 방향으로는, permutation invariant function 들에 대한 universal approximation 을 통해 GNN 의 expressive power 를 다룹니다 (No. 3, 5). 최근에는 GNN 의 width, depth 와 expressive power 의 연관성에 대한 연구도 이루어졌습니다 (No. 6).&lt;/p&gt;

&lt;p&gt;제가 공부하며 expressive power 와 관련된 논문을 아래의 리스트로 정리했습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;No.&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Paper&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.02244.pdf&quot;&gt;Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Morris et al., 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.11136.pdf&quot;&gt;Provably Powerful Graph Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Maron et al., 2019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1901.09342.pdf&quot;&gt;On the Universality of Invariant Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Maron et al., 2019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.04943.pdf&quot;&gt;Universal Invariant and Equivariant Graph Neural Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Keriven et al., 2019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.12560.pdf&quot;&gt;On the equivalence between graph isomorphism testing and function approximation with GNNs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Chen et al., 2019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=B1l2bp4YwS&quot;&gt;What graph neural networks cannot learn: depth vs width&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Loukas, 2020&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;잘 알려져 있는 Graph Convolutional Network, GraphSAGE, Graph Attention Network, Gated Graph Neural Netowork 등 대부분의 GNN 은 recursive neighborhood aggregation (message passing) scheme 을 사용합니다 [2]. 이런 network 들을 Message Passing Neural Network (MPNN) 이라 부릅니다. MPNN 은 매 iteration 마다 node 주변 neighborhood 의 feature vector (representation) 를 수집하여, node 의 새로운 feature vector 를 update 합니다. \(k\) 번의 iteration 후, 각 node 들은 \(k\)-hop neighborhood 의 feature vector 들로 update 된 새로운 feature vector 를 가지게 됩니다. 충분한 수의 iteration 후에는, 각 node 의 feature vector 가 그래프 전체의 구조에 대한 정보를 포함한다고 해석할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Neighborhood aggregataion scheme 을 사용하는 GNN 은 node classification, link prediction, graph classification 등 다양한 task 에 대해 state-of-the-art 성능을 보여줍니다. 하지만, 모델의 설계는 주로 경험적인 직관 혹은 실험을 통한 시행 착오를 통해 이루어집니다. GNN 의 limitation 과 expressive power 등의 이론적인 연구가 바탕이 된다면 더 효율적인 모델을 만들 수 있고, 또한 모델의 hyperparameter tuning 에 큰 도움이 될 것입니다.&lt;/p&gt;

&lt;p&gt;논문에서는 GNN 의 expressive power 를 Weisfeiler-Lehman (WL) graph isomorphism test 를 통해 설명합니다. WL test 또한 MPNN 과 같이 매 iteratin 마다 주변 neighborhood 의 feature vector 를 수집해 각 node 의 feature vector 를 update 합니다. WL test 는 regular graph 와 같이 특수한 그래프를 제외하고는, 대부분의 그래프를 구분해낼 수 있습니다 (up to isomorphism). 그 이유는, 바로 알고리즘에서 neighborhood aggregation 이후 node 의 feature vetor 를 update 하는 과정이 injective 하기 때문입니다. WL test 의 알고리즘에서는 그래프의 두 node 가 서로 다른 neighborhood 를 가지고 있다면, 서로 다른 label 을 가지게 됩니다.&lt;/p&gt;

&lt;p&gt;Node 의 neighborhood 를 feature vector 들의 multiset 으로 표현하면, GNN 의 neighborhood aggregation scheme 은 multiset 에 대한 함수로 볼 수 있습니다. GNN 이 WL test 와 같이 그래프를 구분할 수 있는 능력 (discriminative power) 이 높지려면, neighborhood aggregation scheme 이 서로 다른 multiset 에 대해 서로다른 embedding 으로 보내주어야 합니다. 따라서, GNN 의 expressive power 를 multiset 에 대한 함수를 통해 분석할 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;논문에서 다루는 GNN 들은 모두 MPNN 으로, 매 iteration 마다 각 node 의 neighborhood feature vector 를 수집해 새로운 feature vector 로 update 합니다. 이를 neighborhood aggregation scheme 이라 부르며, 크게 두 단계로 나눌 수 있습니다.&lt;/p&gt;

&lt;p&gt;첫번 째 단계에서는, neighborhood 의 feature vector 들을 수집합니다. \(v\) 의 neighborhood \(N(v)\) 에 대해, \(u\in N(v)\) 의 feature vector 들을 모아줍니다. \(k\) 번째 iteration 에서 node \(v\) 의 feature vector 를 \(h_v^{(k)}\) 라고 하면, 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[a_v^{(k)} = \text{AGGREGATE}^{(k)}\left(\left\{\!\!\left\{h_u^{(k-1)}:u\in N(v)\right\}\!\!\right\}\right)\]

&lt;p&gt;이 때 \(\text{AGGREGATE}\) 함수는 multiset 에 대해 정의된 함수이며, 주로 summation 을 사용합니다. GraphSAGE [4] 에서와 같이 max-pooling 또는 mean-pooling 등을 사용할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;두번 째 단계에서는 전 단계에서 수집한 정보 \(a_v^{(k)}\) 와 현재의 feature vector \(h_v^{(k-1)}\) 를 사용해, node 의 새로운 feature vector 를 update 합니다.&lt;/p&gt;

\[h_v^{(k)} = \text{COMBINE}^{(k)}\left(h_v^{(k-1)},a_v^{(k)}\right)\]

&lt;p&gt;GraphSAGE 는 vector concatenation \([\,\cdot\,]\) 이후 weight matrix \(W\) 를 이용한 linear mapping 을 통해, 다음과 같은 \(\text{COMBINE}\) 함수를 사용했습니다.&lt;/p&gt;

\[\text{COMBINE}^{(k)}\left(h_v^{(k-1)},a_v^{(k)}\right)
= W \cdot \left[ h_v^{(k-1)},a_v^{(k)} \right]\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;위의 과정을 합치면, MPNN 의 \(k\) 번째 iteration 은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
h_v^{(k)} 
&amp;amp;= \text{COMBINE}^{(k)}\left(h_v^{(k-1)},a_v^{(k)}\right) \\
&amp;amp;= \text{COMBINE}^{(k)}\left(h_v^{(k-1)},\text{AGGREGATE}^{(k)}\left(\left\{\!\!\left\{h_u^{(k-1)}:u\in N(v)\right\}\!\!\right\}\right)\right) 
\tag{1}
\end{align}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Node classification 에서는 GNN 의 마지막 layer 에서 얻은 feature vector \(h_v^{(K)}\) 들로 prediction 을 수행합니다.  Graph classificaiton 의 경우 마지막 layer 에서 얻은 feature vector 들을 모아 \(\text{READOUT}\) 함수를 통해 graph representation \(h_G\) 를 표현하고, 이를 통해 prediction 을 수행합니다.&lt;/p&gt;

\[h_G = \text{READOUT}\left( \left\{\!\!\left\{ h_v^{(K)}:v\in V \right\}\!\!\right\} \right)
\tag{2}\]

&lt;p&gt;Graph representation \(h_G\) 가 node 의 ordering 에 따라 달라지지 않아야하기 때문에, \(\text{READOUT}\)  함수로 permutation invariant function 을 사용합니다. 간단한 예로 feature 들을 모두 더하는 summation 이 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;building-powerful-graph-neural-networks&quot;&gt;Building Powerful Graph Neural Networks&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;WL test 와 GNN 의 representational power 의 관계에 대해 알아보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;weisfeiler-lehman-test&quot;&gt;Weisfeiler-Lehman Test&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Lemma 2.&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Let \(G_1\) and \(G_2\) be any two non-isomorphic graphs. If a graph neural network \(\mathcal{A}:\mathcal{G}\rightarrow\mathbb{R}^d\) maps \(G_1\) and \(G_2\) to different embeddings, the Weisfeiler-Lehman graph isomorphism test also decides \(G_1\) and \(G_2\) are not isomorphic.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Lemma 2 에 의해, GNN 의 discriminative power 가 WL test 보다 좋을 수 없다는 것을 알 수 있습니다. 즉 WL test 로 구분하지 못하는 그래프들에 대해서는, 예를 들어 다음의 그림과 같이 circular skip link graph 들에 대해서는 GNN 또한 구분할 수 없습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/How-Powerful-are-Graph-Neural-Networks/csl.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Lemma 2 에 대한 증명의 핵심은 WL test 에서 feature vector 를 update 하는 과정이 injectivite 하다는 것입니다. 그렇다면, 과연 GNN 의 neighborhood aggregation 이 injective 할 때 WL test 와 같은 power 를 가질 수 있을까요? 이에 대한 답은 다음의 Theorem 3 를 통해 얻을 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Let \(\mathcal{A}:\mathcal{G}\rightarrow\mathbb{R}^d\) be a GNN. With a sufficient number of GNN layers, \(\mathcal{A}\) maps any graphs \(G_1\) and \(G_2\) that the Weisfeiler-Lehman test of isomorphism decides as non-isomorphic, to different embeddings if the following conditions hold:&lt;/p&gt;

  &lt;p&gt;a) \(\mathcal{A}\) aggregates and updates node features iteratively with&lt;/p&gt;

\[h_v^{(k)} = \phi\left( h_v^{(k-1)},f\left(\left\{\!\!\left\{ h_u^{(k-1)}:u\in N(v) \right\}\!\!\right\}\right) \right)\]

  &lt;p&gt;where the functions \(f\), which operates on multisets, and \(\phi\) are injective.&lt;/p&gt;

  &lt;p&gt;b) \(\mathcal{A}\) ‘s graph-level readout, which operates on the multiset of node features \(\left\{\!\!\left\{  h_v^{(k)}\right\}\!\!\right\}\), is injective&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Theorem 3 에서 함수 \(f\) 와 \(\phi\) 는 각각 위에서 설명한 \(\text{AGGREGATE}\) 와 \(\text{COMBINE}\) 함수에 해당하며, graph-level readout 은 \(\text{READOUT}\) 함수를 의미합니다. 즉 \(\text{AGGREGATE}\), \(\text{COMBINE}\) 과 \(\text{READOUT}\) 이 모두 multiset 에 대해 injective 일때, GNN 은 WL test 와 같은 discriminative power 를 가질 수 있다는 것이 Theorem 3 의 결론입니다.&lt;/p&gt;

&lt;p&gt;Lemma 2 와 Theorem 3 에 의해, neighborhood aggregation scheme 을 사용하는 GNN 의 discriminative power 에 대한 upper bound 를 WL test 를 통해 나타낼 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;GNN is at most as powerful as WL test in distinguishing different graphs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그래프를 구분하는 능력에 있어 GNN 이 WL test 보다 성능이 떨어진다면,  GNN 을 쓰는 이유가 무엇인지에 대해 생각해보아야 합니다. GNN 의 가장 큰 장점은 바로 그래프 사이의 similarity 에 대해 학습할 수 있다는 것입니다. WL test 에서의 feature vector 는 label 로 one-hot encoding 에 불과합니다. 두 그래프가 다르다는 것은 확실히 알 수 있어도, 얼마나 다른지에 대해서는 알 수 없습니다. 하지만 GNN 의 feature vector 를 통해 그래프를 구분하는 것 뿐만 아니라, 비슷한 그래프를 비슷한 embedding 으로 보내주도록 학습할 수 있습니다. 즉 두 그래프가 얼마나 다른지에 대해서도 알 수 있습니다. 이런 특성 덕분에, 다양한 분야에서 GNN 이 훌륭한 성과를 보여준다고 생각합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;graph-isomorphism-network&quot;&gt;Graph Isomorphism Network&lt;/h3&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;WL test 와 같은 discriminative power 를 가지는 GNN 을 만들기 위해서는, Theorem 3 에 의해 \((1)\) 의 \(\text{AGGREGATE}\) 와 \(\text{COMBINE}\) 함수가 mutiset 에 대해 injective 해야합니다. 그렇다면, 먼저 multiset 에 대해 injective 한 함수가 존재하는지를 알아야합니다. 다음의 Lemma 5 와 Corollary 6 에서 답을 찾을 수 있습니다. 논문에서는 node 의 input feature space \(\chi\) 가 countable universe 라고 가정합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Lemma 5.&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Assume \(\chi\) is countable. There exists a function \(f:\chi \rightarrow\mathbb{R}^n\) so that \(h(X)=\sum_{x\in X}f(x)\) is unique for each multiset \(X\subset\chi\) of bounded size. Moreover, any multiset function \(g\) can be decomposed as \(g(X)=\phi\left(\sum_{x\in X}f(x)\right)\) for some function \(\phi\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Corollary 6.&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Assume \(\chi\) is countable. There exists a function \(f:\chi \rightarrow\mathbb{R}^n\) so that for infinitely many choices of \(\epsilon\), including all irrational numbers, \(h(c,X)=(1+\epsilon)f(c) + \sum_{x\in X}f(x)\) is unique for each pair \((c,X)\) where \(c\in\chi\) and \(X\subset\chi\) is a multiset of bounded size. Moreover, any function \(g\) over such pairs can be decomposed as \(g(c,X)=\varphi\left( (1+\epsilon)f(c)+\sum_{x\in X}f(x) \right)\) for some function \(\varphi\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Lemma 5 와 Corollary 6 의 증명에서 핵심은, countable \(\chi\) 의 enumeration \(Z: \chi\rightarrow\mathbb{N}\) 와 bounded multiset \(X\) 에 대해 \(\vert X\vert&amp;lt;N\) 를 만족하는 \(N\) 을 사용해 \(f(x) = N^{-Z(x)}\) 를 정의하는 것입니다. 쉽게 말해, \(\chi\) 의 각 원소들을 나열하고 각 원소가 포함되었는지 아닌지를 \(N\) 진법으로 표현하는 것입니다.&lt;/p&gt;

&lt;p&gt;\((1)\) 에 Corollary 6 의 결과를 적용하면, 각 layer \(k=1,\cdots,K\) 에 대해 다음을 만족하는 함수 \(f^{(k)}\) 와 \(\varphi^{(k)}\) 가 존재합니다.&lt;/p&gt;

\[h_v^{(k)} = \varphi^{(k-1)}\left( (1+\epsilon)\;f^{(k-1)}\left(h_v^{(k-1)}\right)+\sum_{u\in N(v)}f^{(k-1)}\left(h_u^{(k-1)}\right) \right)
\tag{3}\]

&lt;p&gt;\((3)\) 에서 양변에 \(f^{(k)}\) 를 취해주면 다음과 같습니다.&lt;/p&gt;

\[f^{(k)}\left(h_v^{(k)}\right) = f^{(k)}\circ\varphi^{(k-1)}\left( (1+\epsilon)\;f^{(k-1)}\left(h_v^{(k-1)}\right)+\sum_{u\in N(v)}f^{(k-1)}\left(h_u^{(k-1)}\right) \right)
\tag{4}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;\(k\) 번째 layer 에서 각 node 의 feature vector 를 \(f^{(k)}\left(h_v^{(k)}\right)\) 로 생각한다면, \((4)\) 를 다음과 같이 간단히 쓸 수 있습니다.&lt;/p&gt;

\[h_v^{(k)} = f^{(k)}\circ\varphi^{(k-1)}\left( (1+\epsilon)\;h_v^{(k-1)}+\sum_{u\in N(v)}h_u^{(k-1)} \right)
\tag{5}\]

&lt;p&gt;Universal approximation theorem 덕분에 두 함수의 composition \(f^{(k)}\circ\varphi^{(k-1)}\) 을, multi-layer perceptrons (MLPs) 을 통해 근사할 수 있습니다. 또한 \((5)\) 의 \(\epsilon\) 을 학습 가능한 parameter \(\epsilon^{(k)}\) 으로 설정한다면, \((5)\) 를 다음과 같이 neural network 모델로 표현할 수 있습니다.&lt;/p&gt;

\[h_v^{(k)} = \text{MLP}^{(k)}\left( \left(1+\epsilon^{(k)}\right)\;h_v^{(k-1)}+\sum_{u\in N(v)}h_u^{(k-1)}\right)
\tag{6}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Graph Isomorphism Network (GIN) 은 \((6)\) 을 layer-wise propagation rule 로 사용합니다. Theorem 3 로 인해 GIN 은 WL test 와 같은 discriminative power 를 가지므로, maximally powerful GNN 이라는 것을 알 수 있습니다. WL test 와 같은 discriminative power 를 가지는 모델로 GIN 이 유일하지 않을 수 있습니다. GIN 의 가장 큰 장점은 구조가 간단하면서도 powerful 하다는 것입니다.&lt;/p&gt;

&lt;p&gt;Node classification 에는 \((6)\) 의 GIN 을  바로 사용하면 되지만, graph classification 에는 추가로 graph-level readout function 이 필요합니다. Readout function 은 node 의 feature vector 들에 대한 함수입니다. 이 때 node 의 feature vector (representation) 은 layer 를 거칠수록 local 에서 global 하게 변합니다. Layer 의 수가 너무 많다면, global 한 특성만 남을 것이고, layer 의 수가 너무 적다면 local 한 특성만 가지게 됩니다. 따라서, readout function 을 통해 그래프를 구분하기 위해서는, 적당한 수의 layer 를 거쳐야 합니다.&lt;/p&gt;

&lt;p&gt;이런 특성을 반영하기 위해, GIN 은 각 layer 의 graph representation (\(\text{READOUT}(\,\cdot\,)\) 의 output) 을 concatenation 으로 모두 합쳐줍니다.  그렇다면 최종 결과는 각 layer 마다 나타나는 그래프의 구조적 정보를 모두 포함하게 됩니다.&lt;/p&gt;

\[h_G = \text{CONCAT}\left( \text{READOUT} \left(\left\{\!\!\left\{h_v^{(k)} \right\}\!\!\right\}\right) \,:\, k=0,1,\cdots,K\right)
\tag{7}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Theorem 3 를 다시 보면, \((7)\) 의 결과가 multiset 에 대해 injective 해야 maximally powerful GNN 을 만들 수 있습니다. Lemma 5 를 통해 multiset 에 대해 unique 한 summation 이 존재하기 때문에, 다음과 같이 각 layer 의 graph representation 을 정의하면, \(h_G\) 는 multiset 에 대해 injective 하게 됩니다.&lt;/p&gt;

\[\text{READOUT} \left(\left\{\!\!\left\{h_v^{(k)} \right\}\!\!\right\}\right) = \sum_{v\in V} f^{(k)}\left(h_v^{(k)}\right)\]

&lt;p&gt;따라서, graph classification 에서도 GIN 이 maximally powerful 하다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;논문에서는 node 의 input feature space \(\chi\) 가 countable 인 상황만 고려했지만, 실제로 그래프의 input data 가 countable space 라고 보장할 수 없습니다. \(\chi\) 가 \(\mathbb{R}^n\) 과 같이 continuous space 일 때에 대한 이론적인 연구가 필요해보입니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;less-powerful-but-still-interesting-gnns&quot;&gt;Less Powerful But Still Interesting GNNs&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;논문에서는 \((6)\) 의 두 가지 특징, MLP 와 feature vector summation 에 대한 ablation study 를 보여줍니다.&lt;/p&gt;

&lt;p&gt;다음의 두 가지 변화를 주면, 모델의 성능이 떨어짐을 확인합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;MLP 대신 1-layer perceptron&lt;/li&gt;
  &lt;li&gt;Summation 대신 mean-pooling 또는 max-pooling&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1-layer-perceptrons-instead-of-mlps&quot;&gt;1-Layer Perceptrons instead of MLPs&lt;/h3&gt;

&lt;p&gt;GCN 의 layer-wise propagation rule 은 다음과 같습니다.&lt;/p&gt;

\[h_v^{(k)} = \text{ReLU}\left( W\cdot\text{MEAN}\left\{\!\!\left\{ h_u^{(k-1)} \,:\, u\in N(v)\cup\{v\}\right\}\!\!\right\}  \right)
\tag{8}\]

&lt;p&gt;\((6)\) 과 비교해보면, \(\text{MLP}\) 대신 1-layer perceptron \(\sigma\circ W\) 를 사용했음을 알 수 있습니다. Universal approximation theorem 은 MLP 에 대해 성립하지만, 일반적으로 1-layer perceptron 에 대해서는 성립하지 않습니다.  다음의 Lemma 7 은 1-layer perceptron 을 사용한 GNN 이 구분하지 못하는 non-isomorphic 그래프들이 존재함을 보여줍니다. 즉, 1-layer perceptron 으로는 충분하지 않다는 뜻입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Lemma 7.&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;There exist finite multisets \(X_1\neq X_2\) so that for any linear mapping \(W\),&lt;/p&gt;

\[\sum_{x\in X_1} \text{ReLU}(Wx) = \sum_{x\in X_2} \text{ReLU}(Wx)\]
&lt;/blockquote&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;mean--max-pooling-instead-of-summation&quot;&gt;Mean / Max-Pooling instead of Summation&lt;/h3&gt;

&lt;p&gt;Aggregator \(h\) 를 사용한 GraphSAGE 의 layer-wise propagation rule 은 다음과 같습니다 [4].&lt;/p&gt;

\[h_v^{(k)} = \text{ReLU}\left( W\cdot \text{CONCAT}\left( h_v^{(k-1)}, h\left( \left\{\!\!\left\{ h_u^{(k-1)} \,:\, u\in N(v) \right\}\!\!\right\} \right) \right) \right)\]

&lt;p&gt;Max-pooling 과 mean-pooling 의 경우 aggregator \(h\) 는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt; 
\(\begin{align}
&amp;amp; h_{max}\left( \left\{\!\!\left\{ h_u^{(k-1)} \,:\, u\in N(v) \right\}\!\!\right\} \right) 
= \text{MAX}\left( \left\{\!\!\left\{ f\left(h_u^{(k-1)}\right) \,:\, u\in N(v) \right\}\!\!\right\}  \right) \\
\\
&amp;amp; h_{mean}\left( \left\{\!\!\left\{ h_u^{(k-1)} \,:\, u\in N(v) \right\}\!\!\right\} \right) 
= \text{MEAN}\left( \left\{\!\!\left\{f\left(h_u^{(k-1)}\right) \,:\, u\in N(v) \right\}\!\!\right\}  \right)
\tag{9}
\end{align}\)&lt;/p&gt;

&lt;p&gt;여기서 \(f(x) = \text{ReLU}\left(Wx\right)\), \(\text{MAX}\) 와 \(\text{MEAN}\) 은 element-wise max 와 mean operator 입니다.&lt;/p&gt;

&lt;p&gt;\(h_{max}\) 와 \(h_{mean}\) 모두 multiset 에 대해 정의되며, permutation invariant 하기 때문에, aggregator 로써 역할을 잘 수행합니다. 하지만, 두 함수 모두 multiset 에 대해 injective 하지 않습니다. 다음의 예시를 통해 확인해보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/How-Powerful-are-Graph-Neural-Networks/eg.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Figure 3 에서 node 의 색은 feature vector 를 의미합니다. 즉 같은 색을 가지면, 같은 feature vector 를 가집니다. 위에서 정의된 \(f\) 에 대해, \(f(red) &amp;gt; f(blue)&amp;gt;f(green)\) 을 만족한다고 가정하겠습니다. Figure 3-(a) 를 보면 non-isomorphic 한 두 그래프 모두 \(h_{max}\) 와 \(h_{mean}\) 의 결과가 \(f(blue)\) 로 같습니다. Figure 3-(c) 도 마찬가지로 non-isomorphic 한 두 그래프 모두 \(h_{max}=f(red)\), \(h_{mean}=\frac{1}{2}(f(red)+f(green))\) 으로 결과가 같습니다. Figure 3-(b) 의 경우 \(h_{mean}\) 은 값이 다르지만, \(h_{max}\) 의 값은 같습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;\((6)\) 에서는 다음과 같이 aggregator \(h\) 를 summation 으로 정의합니다.&lt;/p&gt;

\[h_{sum}\left( \left\{\!\!\left\{ h_u^{(k-1)} \,:\, u\in N(v) \right\}\!\!\right\} \right) 
= \sum_{u\in N(v)}f\left(h_u^{(k-1)}\right)\]

&lt;p&gt;\(h_{sum}\) 이 multiset 전체를 injective 하게 표현할 수 있고, \(h_{mean}\) 의 경우 multiset 의 distribution 을, \(h_{max}\) 의 경우 multiset 의 서로다른 원소들로 이루어진 set 을 표현할 수 있다고 설명합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/How-Powerful-are-Graph-Neural-Networks/rank.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;따라서, max-pooling 과 mean-pooling 을 사용한 GraphSAGE 같은 경우 GIN 보다 representation power 가 떨어진다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;experiment--result&quot;&gt;Experiment &amp;amp; Result&lt;/h2&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;논문에서는 GIN 과 다른 GNN 들의 graph classification 성능을 비교하기 위해, 4개의 bioinformatics datasets (MUTAG, PTC, NCI1, PROTEINS) 와 5개의 social network datasets (COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI5K) 에 대해 실험을 수행했습니다.&lt;/p&gt;

&lt;p&gt;GIN 모델로 \((6)\) 에서 \(\epsilon\) 을 학습하는 GIN-\(\epsilon\) 과, \(\epsilon\) 을 0 으로 고정한 GIN-0 를 선택했습니다. GIN 과 비교하기 위해 \((6)\) 의 summation 을 \((9)\) 와 같이 mean-pooling 또는 max-pooling 으로 바꾸거나, MLP 를 1-layer perceptron 으로 바꾼 모델들 (Figure 4 의 Mean - 1-layer 와 같은 variant 들을 의미합니다.) 을 실험 대상으로 선정했습니다.&lt;/p&gt;

&lt;p&gt;Baseline 모델로는 graph classification 의 state-of-the-art 성능을 보여주는 WL subtree kernel, C-SVM, Diffusion-convolutional neural network (DCNN), PATCHY-SAN, Deep Graph CNN (DGCNN), 그리고 Anonymous Walk Embeddings (AWL) 을 사용했습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/How-Powerful-are-Graph-Neural-Networks/train.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;먼저 representational power 를 확인하기 위해 GNN 들의 training accuracy 들을 비교합니다. 모델의 representational power 가 높다면, training set 에서의 accuracy 또한 높아져야합니다. Figure 4 를 보면 GIN-\(\epsilon\) 과 GIN-0 모두 training accuracy 가 거의 1 에 수렴하는 것을 볼 수 있습니다. GIN-\(\epsilon\) 의 경우 각 layer 의 parameter \(\epsilon^{(k)}\) 또한 학습하지만, GIN-0 와 큰 차이를 보이지는 않습니다. Figure 4 에서 1-layer perceptron 보다는 MLP 를 사용했을 때, mean / max-pooling 보다는 summation 을 사용했을 때 정확도가 대체로 더 높게 나타납니다.&lt;/p&gt;

&lt;p&gt;하지만 모든 GNN 모델들은 WL subtree kernel 의 정확도보다 낮은 것이 보입니다. Lemma 2 에서 설명했듯이, neighborhood aggregation scheme 을 사용하는 GNN 은 WL test 의 representational power 를 뛰어 넘을수 없다는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/How-Powerful-are-Graph-Neural-Networks/test.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Table 1 은 test set 에 대한 classification accuracy 를 보여줍니다. GIN 모델, 특히 GIN-0 모델의 성능이 가장 뛰어나다는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2019). &lt;a href=&quot;https://arxiv.org/pdf/1810.00826.pdf&quot;&gt;How powerful are graph neural networks?&lt;/a&gt; In
International Conference on Learning Representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). &lt;a href=&quot;https://arxiv.org/pdf/1704.01212.pdf&quot;&gt;Neural
message passing for quantum chemistry&lt;/a&gt;. In International Conference on Machine Learning, pages 1263–1272.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. &lt;a href=&quot;https://arxiv.org/pdf/1905.12560.pdf&quot;&gt;On the equivalence between graph isomorphism testing and function approximation with GNNs&lt;/a&gt;. In Advances in Neural Information Processing Systems, pages 15868–15876, 2019.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;William L Hamilton, Rex Ying, and Jure Leskovec. &lt;a href=&quot;https://arxiv.org/pdf/1706.02216.pdf&quot;&gt;Inductive representation learning on large graphs&lt;/a&gt;.
In Advances in Neural Information Processing Systems (NIPS), pp. 1025–1035, 2017a.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;paper review&quot;]" /><category term="Analysis" /><category term="Weisfeiler-Lehman" /><summary type="html">[Paper review] Graph Isomorphism Network 이해하기</summary></entry><entry><title type="html">Invariant and Equivariant Graph Networks</title><link href="https://harryjo97.github.io/paper%20review/Invariant-and-Equivariant-Graph-Networks/" rel="alternate" type="text/html" title="Invariant and Equivariant Graph Networks" /><published>2021-01-19T11:00:00+09:00</published><updated>2021-01-19T11:00:00+09:00</updated><id>https://harryjo97.github.io/paper%20review/Invariant-and-Equivariant-Graph-Networks</id><content type="html" xml:base="https://harryjo97.github.io/paper%20review/Invariant-and-Equivariant-Graph-Networks/">&lt;p&gt;[paper review] : Invariant and Equivariant Graph Networks&lt;/p&gt;

&lt;h2 id=&quot;motive&quot;&gt;Motive&lt;/h2&gt;

&lt;h3 id=&quot;translation-invariance&quot;&gt;Translation Invariance&lt;/h3&gt;

&lt;p&gt;CNN 의 translation invariant 한 특성은 이미지를 학습하는 데 큰 장점이 됩니다. 이와 같이 translation invariant 한 모델을 만드는 방법으로  Multi-layer perceptron (MLP) 이 있습니다. MLP 를 통해 임의의 연속 함수를 근사할 수 있기 때문에, translation invariant 한 함수 \(f\) 를 모델링하는 MLP 를 만들 수 있습니다. MLP 의 기본적인 형태는 non-linear function \(\sigma\) 와 linear function \(L(x) = Ax+b\) 로 구성된 layer 들로 이루어고, 각 layer 는 \(\mathcal{L}(x) = \sigma(L(x))\) 의 형태를 가집니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Invariant-and-Equivariant-Graph-Networks/mlp.PNG&quot; style=&quot;max-width: 70%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;일반적인 MLP 의 경우 input size 가 커질수록, depth 가 깊어질수록 parameter 의 수가 감당할 수 없을 정도로 커집니다. Parameter 의 수를 줄이고 translation invariant 속성을 유지하기 위해서, \(L(x) = Ax+b\) 대신 transform invariant 한 linear operator 를 사용합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Invariant-and-Equivariant-Graph-Networks/mlp-invariant.PNG&quot; style=&quot;max-width: 70%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;하지만 이미지의 한 픽셀은 다른 임의의 픽셀로 보내지는 translation 을 찾을 수 있기 때문에, transform invariant operator 는 각 픽셀들의 value 를 모두 더해주는 sum operator 와 일치합니다. 모든 픽셀 value 를 더해주는 것은 이미지의 세부 정보를 무시하기에, 의미 있는 operator 라 할 수 없습니다.&lt;/p&gt;

&lt;p&gt;이 때 transform invariant operator 대신, CNN 의 convolution 과 같이 translation equivariant linear operator 를 사용할 수 있습니다. MLP \(m\), invariant linear layer \(h\), non-linear activation \(\sigma\) 와 equivariant linear layer \(L_i\) 들을 통해, 다음과 같이 invariant function \(f\) 를 만들 수 있습니다.&lt;/p&gt;

\[f = m\circ h\circ L_k\circ\sigma\circ \cdots \circ\sigma\circ L_1 
\tag{1}\]

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Invariant-and-Equivariant-Graph-Networks/mlp-equivariant.PNG&quot; style=&quot;max-width: 70%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;\((1)\) 에서 적절한 layer 들의 선택으로 다양한 invariant model 을 얻을 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;invariant-graph-networks&quot;&gt;Invariant Graph Networks&lt;/h3&gt;

&lt;p&gt;이미지에서 translation invariant function 을 통해 feature 를 학습하는 것과 같이, 그래프에서는 permutation invariant function 을 통해 node representation 을 학습할 수 있습니다. 위와 같이 \((1)\) 을 통해 permutation invariant 한 function 을 만들어 낼 수 있고, 이 모델을 Invariant Graph Network (IGN) 라 부릅니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;graphs-as-tensors&quot;&gt;Graphs as Tensors&lt;/h2&gt;

&lt;p&gt;\(n\) 개의 node 를 가진 그래프를 생각해봅시다. 각 node \(i\) 마다 value \(x_i\in\mathbb{R}\) 를 가지고 각 edge \((i,j)\) 마다 value \(x_{ij}\in\mathbb{R}\) 를 가진다면,  이는 \(X\in\mathbb{R}^{n\times n}\) tensor 를 통해 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[X_{ij} = \begin{cases}
x_i &amp;amp;\mbox{ if }\; i=j \\
x_{ij} &amp;amp; \mbox{ if }\; i\neq j
\end{cases}\]

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Invariant-and-Equivariant-Graph-Networks/2-tensor.PNG&quot; style=&quot;max-width: 50%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이 표현법을 hypergraph 에 대해서도 일반화할 수 있습니다. \(n\) 개의 node 를 가지는 hypergraph 에 대해, 각 hyper-edge 는  \((i_1,\cdots,i_k)\in [n]^k\)  의 형태로 나타낼 수 있습니다. 따라서 \((1)\) 과 마찬가지로, hypergraph 또한 \(X\in\mathbb{R}^{n^k}\) tensor 로 표현할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Invariant-and-Equivariant-Graph-Networks/k-tensor.PNG&quot; style=&quot;max-width: 50%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;만약 hyper-edge 들이 \(a\) 차원의 value 를 가진다면, \(X\in\mathbb{R}^{n^k\times a}\)  tensor 로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;permutation-invariance--equivariance&quot;&gt;Permutation Invariance &amp;amp; Equivariance&lt;/h2&gt;

&lt;p&gt;이미지에서의 translation 은 symmetry 의 한 종류입니다. 그래프에 있어 symmetry 는 node 순서의 재배열 (re-ordering) 을 통해 해석할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Invariant-and-Equivariant-Graph-Networks/symmetry.PNG&quot; style=&quot;max-width: 50%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;permutation-of-tensors&quot;&gt;Permutation of Tensors&lt;/h3&gt;

&lt;p&gt;그래프 node 가 재배열되면, 그에 따라 그래프 tensor 또한 변하게 됩니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Invariant-and-Equivariant-Graph-Networks/permutation.PNG&quot; style=&quot;max-width: 70%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Permutation \(p\in S_n\) 와 \(k\)-tensor \(X\in\mathbb{R}^{n^k}\) 에 대해, \(X\) 에 대한 permutation \(p\) 는 각 hyper-edge \((i_1,\cdots,i_k)\in [n]^k\) 에 대해 다음과 같이 쓸수 있습니다.&lt;/p&gt;

\[(p\cdot X)_{i_1,\cdots,i_k} = X_{p^{-1}(i_1),\cdots,p^{-1}(i_k)}
\tag{2}\]

&lt;h3 id=&quot;permutation-invariant&quot;&gt;Permutation Invariant&lt;/h3&gt;

&lt;p&gt;함수 \(f\) 가 permutation invariant 하다는 것은, input element 들의 순서와 상관 없이 output 이 같다는 뜻입니다. \(f\) 의 input 이 tensor 일 경우, permutation invariant 는 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[f(p\cdot A) = f(A)
\tag{3}\]

&lt;h3 id=&quot;permutation-equivariant&quot;&gt;Permutation Equivariant&lt;/h3&gt;

&lt;p&gt;함수 \(f\) 가 permutation equivariant 하다는 것은, 임의의 permutation \(p\) 에 대해 \(p\) 와 \(f\)  가 commute 함을 의미합니다. \(f\) 의 input 이 tensor 일 경우, permutation equivariant 는 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[f(p\cdot A) = p\cdot f(A)
\tag{4}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;

&lt;p&gt;논문에 나오는 notation 들을 정리하면 다음과 같습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; notation &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;center&gt;&lt;/center&gt;&lt;/center&gt;&lt;/th&gt;
      &lt;th&gt;&lt;center&gt; explanation &lt;center&gt;&lt;/center&gt;&lt;/center&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\([\,\cdot\,]\)&lt;/td&gt;
      &lt;td&gt;\([n]=\{1,\cdots,n\}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(\text{vec}(\,\cdot\,)\)&lt;/td&gt;
      &lt;td&gt;\(X\) 의 column 들을 쌓아 만든 행렬; \(\mathbb{R}^{a\times b}\) matrix \(X\) 에 대해 \(\text{vec}(X)\in\mathbb{R}^{ab\times 1}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\([\text{vec}(\,\cdot\,)]\)&lt;/td&gt;
      &lt;td&gt;\([\text{vec}(X)]=X\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(\otimes\)&lt;/td&gt;
      &lt;td&gt;Kronecker product&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(P^{\otimes k}\)&lt;/td&gt;
      &lt;td&gt;\(\overbrace{P\otimes \cdots \otimes P}^{k}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(b(l)\)&lt;/td&gt;
      &lt;td&gt;\(l\) 번째 bell number&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;fixed-point-equations&quot;&gt;Fixed-Point Equations&lt;/h2&gt;

&lt;p&gt;먼저 permutation invariant linear operator 에 대해 알아보겠습니다. 일반적인 linear operator \(L:\mathbb{R}^{n^k}\rightarrow\mathbb{R}\) 을 \(\mathbb{R}^{1\times n^k}\) matrix \(\mathbf{L}\) 로 나타낼 때, \(L\) 이 permutation invaraint 하다면 임의의 permutation \(p\in S_n\) 에 대해 다음을 만족해야합니다.&lt;/p&gt;

\[\mathbf{L}\text{vec}(p\cdot A) = \mathbf{L}\text{vec}(A)
\tag{5}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Permutation \(p\in S_n\) 를 나타내는 matrix \(P\) 에 대해 \(\text{vec}(p\cdot A) = P^{T\otimes k}\text{vec}(A)\) 이기 때문에, \((5)\) 는 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

\[P^{T\otimes k}\mathbf{L}\text{vec}(A) = \mathbf{L}\text{vec}(A)
\tag{6}\]

&lt;p&gt;\((6)\) 은 모든 \(A\in\mathbb{R}^{n^k}\) 에 대해 성립해야하기 때문에,&lt;/p&gt;

\[P^{T\otimes k}\mathbf{L} = \mathbf{L}
\tag{7}\]

&lt;p&gt;\((7)\) 의 양변에 transpose 를 취하면, 다음의 fixed-point equation 을 얻을 수 있습니다.&lt;/p&gt;

\[P^{\otimes k}\text{vec}(\mathbf{L}) = \text{vec}(\mathbf{L})
\tag{8}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;이제 permutation equivariant linear operator 에 대해 알아보겠습니다. 일반적인 linear operator \(L:\mathbb{R}^{n^k}\rightarrow\mathbb{R}^{n^k}\) 을 \(\mathbb{R}^{n^k\times n^k}\) matrix \(\mathbf{L}\) 로 나타낼 때, \(L\) 이 permutaion equivariant 하다면 임의의 permutation \(p\in S_n\) 에 대해 다음을 만족해야합니다.&lt;/p&gt;

\[[\mathbf{L}\text{vec}(p\cdot A)] = p\cdot[\mathbf{L}\text{vec}(A)]
\tag{9}\]

&lt;p&gt;양변에 \(\text{vec}(\cdot)\) 을 취하고 \(\text{vec}(p\cdot A) = P^{T\otimes k}\text{vec}(A)\) 을 이용하면, \((9)\) 를 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

\[\mathbf{L}P^{T\otimes k}\text{vec}(A) = P^{T\otimes k}\mathbf{L}\text{vec}(A)
\tag{10}\]

&lt;p&gt;\((10)\) 은 모든 \(A\in\mathbb{R}^{n^k}\) 에 대해 성립해야하기 하며 \(P^{T\otimes k}\) 의 역행렬이 \(P^{\otimes k}\) 이므로,&lt;/p&gt;

\[P^{\otimes k}\mathbf{L}P^{T\otimes k} = \mathbf{L}
\tag{11}\]

&lt;p&gt;\((11)\) 의 양변에 \(\text{vec}(\cdot)\) 을 취하고 Kronecker product 의 성질인 \(\text{vec}(XAY) = Y^T\otimes X\text{vec}(A)\) 을 사용하면 다음의 fixed-point equation 을 얻을 수 있습니다.&lt;/p&gt;

\[P^{\otimes 2k}\text{vec}(\mathbf{L}) = \text{vec}(\mathbf{L})
\tag{12}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;solving-the-fixed-point-equations&quot;&gt;Solving the Fixed-Point Equations&lt;/h2&gt;

&lt;p&gt;모든 permutation invaraint  / equivariant 한 linear operator 들을 찾아내는 것은, \((8)\) 과 \((12)\) 의 해를 구하는 것과 같습니다. 즉 다음과 같은 fixed-point equation 의 해 \(X\in\mathbb{R}^{n^l}\) 를 구해야합니다.&lt;/p&gt;

\[P^{\otimes l}\text{vec}(X) = \text{vec}(X)
\tag{13}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;이 때, \(P^{\otimes l}\text{vec}(X)=\text{vec}(p^{-1}\cdot X)\) 이므로, \((13)\) 은 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[q\cdot X = X \;\;\text{for all permutation} \;\; q\in S_{n}
\tag{14}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;\((14)\) 에 대한 solution space 의 basis 를 특별한 equivalence relation 을 통해 표현하려고 합니다. \(a,\, b\in\mathbb{R}^l\) 에 대해  equivalence relation \(\sim\) 을 다음과 같이 정의하겠습니다.&lt;/p&gt;

\[a\sim b \;\text{ iff }\; a_i=a_j \Leftrightarrow b_i=b_j \;\forall i,j\in [\,l\,]\]

&lt;p&gt;\(a\in\mathbb{R}^l\) 에 대해 \(a_i\) 값이 같은 index \(i\) 들로 \([\,l\,]\) 을 분할한 집합을 \(S_a\) 라고 한다면, \(a\sim b\) 임은 \(S_a=S_b\) 와 동치입니다. 따라서, equivalence classes 들은 \([\,l\,]\) 의 분할과 일대일 대응됩니다. 예를 들어, \(l=2\) 라면 equivalence class 는 \(\{a\in\mathbb{R}^2: a_1=a_2\}\) 와 \(\{a\in\mathbb{R}^2: a_1\neq a_2\}\) 두 개 뿐입니다. 이 때 \(\{a\in\mathbb{R}^2: a_1=a_2\}\) 는 \(\{ \{1,2\} \}\) 와, \(\{a\in\mathbb{R}^2: a_1=a_2\}\) 는 \(\{\{1\},\{2\}\}\) 와 대응됩니다. 일대일 대응에 의해, equivalence class 는 총 \(b(l)\) 개 존재합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;결론적으로, \(X\) 가 \((14)\) 의 해인 것과 \(X\) 가 각각의 equivalence class 내에서 상수라는 것이 동치입니다. 증명은 다음과 같습니다. 먼저 \(X\) 가 각 equivalence class 내에서 상수라고 가정하겠습니다. 임의의 permutation \(q\in S_n\) 와 \(a\mathbb{R}^l\) 에 대해, \(a_i=a_j \Leftrightarrow q(a_i)=q(a_j)\) 이므로 \(a\sim q(a)\) 이고, 가정에 의해 \(X_a=X_{q(a)}\) 를 만족합니다. \((14)\) 에서 양변의 \(a\in\mathbb{R}^l\) 성분을 비교하면 \(X_a=X_{q(a)}\) 이므로, \(X\) 는 \((14)\) 의 해입니다.&lt;/p&gt;

&lt;p&gt;반대로, \(X\) 가 \((14)\) 의 해라고 가정하겠습니다. 만약, \(a\sim b\) 라면 permutation \(q\) 가 존재해 \(b=q(a)\) 를 만족합니다. 이 때 \(X_a\neq X_b\) 라면, \(X\) 가 \((14)\) 의 해라는 것에 모순이므로, \(X\) 가 각 equivalence class 내에서 상수여야 합니다.&lt;/p&gt;

&lt;p&gt;이제 각 equivalence class \(\gamma\in [n]^l/\sim\) 에 대해 tensor \(B^{\gamma}\in\mathbb{R}^l\) 을 다음과 같이 정의하겠습니다.&lt;/p&gt;

\[B^{\gamma}_a = \begin{cases}
1 &amp;amp;\mbox{ if }\; a\in\gamma \\
0 &amp;amp;\mbox{ otherwise}
\end{cases}
\tag{15}\]

&lt;p&gt;\((14)\) 의 해 \(X\) 에 대해 \(X\) 가 각 equivalence class 내에서 상수여야 하므로, \(X\) 는 \(B^{\gamma}\) 들의 linear combination 으로  표현할 수 있습니다. 또한 \(B^{\gamma}\) 들의 support 는 disjoint 하므로, orthogonal 합니다. 따라서 \(B^{\gamma}\) 들은 \((14)\) 에 대한 solution space 의 orthogonal basis 를 이룹니다. 이 때, equivalence class 는 총 \(b(l)\) 개 존재하므로 solution space 는 \(b(l)\) 차원입니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;\(n=5\), \(k=2\) 일 때 permutation equivariant linear operator 공간의 orthogonal basis 는 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Invariant-and-Equivariant-Graph-Networks/basis.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;특히 \(b(2k)=b(4)=15\) 이므로 총 15개의 basis element 가 존재한다는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y. (2019). &lt;a href=&quot;https://arxiv.org/pdf/1812.09902.pdf&quot;&gt;Invariant and equivariant graph
networks&lt;/a&gt;. In International Conference on Learning Representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017).
&lt;a href=&quot;https://arxiv.org/abs/1703.06114&quot;&gt;Deep sets&lt;/a&gt;. In Advances in Neural Information Processing Systems, pages 3391–3401.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;H. Maron, H. Ben-Hamu, H. Serviansky, and Y. Lipman. &lt;a href=&quot;https://arxiv.org/pdf/1905.11136.pdf&quot;&gt;Provably Powerful Graph Networks&lt;/a&gt;. In Neural
Information Processing Systems (NeurIPS), pages 2153–2164, 2019.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep Learning of Irregular Data &lt;a href=&quot;http://irregulardeep.org/An-introduction-to-Invariant-Graph-Networks-(1-2)/&quot;&gt;An Introduction To Invariant Graph Networks (1:2)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;paper review&quot;]" /><category term="Analysis" /><summary type="html">[paper review] : Invariant and Equivariant Graph Networks</summary></entry><entry><title type="html">Semi-Supervised Classification with Graph Convolutional Networks</title><link href="https://harryjo97.github.io/paper%20review/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/" rel="alternate" type="text/html" title="Semi-Supervised Classification with Graph Convolutional Networks" /><published>2021-01-13T20:00:00+09:00</published><updated>2021-01-13T20:00:00+09:00</updated><id>https://harryjo97.github.io/paper%20review/Semi-Supervised-Classification-with-Graph-Convolutional-Networks</id><content type="html" xml:base="https://harryjo97.github.io/paper%20review/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/">&lt;p&gt;[paper review] : Graph Convolutional Network 이해하기&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;논문에서 해결하고자 하는 문제는 다음과 같습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Classifying nodes in a graph where labels are only available for a small subset of nodes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉 그래프의 node 들 중 label 이 주어진 node 들의 수가 적은 상황에서 출발합니다.&lt;/p&gt;

&lt;p&gt;Graph Convolution Network 를 사용하기 이전에는, 주로 explicit graph-based regularization &lt;a href=&quot;https://www.aaai.org/Papers/ICML/2003/ICML03-118.pdf&quot;&gt;(Zhu et al., 2003)&lt;/a&gt; 을 이용하여 문제에 접근하였습니다. 이 방법은 supervised loss \(\mathcal{L}_0\) 에 graph Laplacian regularization 항 \(\mathcal{L}_{reg}\) 을 더한 loss function 을 학습에 사용합니다. Neural network 와 같이 differentiable 함수 \(f\) 와 feature vector matrix  \(X\), 그리고 unnormalized graph Laplacian \(L\) 로 Laplacian regularization 을 다음과 같이 정의합니다.&lt;/p&gt;

\[\mathcal{L} = \mathcal{L}_0 + \lambda\mathcal{L}_{reg},\;\;\; \mathcal{L}_{reg} = f(X)^T\,L\,f(X)
\tag{1}\]

&lt;p&gt;\(\mathcal{L}_{reg}\) 를 자세히 들여다보면, 그래프의 adjacency matrix \(A\) 에 대해 다음을 만족합니다.&lt;/p&gt;

\[f(X)^T\;L\;f(X) = \sum_{i,j} A_{ij} \|f(X_i)-f(X_j)\|^2\]

&lt;p&gt;\(\mathcal{L}_{reg}\) 의 값이 작다는 것은 곧 인접한 두 node 의 feature 가 비슷하다는 뜻입니다. 이와 같이 explicit graph-based regularization 은 그래프의 인접한 node 들은 비슷한 feature 를 가질 것이라는 가정을 전제로 하기 때문에, 일반적인 상황에서 제약을 받습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 Explicit graph-based regularization 을 사용하지 않기 위해 그래프의 구조를 포함하는 neural network model \(f(X,A)\) 를 제시합니다.  [1] 에서 제시된 spectral convolution 과 [3] 의 truncated Chebyshev expansion 을 사용한 ChebyNet 을 발전시킨 Graph Convolutional Network (GCN) 을 통해 semi-supervised node classification 을 해결합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;fast-approximate-convolutions-on-graphs&quot;&gt;Fast Approximate Convolutions On Graphs&lt;/h2&gt;

&lt;h3 id=&quot;spectral-graph-convolution&quot;&gt;Spectral Graph Convolution&lt;/h3&gt;

&lt;p&gt;Graph signal \(x\in\mathbb{R}^N\) 와 filter \(g_{\theta}=\text{diag}(\theta)\) 에 대해 spectral convolution 은 다음과 같이 정의됩니다 [1, 2].&lt;/p&gt;

\[g_{\theta}\ast x = Ug_{\theta}U^Tx
\tag{2}\]

&lt;p&gt;여기서 \(U\)  는 normalized graph Laplacian \(L = I - D^{-1/2}AD^{-1/2}\) 의 eigenvector 로 이루어진 Fourier basis 이고, \(L=U\Lambda U^T\) 로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Filter \(g_{\theta}\) 는 다음과 같이 \(L\) 의 eigenvalue 들의 함수로 생각할 수 있습니다 [3].&lt;/p&gt;

\[g_{\theta}(\Lambda) =
\begin{bmatrix}
g_{\theta}(\lambda_0) &amp;amp; &amp;amp; &amp;amp; \\
 &amp;amp; g_{\theta}(\lambda_1) &amp;amp; &amp;amp; \\
  &amp;amp; &amp;amp; \ddots &amp;amp; \\
  &amp;amp; &amp;amp; &amp;amp; g_{\theta}(\lambda_{N-1})
\end{bmatrix}\]

&lt;p&gt;\((2)\) 을 계산하기 위해서는 \(U\) 의 matrix multiplication 을 수행해야하며, 이는 \(O(N^2)\) 으로 상당히 복잡한 연산입니다. 또한 \(U\) 를 구하기 위한 eigendecomposition 은 복잡도가 \(O(N^3)\) 이므로, node 의 개수가 수천 수만개인 그래프에 대해서 \((2)\) 를 계산하는 것은 굉장히 힘듭니다.&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해, truncated Chebyshev expansion 을 통해 \(g_{\theta}(\Lambda)\) 를 다음과 같이 근사합니다 [3, 5].&lt;/p&gt;

\[g_{\theta'}(\Lambda) \approx \sum^K_{k=0} \theta'_{k}T_k(\tilde{\Lambda})
\tag{3}\]

&lt;p&gt;여기서 \(\tilde{\Lambda} = \frac{2}{\lambda_{max}}\Lambda - I\) 로 정의하고, \(L\) 의 가장 큰 eigenvalue \(\lambda_{max}\) 를 사용해 Chebyshev expansion 을 위해 \(\Lambda\) 를  scaling 해준 것입니다.&lt;/p&gt;

&lt;p&gt;\((3)\) 의 근사를 \((2)\) 에 대입하면, \(\tilde{L} = \frac{2}{\lambda_{max}}L - I\) 에 대해 다음의 결과를 얻을 수 있습니다.&lt;/p&gt;

\[g_{\theta'}\ast x \approx \sum^K_{k=0} \theta'_kT_k(\tilde{L})x = y
\tag{4}\]

&lt;p&gt;\((4)\) 의 결과가 특별한 이유는 각 node 에 대해 localized 되어 있기 때문입니다. 우선 graph Laplacian \(L\) 은 다음과 같이 localization 특성을 가집니다 [5].&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;\(\left(L^s\right)_{ij}\) 는 그래프의 두 node \(i\) 와 \(j\) 를 연결하는 path 들 중 길이가 \(s\) 이하인 path 들의 개수와 일치한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;\((4)\) 에서 \(L\) 의 \(K\)-th power 까지만 존재하기 때문에, \(y(i)\) 는 \(i\) 의 \(K\)-th order neighborhood signal 들의 합으로 표현할 수 있습니다. 따라서 \((4)\) 의 근사는 \(K\)-localized 됨을 확인할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;layer-wise-linear-model&quot;&gt;Layer-wise Linear Model&lt;/h3&gt;

&lt;p&gt;\((4)\) 에서 \(K\) 가 클수록 더 많은 종류의 convolutional filter 를 얻을 수 있지만, 그만큼 계산이 복잡해지며 overfitting 의 가능성도 커집니다. 여러개의 convolutional layer 를 쌓아 deep model 을 만든다면, \(K\) 가 작아도 다양한 종류의 convolutional filter 를 표현할 수 있습니다. 특히 overfitting 의 가능성을 덜 수 있고, 한정된 자원에 대해서 $K$ 가 클 때보다 더 깊은 모델을 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 극단적으로 \(K=1\) 로 제한을 두었습니다. 또한 normalized graph Laplacian 의 eigenvalue 들은 \([0,2]\) 구간에 속하기 때문에 [6], \(\lambda_{max}\approx 2\) 로 근사합니다.이 경우 \((4)\) 는 다음과 같이 두 개의 parameter \(\theta'_0\) 와 \(\theta'_1\) 을 통해 표현할 수 있습니다.&lt;/p&gt;

\[g_{\theta'}\ast x \approx \theta'_0x + \theta'_1(L-I)x = \theta'_0x - \theta'_1D^{-1/2}AD^{-1/2}x\]

&lt;p&gt;더 나아가, 계산을 줄이기 위해 하나의 parameter \(\theta = \theta'_0 = -\theta'_1\) 만을 사용한다면, 다음과 같은 간단한 결과를 얻게됩니다.&lt;/p&gt;

\[g_{\theta}\ast x \approx \theta(I + D^{-1/2}AD^{-1/2})x
\tag{5}\]

&lt;p&gt;\(M = I + D^{-1/2}AD^{-1/2}\) 의 eigenvalue 는 \([0,2]\) 에 속합니다 [Appendix A]. 그렇기 때문에, \((5)\) 를 사용한 layer 를 여러개 쌓아 deep model 을 만든다면 exploding / vanishing gradient problem 과 같이 불안정한 학습이 이루어질 수 있습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 이를 해결하기 위해 renormalization trick 을 사용합니다. \(\tilde{A} = A + I\) 와 \(\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}\)  에 대해, \((5)\) 에서 \(I + D^{-1/2}AD^{-1/2}\) 대신 \(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\) 를 이용해 다음과 같이 convolutional filter 를 정의합니다 [Appendix B].&lt;/p&gt;

\[g_{\theta}\ast x \approx \theta\, \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2} x
\tag{6}\]

&lt;p&gt;\((6)\) 의 결과는 각 node 가 1차원의 feature 를 가질 때로 한정되어 있습니다. 이제 각 node 마다 \(C\) 차원의 feature vector 를 가지는 상황을 고려하겠습니다. 주어진 signal \(X\in\mathbb{R}^{N\times C}\) 와 \(F\) 개의 feature map 에 대해서 \((6)\) 을 다음과 같이 일반화할 수 있습니다 [Appendix C].&lt;/p&gt;

\[Z = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}X\Theta
\tag{7}\]

&lt;p&gt;여기서 \(\Theta\in\mathbb{R}^{C\times F}\) 는 filter의 parameter matrix 이고 \(Z\in\mathbb{R}^{N\times F}\) 가 filtering 의 결과입니다. 특히 \(\Theta\) 는 그래프의 모든 node 들에 대해 동일하게 사용되기 때문에, CNN 의 filter 와 같이 weight-sharing 의 관점에서 큰 의미가 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;\((7)\) 을 사용해 muli-layer GCN 의 layer-wise propagation rule 을 정의할 수 있습니다. \(l\) 번째 layer 와 \(l+1\) 번째 layer 의 activation 을 다음과 같이 쓰면,&lt;/p&gt;

\[H^{(l)}\in\mathbb{R}^{N\times C_l}\, , \;\; H^{(l+1)}\in\mathbb{R}^{N\times C_{l+1}}\]

&lt;p&gt;trainable weight matrix \(W^{(l)}\in\mathbb{R}^{C_l\times C_{l+1}}\) 와 activation function \(\sigma\) (e.g. ReLU, tanh) 를 사용해 다음과 같이 propagation rule 을 정의할 수 있습니다.&lt;/p&gt;

\[H^{(l+1)} = \sigma\left( \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\,H^{(l)}W^{(l)} \right)
\tag{8}\]

&lt;p&gt;\((8)\) 에서 혼동하지 말아야 점은, 각 layer 들에 대해 그래프의 구조 (node 들과 node 들의 연결 상태) 는 변하지 않고, 각 node 에 주어진 feature vector 의 dimension \(C_l\) 만 변한다는 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/gcn.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;semi-supervised-node-classification&quot;&gt;Semi-Supervised Node Classification&lt;/h2&gt;

&lt;h3 id=&quot;example--two-layer-gcn&quot;&gt;Example : Two-layer GCN&lt;/h3&gt;

&lt;p&gt;\((8)\) 의 propagation rule 을 사용해 node classification 을 위한 two-layer GCN 을 보겠습니다.. 먼저 전처리 단계에서 \(\hat{A} = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\) 를 계산하여 다음과 같이 두 개의 layer 를 가지는 model 을 만들 수 있습니다.&lt;/p&gt;

\[Z = \text{softmax}\left( \hat{A}\;\text{ReLU}\left( \hat{A}XW^{(0)} \right)W^{(1)} \right)
\tag{9}\]

&lt;p&gt;마지막 output layer 에서 activation function 으로 softmax 를 각 행 별로 적용해줍니다. 
Loss function 으로 label 이 있는 node 들에 대해서만 cross-entropy error 를 계산합니다.&lt;/p&gt;

\[\mathcal{L} = -\sum_{l\in\text{labled}}\sum^{\text{output dim}}_{f=1} Y_{lf}\ln Z_{lf}\]

&lt;p&gt;이를 통해 \((9)\) 의 weight matrix \(W^{(0)}\) 와 \(W^{(1)}\) 은 gradient descent 를 통해 업데이트 합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;experiments--results&quot;&gt;Experiments &amp;amp; Results&lt;/h2&gt;

&lt;p&gt;실험 방법 및 데이터에 관해서 더 자세한 설명은 &lt;a href=&quot;https://arxiv.org/pdf/1603.08861.pdf&quot;&gt;Yang et al., 2016&lt;/a&gt; 을 참고하기 바랍니다.&lt;/p&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;논문에서는 크게 네 가지 dataset : Citeseer, Cora, Pubmed, NELL 을 실험에 사용했습니다.&lt;/p&gt;

&lt;p&gt;이들 중 Citeseer, Cora, 그리고 Pubmed  는 citation network dataset 으로, 각 node 는 문서들이며 edge 는 citation link 를 의미합니다.  NELL 은 knowledge graph 에서 추출된 이분 그래프 dataset 으로 relation node 와 entity node 모두 사용했습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/dataset.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;h3 id=&quot;node-classification&quot;&gt;Node Classification&lt;/h3&gt;

&lt;p&gt;각 데이터셋에 대한 baseline method 들과 two-layer GCN 의 classification accuracy 는 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/result1.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;GCN 의 정확도가 다른 baseline method 들에 비해 월등히 높은 것을 볼 수 있습니다. 특히 baseline method 들 중 정확도가 가장 높은  Planetoid 와 비교해, GCN 의 수렴 속도가 훨씬 빠르다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;evaluation-of-propagation-model&quot;&gt;Evaluation of Propagation Model&lt;/h3&gt;

&lt;p&gt;위에서 제시된 다양한 propagation model 들의 performance 를 비교한 결과는 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Semi-Supervised-Classification-with-Graph-Convolutional-Networks/result2.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;\((7)\) 에서 사용한 renormalization trick 이 가장 높은 정확도를 보여줍니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&quot;a--largesest-eigenvalue-of-m&quot;&gt;A.  Largesest Eigenvalue of \(M\)&lt;/h3&gt;

&lt;p&gt;\(M = I + D^{-1/2}AD^{-1/2}\) 가 real symmetric matrix 이기 때문에, Courant-Fischer 정리에 의해 \(M\) 의 가장 큰 eigenvalue \(\mu\) 는 다음을 만족합니다.&lt;/p&gt;

\[\mu = \sup_{\|x\|=1} x^TMx\]

&lt;p&gt;\(L\) 의 정의에 의해 \(M = 2I-L\) 이며 \(L\) 은 positive semi-definite matrix 이기 때문에, \(\|x\|=1\) 를 만족하는 \(x\in\mathbb{R}^N\) 에 대해 다음이 성립합니다.&lt;/p&gt;

\[x^TMx 
=x^T(2I-L)x
= 2 - x^TLx \leq 2\]

&lt;p&gt;따라서,&lt;/p&gt;

\[\mu = \sup_{\|x\|=1} x^TMx \leq 2\]

&lt;h3 id=&quot;b-about-renormalization-trick&quot;&gt;B. About Renormalization Trick&lt;/h3&gt;

&lt;p&gt;\(I + D^{-1/2}AD^{-1/2}\) 와 \(\tilde{D}^{-1/2}\tilde{A}\,\tilde{D}^{-1/2}\) 의 matrix 를 자세히 살펴보면 다음과 같습니다.&lt;/p&gt;

\[I + D^{-1/2}AD^{-1/2} = \begin{cases}
1 &amp;amp; i=j \\
A_{ij}/\sqrt{D_{ii}D_{jj}} &amp;amp; i\neq j
\end{cases}\]

\[\tilde{D}^{-1/2}\tilde{A}\,\tilde{D}^{-1/2} = \begin{cases}
1/(D_{ii}+1) &amp;amp; i=j \\
A_{ij}/\sqrt{(D_{ii}+1)(D_{jj}+1)} &amp;amp; i\neq j
\end{cases}\]

&lt;h3 id=&quot;c-generalization-to-high-dimensional-feature-vectors&quot;&gt;C. Generalization to high dimensional feature vectors&lt;/h3&gt;

&lt;p&gt;먼저 filter 의 개수가 1개일 때를 생각하겠습니다. 각 node 가 \(C\) 차원의 feature vector 를 가질 때, 이를 signal \(X\in\mathbb{R}^{N\times C}\) 로 표현할 수 있습니다.&lt;/p&gt;

\[X = \begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
x_1 &amp;amp; \cdots &amp;amp; x_C \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix}\]

&lt;p&gt;\(X\) 의 각 column 은 특정 feature 에 대한 signal \(x_{i}\in\mathbb{R}^N\) 입니다. 각 feature 마다 convolutional filter \((6)\) 을 적용해 새로운 feature \(Z\in\mathbb{R}^N\) 를 얻어내는 과정을 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
Z 
&amp;amp;= \sum^{C}_{i=1} \hat{A}x_i\theta_i\\
&amp;amp;= \begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
\hat{A}x_1 &amp;amp; \cdots &amp;amp; \hat{A}x_C \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix}
\begin{bmatrix}
\theta_1 \\
\vdots \\
\theta_C
\end{bmatrix} \\
\\
&amp;amp;= \hat{A}\;
\begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
x_1 &amp;amp; \cdots &amp;amp;x_C \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix}
\begin{bmatrix}
\theta_1 \\
\vdots \\
\theta_C
\end{bmatrix}
= \hat{A}X\Theta
\end{align}\]

&lt;p&gt;이제 Filter 의 개수가 \(F\) 개라면, \(i\) 번째 filter 로 만들어진 새로운 feature \(Z_i = \hat{A}X\Theta_i\) 들에 대해 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\begin{align}
Z 
&amp;amp;= \begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
Z_1 &amp;amp; \cdots &amp;amp; Z_F \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix} 
= \begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
\hat{A}X\Theta_1 &amp;amp; \cdots &amp;amp; \hat{A}X\Theta_F \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix} \\
\\
&amp;amp;= \hat{A}X\begin{bmatrix}
\vert &amp;amp; &amp;amp; \vert \\
\Theta_1 &amp;amp; \cdots &amp;amp;\Theta_F \\
\vert &amp;amp; &amp;amp; \vert
\end{bmatrix}
= \hat{A}X\Theta
\end{align}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. &lt;a href=&quot;https://arxiv.org/pdf/1312.6203.pdf&quot;&gt;Spectral networks and locally
connected networks on graphs&lt;/a&gt;. In International Conference on Learning Representations (ICLR),
2014.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M. Henaff, J. Bruna, and Y. LeCun. &lt;a href=&quot;https://arxiv.org/pdf/1506.05163.pdf&quot;&gt;Deep Convolutional Networks on Graph-Structured Data&lt;/a&gt;.
arXiv:1506.05163, 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. &lt;a href=&quot;https://arxiv.org/pdf/1606.09375.pdf&quot;&gt;Convolutional neural networks on
graphs with fast localized spectral filtering&lt;/a&gt;. In Advances in neural information processing systems
(NIPS), 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thomas N. Kipf and Max Welling. &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;Semi-supervised classification with graph convolutional networks&lt;/a&gt;.
In International Conference on Learning Representations (ICLR), 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;F. R. K. Chung. Spectral Graph Theory, volume 92. American Mathematical Society, 1997.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;paper review&quot;]" /><category term="GCN" /><summary type="html">[paper review] : Graph Convolutional Network 이해하기</summary></entry><entry><title type="html">Weisfeiler-Lehman Algorithm</title><link href="https://harryjo97.github.io/theory/Weisfeiler-Lehman-Algorithm/" rel="alternate" type="text/html" title="Weisfeiler-Lehman Algorithm" /><published>2021-01-12T19:00:00+09:00</published><updated>2021-01-12T19:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Weisfeiler-Lehman-Algorithm</id><content type="html" xml:base="https://harryjo97.github.io/theory/Weisfeiler-Lehman-Algorithm/">&lt;p&gt;Weisfeiler-Lehman Algorithm&lt;/p&gt;

&lt;h2 id=&quot;graph-isomorphism&quot;&gt;Graph Isomorphism&lt;/h2&gt;

&lt;p&gt;주어진 두 그래프 \(G = (V_{G},E_{G})\) 와 \(H=(V_{H}, E_{H})\) 에 대해, 두 그래프가 isomorphic 하다는 것은 다음을 만족하는 bijection \(f:V_{G}\rightarrow V_{H}\) 가 존재한다는 뜻입니다.&lt;/p&gt;

\[u, v \text{ are adjacent in }G \iff f(u), f(v) \text{ are adjacent in }H\]

&lt;p&gt;즉 \(G\) 에서 edge 로 이웃한 모든 node 들의 쌍에 대해, \(H\) 에서 대응되는 각 node 들의 쌍 또한 edge 로 이웃해 있을 때 isomorphic 하다고 표현합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/isomorphism.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;
&lt;p&gt;위의 그림에서 보면, 각 그래프에서 같은 숫자를 가진 node 들끼리 대응 되기 때문에, 두 그래프는 isomorphic 합니다.&lt;/p&gt;

&lt;h2 id=&quot;weisfeiler-lehman-algorithm&quot;&gt;Weisfeiler-Lehman Algorithm&lt;/h2&gt;

&lt;p&gt;주어진 두 그래프가 isomorphic 한지를 확인하는 방법으로 Weisfeiler-Lehman algorithm 이 있습니다. 보통 줄여서 WL 알고리즘 혹은 WL test 라고 부릅니다.&lt;/p&gt;

&lt;p&gt;1차원의 WL 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/algorithm.PNG&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;
&lt;p&gt;1 차원 WL 알고리즘을 통해 regular graph 를 제외한 대부분의 그래프에 대한 node embedding 이 가능합니다.&lt;/p&gt;

&lt;p&gt;주의할 점은 WL 알고리즘의 결과가 다르다면 두 그래프는 확실히 isomorphic 하지 않지만, 결과가 같다고 해서 두 그래프가 isomorphic 하다고는 결론 지을 수 없습니다. Isomorphic 하지 않은 두 그래프의 WL 알고리즘의 결과는 같을 수 있기 때문에, Graph Isomorphism 에 대한 완벽한 해결법이라고는 할 수 없습니다. WL  알고리즘의 반례로는 Reference [3] 을 참고하기 바랍니다.&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;다음의 두 그래프에 대해 WL 알고리즘을 적용해보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-0.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;주어진 두 그래프에 대해 initial node coloring  \(h^{(0)}_{i}=1\) 을 주겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-1.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;각 node 에 대해 이웃한 node 들의 coloring 정보를 모읍니다. 다음과 같이 multi-set 으로 표시하겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-2.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;이 예시에서는 편의상 hash 함수로 identity 함수를 사용하겠습니다. 
다음과 같이 1 번째 iteration 의 coloring \(h^{(1)}_{i}\) 를 계산할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-3.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;다시 각 node 에 대해 이웃한 node 들의 coloring 정보를 모은 후,&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-4.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;2 번째 iteration 의 coloring \(h^{(2)}_i\) 를 계산해 줍니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-5.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;위의 과정을 반복해 3 번째 iteration 의 coloring \(h^{(3)}_i\) 를 계산해 줍니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-6.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Weisfeiler-Lehman-Algorithm/eg-7.png&quot; style=&quot;max-width: 100%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt;3 번째 iteration 의 coloring 으로 인한 node 들의 분할이 2 번째 iteration 의 분할과 동일하므로, 알고리즘을 끝냅니다. 마지막 그림에서 보다시피, 두 그래프에 대해 WL 알고리즘을 통한 node 들의 분할이 일치합니다. 두 그래프는 실제로 isomorphic 하지만, WL 알고리즘의 결과만으로는 판별할 수 없습니다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Brendan L. Douglas. &lt;a href=&quot;https://arxiv.org/pdf/1101.5211.pdf&quot;&gt;The Weisfeiler-Lehman method and graph isomorphism testing&lt;/a&gt;. arXiv preprint
arXiv:1101.5211, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David Bieber. &lt;a href=&quot;https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/&quot;&gt;The Weisfeiler-Lehman Isomorphism Test&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;J. Cai, M. Furer, and N. Immerman. An optimal lower bound on the number of variables for graph identification. Combinatorica, 12(4):389–410, 1992.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="Weisfeiler-Lehman" /><summary type="html">Weisfeiler-Lehman Algorithm</summary></entry><entry><title type="html">Polynomial Approximation Using Chebyshev Expansion</title><link href="https://harryjo97.github.io/theory/Polynomial-Approximation-of-Spectral-Filtering/" rel="alternate" type="text/html" title="Polynomial Approximation Using Chebyshev Expansion" /><published>2021-01-04T22:00:00+09:00</published><updated>2021-01-04T22:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Polynomial-Approximation-of-Spectral-Filtering</id><content type="html" xml:base="https://harryjo97.github.io/theory/Polynomial-Approximation-of-Spectral-Filtering/">&lt;p&gt;Graph Convolutional Network 이해하기 : (5) Polynomial approximation using Chebyshev expansion&lt;/p&gt;

&lt;h2 id=&quot;chebyshev-polynomial&quot;&gt;Chebyshev Polynomial&lt;/h2&gt;

&lt;p&gt;Chebyshev polynomial \(\{T_{k}(x)\}_{k\geq 0}\) 는 다음과 같이 점화식으로 정의됩니다.&lt;/p&gt;

\[T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)
\;\; \text{ with } \;\; T_0(x) = 1 ,\; T_1(x) = x
\tag{1}\]

&lt;p&gt;특히 Chebyshev polynomial \(\{T_{k}(x)\}_{k\geq 0}\) 는 \(L^2\left( [-1,1],\, \frac{dx}{\sqrt{1-x^2}} \right)\) 의 orthogonal basis 를 이루기 때문에 \(h\in L^2\left( [-1,1],\, \frac{dx}{\sqrt{1-x^2}} \right)\) 에 대해, 다음과 같이 uniformly convergent 한 Chebyshev expansion 이 존재합니다.&lt;/p&gt;

\[h(x) = \frac{1}{2}c_0 + \sum^{\infty}_{k=1} c_kT_k(x)
\tag{2}\]

&lt;p&gt;\((2)\) 에서 Chebyshev coefficeint \(c_k\) 는 다음과 같이 계산할 수 있습니다.&lt;/p&gt;

\[c_k = \frac{2}{\pi}\int^1_{-1} \frac{T_k(x)h(x)}{\sqrt{1-x^2}}dx
\tag{$\ast$}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;truncated-chebyshev-expansion&quot;&gt;Truncated Chebyshev Expansion&lt;/h2&gt;

&lt;p&gt;Filter \(g_{\theta}\) 에 대한 spectral convolution 의 결과 \(f_{out}\) 은 다음과 같습니다.&lt;/p&gt;

\[f_{out} 
= Ug_{\theta}(\Lambda)U^T\;f_{in}
\tag{3}\]

&lt;p&gt;\((3)\) 을 계산하기 위해서는 Fourier basis \(U\) 가 필요하기 때문에, graph Laplacian \(L\) 의 eigenvector 를 모두 찾아야 합니다. \(N\) 개의 node 를 가지는 그래프에 대해서, QR decomposition 과 같은 eigenvalue decomposition 의 computational complexity 의 \(O(N^3)\) 입니다. 즉 node 가 수천개 혹은 수만개 이상인 그래프에 대해서 직접 \((3)\) 을 계산하는 것은 현실적으로 불가능합니다.&lt;/p&gt;

&lt;p&gt;따라서, 그래프의 크기가 큰 경우에는 \((3)\) 을 근사할 수 있는 효율적인 방법이 필요합니다. 만약 \(g_{\theta}\) 가 order \(K\) polynomial \(\sum^K_{k=0} a_k x^k\) 이라면, \((3)\) 을 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

\[\begin{align}
f_{out}
&amp;amp;= U\left(\sum^{N-1}_{k=0} a_k\Lambda^k \right)U^T f_{in} \\
&amp;amp;= \sum^{K}_{k=0} a_k\left( U\Lambda U^T \right)^k f_{in} 
= g_{\theta}(L)f_{in}
\tag{4}
\end{align}\]

&lt;p&gt;\((4)\) 에서 볼 수 있듯이, Fourier basis \(U\)  없이도 \((3)\) 의 결과를 얻을 수 있습니다. 만약 \(g_{\theta}\) 에 대한 polynomial approximant \(p\) 를 찾을 수 있다면, \(p\) 를 사용해 \((4)\) 와 같이 \((3)\) 을 효율적으로 근사할 수 있습니다.&lt;/p&gt;

&lt;p&gt;만약 \(p\) 가 \(L\) 의 spectrum 에 대한 upper bound \(\lambda_{max}\) 에 대해 다음의 조건을 만족한다면,&lt;/p&gt;

\[\left\vert g_{\theta}(x) - p(x) \right\vert \leq B &amp;lt; \infty
\;\;\text{ for all }\;\; x\in [0,\lambda_{max}]
\tag{5}\]

&lt;p&gt;Polynomial \(p(L)\) 과의 spectral convolution \(\tilde{f}_{out} = p(L)f_{in}\) 을 통해, 다음과 같이 \((3)\) 을 근사할 수 있습니다 [3].&lt;/p&gt;

\[\begin{align}
\vert f_{out}(i) - \tilde{f}_{out}(i) \vert 
&amp;amp;= \left\vert \sum_{l} g_{\theta}(\lambda_l)\hat{f}(\lambda_l)u_l(i) - \sum_{l} p(\lambda_l)\hat{f}(\lambda_l)u_l(i) \right\vert \\
\\
&amp;amp;\leq \sum_{l} \vert g_{\theta}(\lambda_l) - p(\lambda_l) \vert \left\vert \hat{f}(\lambda_l)u_l(i) \right\vert \\
&amp;amp;\leq B \left( \sum_l \left\vert \hat{f}(\lambda_l) \right\vert^2\sum_l \vert u_l(i) \vert^2 \right)^{1/2} 
= B\;\|f\| 
\tag{6}
\end{align}\]

&lt;p&gt;이 때 \(f_{out}\) 과 \(\tilde{f}_{out}\) 에 대한 오차 \((6)\) 을 줄이기 위해서는, \(g_{\theta}\) 와 \(p\) 에 대한 \(L_{\infty}\) error \((5)\) 를  최소화해야 합니다. 만약  \(p\) 가 order \(K\) polynomial 이라면, \((5)\) 는 \(p\) 가 minimax polynomial of order \(K\) 일 때 최소가 됩니다. 더 나아가, minimax polynomial 은 truncated Chebyshev expansion 을 통해 충분히 근사할 수 있습니다.&lt;/p&gt;

&lt;p&gt;따라서, \((6)\) 의 오차를 줄이기 위해 \(p\) 로 \(g_{\theta}\) 의 truncated Chebyshev expansion 을 선택할 수 있습니다. 하지만  \(g_{\theta}\) 는 \(L\) 의 spectrum 을 포함하는 domain 에서 정의된 함수이기 때문에, \((2)\) 를 적용하기 위해서는 domain 의 변환이 필요합니다. \(L\) 의 eigenvalue 들은 모두 \([0, \lambda_{max}]\) 구간에 속하기 때문에 \(h_{\theta}\) 를 다음과 같이 정의하면 \(g_{\theta}\) 를 \([-1,1]\) 에서 정의된 함수로 바꿀 수 있습니다.&lt;/p&gt;

\[h_{\theta}(x) = g_{\theta}\left( \frac{\lambda_{\max}}{2}(x+1) \right)\]

&lt;p&gt;\((2)\) 를 \(h_{\theta}\) 에 적용하고 order \(K\) 까지의 truncation 을 생각하면, 다음과 같이 \(h_{\theta}\) 를 근사할 수 있습니다.&lt;/p&gt;

\[h_{\theta}(x) \approx \frac{1}{2}c_0 + \sum^{K}_{k=1} c_kT_k(x)\]

&lt;p&gt;\(\tilde{L} = \frac{2}{\lambda_{max}}L - I\) 에 대해 \(g_{\theta}(L) = h_{\theta}(\tilde{L})\) 를 만족하기 때문에, \(p\) 를 다음과 같이 정의합니다.&lt;/p&gt;

\[p(\tilde{L}) = h_{\theta}(\tilde{L}) =  \frac{1}{2}c_0I + \sum^{\infty}_{k=1} c_kT_k(\tilde{L})
\tag{7}\]

&lt;p&gt;\((7)\) 을 사용하면, Fourier basis \(U\) 를 이용하지 않고도 \((3)\) 에 대한 근사가 가능합니다.&lt;/p&gt;

\[\begin{align}
&amp;amp; Ug_{\theta}(\Lambda)U^T 
= Uh_{\theta}(\tilde{\Lambda})U^T 
\approx Up(\tilde{\Lambda})U^T = p(\tilde{L}) \\
\\
&amp;amp; f_{out} = Ug_{\theta}(\Lambda)U^Tf_{in} \approx p(\tilde{L})f_{in}
\tag{8}
\end{align}\]

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;마지막으로, 두 가지 확인해야할 것이 있습니다. 첫번 째로, \(\tilde{L}\) 을 계산하기 위해서 \(\lambda_{max}\) 에 대한 정보가 필요합니다. Spectrum 의 upper bound \(\lambda_{max}\) 는 Arnoldi iteration 혹은 Jacobi-Davidson method 등을 사용하면 \(L\) 의 전체 spectrum 을 찾는 것에 비해서 훨씬 쉽게 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;두번 째로, \(p(\tilde{L})\) 을 계산하기 위해서는 Chebyshev coefficient \(c_k\) 에 대해 알아야합니다. 이는 \((\ast)\) 를 통해 이론적으로 계산할 수 있지만, 현실적으로 도움이 되지 않습니다. 여기서, neural network 가 등장합니다. Universal approximation theorem 에 의해 \((7)\) 을 근사할 수 있는 neural network 가 존재합니다. 따라서, coefficient \(c_k\) 를 parameter 로 학습하는 neural network 가 바로 ChebNet 입니다 [1].&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;advantage-of-using-chebyshev-expansion&quot;&gt;Advantage of Using Chebyshev Expansion&lt;/h2&gt;

&lt;p&gt;\((8)\) 과 같이 truncated Chebyshev expansion 을 통한 근사는 다음과 같은 두 가지 이점이 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;fast-filtering-using-recurrence-relation&quot;&gt;Fast filtering using recurrence relation&lt;/h3&gt;

&lt;p&gt;Chebyshev polynomial 의 중요한 특성은 \((1)\) 의 점화식을 통해 재귀적으로 계사할 수 있다는 것입니다. Graph Laplacian \(L\) 에서부터 시작해 재귀적 연산으로 order \(K\) polynomial \(T_K\) 까지 구하는 computational cost 는 \(L\) 이 sparse matrix 일 때 \(O(K\vert E\vert)\) 입니다.&lt;/p&gt;

&lt;h3 id=&quot;localized-filter&quot;&gt;Localized Filter&lt;/h3&gt;

&lt;p&gt;\((8)\) 의 결과 \(p(\tilde{L})f_{in}\) 은 각 vertex \(i\) 에 대해 \(i\) 의 \(K\)- hop local neighborhood 만을 이용해 표현할 수 있습니다. 이를 통해 CNN 의 중요한 특성인 locality 가 그래프에서 일반화될 수 있습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;lanczos-algorithm&quot;&gt;Lanczos Algorithm&lt;/h2&gt;

&lt;p&gt;\((3)\) 을 효율적으로 계산하는 다른 해결 방법으로는 Lanczos Algorithm 이 있습니다. &lt;a href=&quot;https://arxiv.org/pdf/1901.01484.pdf&quot;&gt;LanczosNet: Multi-Scale Deep Graph Convolutional Networks&lt;/a&gt; 의 paper review 를 통해 더 자세히 설명하겠습니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. &lt;a href=&quot;https://arxiv.org/pdf/1606.09375.pdf&quot;&gt;Convolutional neural networks on
graphs with fast localized spectral filtering&lt;/a&gt;. In Advances in neural information processing systems
(NIPS), 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thomas N. Kipf and Max Welling. &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;Semi-supervised classification with graph convolutional networks&lt;/a&gt;.
In International Conference on Learning Representations (ICLR), 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="GCN" /><summary type="html">Graph Convolutional Network 이해하기 : (5) Polynomial approximation using Chebyshev expansion</summary></entry><entry><title type="html">Localized Polynomial Filter</title><link href="https://harryjo97.github.io/theory/Localized-Polynomial-Filter/" rel="alternate" type="text/html" title="Localized Polynomial Filter" /><published>2021-01-04T20:00:00+09:00</published><updated>2021-01-04T20:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Localized-Polynomial-Filter</id><content type="html" xml:base="https://harryjo97.github.io/theory/Localized-Polynomial-Filter/">&lt;p&gt;Graph Convolutional Network 이해하기 : (4) Localized Polynomial Filter&lt;/p&gt;

&lt;h2 id=&quot;localization-of-graph-laplacian&quot;&gt;Localization of Graph Laplacian&lt;/h2&gt;

&lt;p&gt;그래프 \(G\) 의 vertex \(i\) 와 \(j\) 에 대해, 두 vertex 사이의 거리 \(d_G(i,j)\) 를 \(i\) 와 \(j\) 를 연결하는 모든 path 들 중 edge 들의 수가 가장 적은 path 의 길이로 정의합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(Lemma 1)&lt;/strong&gt;  그래프 \(G\) 의 adjacency matrix \(A\) 에 대해 \(\tilde{A}\) 를 다음과 같다면,&lt;/p&gt;

\[\tilde{A} = \begin{cases}
A_{ij} &amp;amp;\mbox{ if } i\neq j \\
1 &amp;amp;\mbox{ if } i=j
\end{cases}\]

&lt;p&gt;임의의 양의 정수 \(s\) 에 대해, \(\left( \tilde{A}^s \right)_{ij}\) 은 vertex \(i\) 와 \(j\) 를 연결하는 path 들 중 길이가  \(s\) 이하인 path 들의 수와 일치합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(Lemma 2)&lt;/strong&gt;  \(N\times N\) matrix \(A\) , \(B\) 와 모든 \(1\leq m,n\leq N\) 에 대해, \(B_{mn}=0\) 이면 \(A_{mn}=0\) 을 만족한다면, 임의의 양의 정수 \(s\) 에 대해서도 \(\left( B^s \right)_{mn}=0\) 이면  \(\left( A^s \right)_{mn}=0\) 이 성립합니다.&lt;/p&gt;

&lt;p&gt;위의 두 lemma 를 사용하면, graph Laplacian \(L\) 의 localization 을 보일 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(Localization of graph Laplacian)&lt;/strong&gt;  그래프 \(G\) 의 vertex \(i, \;j\) 와 \(d_G(i,j)\)  보다 작은 모든 \(s\) 에 대해 다음이 성립합니다.&lt;/p&gt;

\[\left(L^s\right)_{ij} = 0
\tag{1}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;localized-polynomial-filter&quot;&gt;Localized Polynomial Filter&lt;/h2&gt;

&lt;p&gt;Filter \(g_{\theta}\) 에 대한 spectral convolution 의 결과 \(f_{out}\) 은 다음과 같습니다.&lt;/p&gt;

\[f_{out} = Ug_{\theta}(\Lambda)U^T\;f_{in} 
\tag{2}\]

&lt;p&gt;만약 filter \(g_{\theta}\) 가 order \(K\) polynomial \(\sum^K_{k=0} a_k x^k\) 이라면, \((2)\) 를 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\begin{align}
f_{out}
&amp;amp;= U\left(\sum^{N-1}_{k=0} a_k\Lambda^k \right)U^T f_{in} \\
&amp;amp;= \sum^{K}_{k=0} a_k\left( U\Lambda U^T \right)^k f_{in} 
= g_{\theta}(L)f_{in}
\tag{3}
\end{align}\]

&lt;p&gt;\((3)\) 에서 볼 수 있듯이, Fourier basis \(U\) 를 직접 계산하지 않고도 \((2)\) 의 결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;특히 vertex \(i\) 에 대해서만 자세히 살펴보겠습니다.&lt;/p&gt;

\[f_{out}(i) 
= (g_{\theta}(L) f_{in})(i) 
= \sum^{K}_{k=0}\sum^{N}_{j=1} a_{k} \left(L^k\right)_{ij} f_{in}(j) 
\tag{4}\]

&lt;p&gt;Vertex \(i\) 로부터 거리가 \(K\) 이하인 vertex 들의 집합을 \(N(i,K)\) 라고 하고, 이를 \(i\) 의 \(K\)- hop local neighborhood 라고 부르겠습니다. 만약 vertex \(j\) 가 \(N(i,K)\) 의 원소가 아니라면, \((1)\) 에 의해 \((L^k)_{ij}=0\) 입니다.&lt;/p&gt;

&lt;p&gt;따라서, \((4)\) 를 정리하면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
f_{out}(i) 
&amp;amp;= \sum^N_{j=1}\sum^K_{k=0} a_k(L^k)_{ij}f_{in}(j) \\
&amp;amp;= \sum_{j\in N(i,K)}\left[\sum^K_{k=0} a_k(L^k)_{ij}\right]f_{in}(j) \\
&amp;amp;= \sum_{j\in N(i,K)} b_{ij}f_{in}(j)
\tag{5}
\end{align}\]

&lt;p&gt;결국 \(f_{out}(i)\) 는 \(i\) 의 \(K\)- hop local neighborhood 원소들만을 이용해서 표현할 수 있습니다. 즉 \(f_{out}(i)\) 를 계산하기 위해 모든 vertex 들의 정보를 사용하지 않아도 된다는 뜻입니다. CNN 의 convolutional fiter 가 각 픽셀을 중심으로 주변의 픽셀 값만을 사용하는 것과 같은 맥락입니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;D. Shuman, S. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. &lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6494675&quot;&gt;The Emerging Field of Signal
Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and other Irregular Domains&lt;/a&gt;. &lt;em&gt;IEEE Signal Processing Magazine&lt;/em&gt;, 30(3):83–98, 2013.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="GCN" /><summary type="html">Graph Convolutional Network 이해하기 : (4) Localized Polynomial Filter</summary></entry><entry><title type="html">Graph Convolution and Spectral Filtering</title><link href="https://harryjo97.github.io/theory/Graph-Convoloution-and-Spectral-Filtering/" rel="alternate" type="text/html" title="Graph Convolution and Spectral Filtering" /><published>2021-01-03T17:00:00+09:00</published><updated>2021-01-03T17:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Graph-Convoloution-and-Spectral-Filtering</id><content type="html" xml:base="https://harryjo97.github.io/theory/Graph-Convoloution-and-Spectral-Filtering/">&lt;p&gt;Graph Convolutional Network 이해하기 :  (3) Graph convolution 과 spectral filtering&lt;/p&gt;

&lt;h2 id=&quot;why-do-we-need-graph-convolution&quot;&gt;Why do we need Graph Convolution?&lt;/h2&gt;

&lt;p&gt;Fourier transform 을 통해 그래프에서 convolution 을 정의한 이유는, 바로 CNN 을 그래프에 적용하기 위해서입니다. CNN 은 ML 의 여러 분야에서 뛰어난 성과를 거두었습니다. 특히, CNN 은 large-scale high dimensional 데이터로부터 local structure 를 학습하여 의미있는 패턴을 잘 찾아냅니다. 이 때 local feature 들은 convolutional filter 로 표현되며, filter 는 translation-invariant 이기 때문에 공간적인 위치나 데이터의 크기에 상관없이 같은 feature 를 뽑아낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;하지만, 그래프와 같이 irregular (non-Euclidean) domain 에서는 직접 convolution 을 정의할 수 없습니다. 기존의 convolution 의 정의는 discrete 한 그래프에서는 의미를 갖지 못합니다. 따라서, Graph Convolutional Network 를 위해서는 그래프에서 정의되는 convolution operator 가 새로 필요합니다.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;graph-convolution&quot;&gt;Graph Convolution&lt;/h2&gt;

&lt;p&gt;Vertex domain 에서 직접 convolution operator 를 정의할 수 없기 때문에, Fourier transform 을 이용하여 Fourier domain 에서 convolution operator 를 정의합니다.&lt;/p&gt;

&lt;p&gt;기존의 convolution 과 같이 graph convolution 또한 Fourier transform 에 대해 다음의 조건을 만족해야 합니다. (Convolution theorem).&lt;/p&gt;

\[\widehat{g\ast f}(l) = \hat{g}(l)\hat{f}(l)
\tag{1}\]

&lt;p&gt;즉 vertex  domain 에서의 convolution 과 Fourier domain 에서의 multiplication 이 일치하도록 만들고 싶습니다. \((1)\) 에 대해 inverse Fourier transform 을 적용하면, 다음의 결과를 얻게 됩니다.&lt;/p&gt;

\[g\ast f = \sum^{N-1}_{l=0} \hat{g}(l) \hat{f}(l)u_l
\tag{2}\]

&lt;p&gt;따라서, vertex domain 에서 정의된 두 graph signal \(f\) 와 \(g\) 에 대해 convolution operator \(\ast\) 는 \((2)\) 과 같이 정의합니다. 이는 기존의 convolution 에서 complex exponential \(\left\{e^{2\pi i\xi t}\right\}_{\xi\in\mathbb{R}}\) 대신 graph Laplacian eigenvector \(\{u_l\}^{N-1}_{l=0}\)  을 사용했다고 이해할 수 있습니다. \((2)\) 는 Hadamard product \(\odot\) 와  \(\{u_l\}^{N-1}_{l=0}\) 을 column vector 로 가지는 Fourier basis \(U\) 를 사용해, 다음과 같은 형태로 표현할 수 있습니다.&lt;/p&gt;

\[g \ast f = U((U^Tg) \odot (U^Tf))
\tag{3}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;spectral-filtering-of-graph-signal&quot;&gt;Spectral Filtering of Graph Signal&lt;/h2&gt;

&lt;p&gt;위에서 정의한 graph convolution 을 사용해, 다음과 같이 graph signal \(f_{in}\) 의 \(g\) 에 대한 filtering 을 정의할 수 있습니다.&lt;/p&gt;

\[f_{out} = g\ast f_{in}\]

&lt;p&gt;\((1)\) 을 사용하면 filtering 은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}

f_{out} 
&amp;amp;= \sum^{N-1}_{l=0} \hat{f}_{out}(l)u_l \\
&amp;amp;= 
\begin{bmatrix}
\big| &amp;amp; \big| &amp;amp;  &amp;amp; \big| \\
u_0 &amp;amp; u_1 &amp;amp; \cdots &amp;amp; u_{N-1} \\
\big| &amp;amp; \big| &amp;amp;  &amp;amp; \big|
\end{bmatrix}
\begin{bmatrix}
\hat{f}_{out}(0) \\
\vdots \\
\hat{f}_{out}({N-1})
\end{bmatrix} \\
\\
&amp;amp;= U
\begin{bmatrix}
\hat{g}(0)\hat{f}_{in}(0) \\
\vdots \\
\hat{g}({N-1})\hat{f}_{in}({N-1})
\end{bmatrix} \\
\\
&amp;amp;= U
\begin{bmatrix}
\hat{g}(0) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; \hat{g}(1) &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp;  &amp;amp; \ddots &amp;amp; \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \hat{g}({N-1})
\end{bmatrix}
\begin{bmatrix}
\hat{f}_{in}(0) \\
\vdots \\
\hat{f}_{in}({N-1})
\end{bmatrix} \\
\\
&amp;amp;= U\,\text{diag}(\hat{g})\,U^T f_{in}

\tag{4}

\end{align}\]

&lt;p&gt;\((4)\) 를 통해 convolution operator 는 Fourier domain 에서 diagonalize 되는 operator 로 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Reference 의 [1, 2] 에서는 \(\text{diag}(\hat{g})\) 를 함수가 아닌, filter 의 parameter 로 해석했습니다. 이를 spectral construction 이라 부르며, vertex domain 에서 localized filter 를 사용하는spatial construction 에 비해 parameter 의 수를 \(N^2\) 에서 \(N\) 으로 줄였다는데 의의가 있습니다. 하지만, Fourier basis \(U\) 를 사용하기 위해서는 computational cost 가 높은 eigenvalue decomposition 을 수행해야 하기 때문에, 효율적인 방법이 아닙니다.&lt;/p&gt;

&lt;p&gt;이런 문제를 해결하기 위해 [3] 에서는 \(g_{\theta}\) 로 \(L\) 의 eigenvalue 들에 대한 polynomial 을 사용했습니다. 이 경우,  \(L = U\Lambda U^T\) 를 만족하기 때문에, \(U\) 대신 \(L\) 로써 \((5)\) 를 표현할 수 있고, eigen decomposition 을 하지 않아도 되기 때문에 굉장히 효율적입니다.&lt;/p&gt;

&lt;p&gt;따라서 GCN 에서의 spectral convolution 은 \(\hat{g}\) 을 \(L\) 의 eigenvalue 에 대한 함수 \(g_{\theta}\) 로 생각하고, \(\text{diag}(\hat{g})\) 대신 다음의 \(g_\theta(\Lambda)\) 를 사용하여 \((4)\) 를 표현합니다.&lt;/p&gt;

\[g_{\theta}(\Lambda) =
\begin{bmatrix}
g_{\theta}(\lambda_0) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; g_{\theta}(\lambda_1) &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp;  &amp;amp; \ddots &amp;amp; \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; g_{\theta}(\lambda_{N-1})
\end{bmatrix}\]

&lt;p&gt;따라서, \(g_{\theta}\) 에 대한 filtering 의 결과는 다음과 같습니다.&lt;/p&gt;

\[f_{out} = Ug_{\theta}(\Lambda)U^T f_{in}
\tag{5}\]

&lt;p&gt;\((5)\) 의 spectral filtering 은 다음과 같이 Fourier domain 에서 \(g_{\theta}\) 에 대한 filtering 으로 이해할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Convolution-and-Spectral-Filtering/filtering.PNG&quot; style=&quot;max-width: 80%; height: auto&quot; /&gt;	
&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. &lt;a href=&quot;https://arxiv.org/pdf/1312.6203.pdf&quot;&gt;Spectral networks and locally
connected networks on graphs&lt;/a&gt;. In International Conference on Learning Representations (ICLR),
2014.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M. Henaff, J. Bruna, and Y. LeCun. &lt;a href=&quot;https://arxiv.org/pdf/1506.05163.pdf&quot;&gt;Deep Convolutional Networks on Graph-Structured Data&lt;/a&gt;.
arXiv:1506.05163, 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. &lt;a href=&quot;https://arxiv.org/pdf/1606.09375.pdf&quot;&gt;Convolutional neural networks on
graphs with fast localized spectral filtering&lt;/a&gt;. In Advances in neural information processing systems
(NIPS), 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thomas N. Kipf and Max Welling. &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;Semi-supervised classification with graph convolutional networks&lt;/a&gt;.
In International Conference on Learning Representations (ICLR), 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;David K Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory&lt;/a&gt;. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="GCN" /><summary type="html">Graph Convolutional Network 이해하기 : (3) Graph convolution 과 spectral filtering</summary></entry><entry><title type="html">Graph Fourier Transform</title><link href="https://harryjo97.github.io/theory/Graph-Fourier-Transform/" rel="alternate" type="text/html" title="Graph Fourier Transform" /><published>2021-01-01T19:00:00+09:00</published><updated>2021-01-01T19:00:00+09:00</updated><id>https://harryjo97.github.io/theory/Graph-Fourier-Transform</id><content type="html" xml:base="https://harryjo97.github.io/theory/Graph-Fourier-Transform/">&lt;p&gt;Graph Convolutional Network 이해하기 : (2) Graph Fourier transform&lt;/p&gt;

&lt;h2 id=&quot;classical-fourier-transform&quot;&gt;Classical Fourier Transform&lt;/h2&gt;

&lt;h3 id=&quot;fourier-transform&quot;&gt;Fourier transform&lt;/h3&gt;

&lt;p&gt;Integrable function \(f : \mathbb{R} \rightarrow \mathbb{C}\) 의 Fourier transform 은 다음과 같이 정의합니다.&lt;/p&gt;

\[\hat{f}(\xi) = \langle f, e^{2\pi i\xi t} \rangle = \int_{\mathbb{R}} f(t) e^{-2\pi i\xi t}dt \tag{$1$}\]

&lt;p&gt;즉 \(\hat{f}(\xi)\) 은 \(f(t)\) 의 \(e^{2\pi i \xi t}\) 성분 (frequency) 의 크기 (amplitude) 를 의미합니다. \((1)\) 을 살펴보면, Fourier transform 은 time domain \(t\in\mathbb{R}\) 에서 정의된 함수 \(f(t)\) 를 frequency domain \(\xi\in\mathbb{C}\) 에서 정의된 함수 \(\hat{f}(\xi)\) 로 변환시켜준다는 것을 알 수 있습니다. 다음의 그림을 통해 time domain 으로부터 frequency domain 으로의 Fourier transform 을 이해할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/post/Graph-Fourier-Transform/fourier.jpg&quot; style=&quot;max-width: 80%; height: auto&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;inverse-fourier-transform&quot;&gt;Inverse Fourier transform&lt;/h3&gt;

&lt;p&gt;\((1)\) 에서 정의된 Fourier transform 의 역과정인 inverse Fourier transform 은 다음과 같습니다.&lt;/p&gt;

\[f(x) = \int_{\mathbb{R}} \hat{f}(\xi)e^{2\pi i\xi t}d\xi \tag{$2$}\]

&lt;p&gt;Invere Fourier transform 은 Fourier transform 과 반대로 frequency domain 에서 정의된 함수 \(\hat{f}\) 을 time domain 에서의 함수 \(f\) 로 변환시켜줍니다. \((2)\) 는 주어진 \(\hat{f}\) 으로부터 원래의 함수 \(f\) 를 복원하는 과정이며, 각 \(e^{2\pi i \xi t}\) 성분의 amplitude \(\hat{f}(\xi)\) 이 주어졌을 때 원래의 함수 \(f\) 를 각 성분들의 합으로 표현하는 변환입니다.&lt;/p&gt;

&lt;h3 id=&quot;laplacian-operator&quot;&gt;Laplacian operator&lt;/h3&gt;

&lt;p&gt;\((1)\) 과 \((2)\) 에서 등장하는 complex exponentials \(\left\{ e^{2\pi i \xi t} \right\}_{\xi\in\mathbb{C}}\) 는 1 차원 Laplacian operator \(\Delta\) 의 eigenfunction 입니다.&lt;/p&gt;

\[\Delta(e^{2\pi i \xi t}) = \frac{\partial^2}{\partial t^2}e^{2\pi i \xi t} 
= -(2\pi\xi)^2 e^{2\pi i \xi t}\]

&lt;p&gt;즉 Fourier transform 은 Laplacian operator \(\Delta\) 의 eigenfunction 들의 합으로 분해하는 변환으로 생각 할 수 있습니다. [3]&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;graph-fourier-transform&quot;&gt;Graph Fourier Transform&lt;/h2&gt;

&lt;h3 id=&quot;graph-fourier-transform-1&quot;&gt;Graph Fourier transform&lt;/h3&gt;

&lt;p&gt;그래프에서의 graph Laplacian \(L\) 은 Euclidean space 의 Laplacian operator \(\Delta\) 와 같은 역할을 합니다. 특히, 1 차원 Laplacian operator \(\Delta\) 에 대해 complex exponential 들은 eigenfunction 이며, 그래프에서는 \(L\) 의 eigenvector 들이 그 역할을 대신합니다.&lt;/p&gt;

&lt;p&gt;이 때 graph Laplacian 의 eigenvector 들로 \(\mathbb{R}^N\) 의 orthonormal basis 를 구성할 수 있기 때문에, orthonormal eigenvector \(\left\{ u_l \right\}^{N-1}_{l=0}\) 을 사용해 \((2)\) 와 같이 graph Fourier transform 을 정의할 수 있습니다. Euclidean space 에서 compex exponential 들이 Fourier basis 를 이루듯이, 그래프에서는 eigenvector \(\left\{ u_l \right\}^{N-1}_{l=0}\) 들이 Fourier basis 가 됩니다.&lt;/p&gt;

&lt;p&gt;Graph signal \(f \in\mathbb{R}^{N}\) 에 대한 Fourier transform 은  \(\left\{ u_l \right\}^{N-1}_{l=0}\) 성분들의 합으로 분해하는 변환으로 이해할 수 있습니다.  \(u_l\) 성분의 크기 (amplitude) 는 inner product 를 사용해 다음과 같이 계산할 수 있습니다.&lt;/p&gt;

\[\hat{f}(l) = \langle u_l, f\rangle =  \sum^N_{i=1} u^{T}_l(i)f(i) 
\tag{3}\]

&lt;p&gt;\(\hat{f}\) 을 다음과 같이 \(\mathbb{R}^N\) 의 vector 로 생각하겠습니다.&lt;/p&gt;

\[\hat{f} 
= \begin{bmatrix}
\hat{f}(0) \\
\vdots \\
\hat{f}({N-1})
\end{bmatrix}\]

&lt;p&gt;\(L\) 의 eigenvector \(\left\{ u_l \right\}^{N-1}_{l=0}\) 를 column 으로 가지는 행렬 \(U\) 에 대해&lt;/p&gt;

\[U = \begin{bmatrix}
\bigg| &amp;amp; \bigg| &amp;amp; &amp;amp; \bigg| \\
u_0 &amp;amp; u_1 &amp;amp; \cdots &amp;amp; u_{N-1} \\
\bigg| &amp;amp; \bigg| &amp;amp; &amp;amp; \bigg|
\end{bmatrix}\]

&lt;p&gt;\((3)\) 를 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\hat{f} 
= \begin{bmatrix}
\hat{f}(0) \\
\vdots \\
\hat{f}({N-1})
\end{bmatrix}
= \begin{bmatrix}
- &amp;amp; u_0^{T} &amp;amp; - \\

&amp;amp; \vdots &amp;amp; \\
- &amp;amp; u_{N-1}^{T} &amp;amp; -
\end{bmatrix}
\begin{bmatrix}
f(1) \\
\vdots \\
f(N)
\end{bmatrix}
= U^{T}f\]

&lt;p&gt;따라서, graph signal \(f\) 에 대한 graph Fourier transform 은 Fourier basis \(U\) 를 사용해 다음과 같이 쓸 수 있습니다.
\(\hat{f} = U^T f
\tag{4}\)&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;inverse-graph-fourier-transform&quot;&gt;Inverse graph Fourier transform&lt;/h3&gt;

&lt;p&gt;Inverse Fourier transform 과 마찬가지로, inverse graph Fourier transform 은 graph Fourier transform 의 역과정입니다. 그렇기 때문에, \((5)\) 에서 정의된 graph Fourier transform 을 되돌리는 과정은 다음과 같습니다.&lt;/p&gt;

\[f \overset{\mathrm{(a)}}{=} UU^Tf = U\hat{f}
\tag{5}\]

&lt;p&gt;위의 식에서 Fourier basis 가 orthonormal 하기 때문에 \(UU^T = I\) 이므로 \((a)\) 가 성립합니다.&lt;/p&gt;

&lt;p&gt;\((6)\) 의 결과는 \(\hat{f}\) 의 의미를 통해서 유도할 수 있습니다. \(\left\{ u_l \right\}^{N-1}_{l=0}\) 은 \(\mathbb{R}^N\) 의 orthonormal basis 를 이루기 때문에, Fourier transform 의 결과인 \(\hat{f}\) 으로 부터 원래의 \(f\) 를 얻어내기 위해서는 각 성분들을 모두 더해주면 됩니다. 이 때, \(f\) 의 \(u_l\) 성분의 크기는 \(\hat{f}(l)\) 이기 때문에, 다음의 등식이 성립합니다.&lt;/p&gt;

\[f(i) = \sum^{N-1}_{l=0} \hat{f}(l)u_l(i) 
\tag{6}\]

&lt;p&gt;\((6)\) 를 \(\mathbb{R}^N\) 의 vector 로 표현하면, \((5)\) 의 결과와 일치합니다.&lt;/p&gt;

\[f 
= \begin{bmatrix}
f(1) \\
\vdots \\
f(N)
\end{bmatrix}
= \begin{bmatrix}
\bigg| &amp;amp; \bigg| &amp;amp; &amp;amp; \bigg| \\
u_0 &amp;amp; u_1 &amp;amp; \cdots &amp;amp; u_{N-1} \\
\bigg| &amp;amp; \bigg| &amp;amp; &amp;amp; \bigg|
\end{bmatrix} 
\begin{bmatrix}
\hat{f}(0) \\
\vdots \\
\hat{f}({N-1})
\end{bmatrix}
= U\hat{f}\]

&lt;h3 id=&quot;parseval-relation&quot;&gt;Parseval relation&lt;/h3&gt;

&lt;p&gt;Classical Fourier transform 에서와 마찬가지로, graph Fourier transform 은 Parseval relation 을 만족합니다.&lt;/p&gt;

\[\begin{align}
	\langle f, g\rangle 
	&amp;amp;= \langle \sum^{N-1}_{l=0}\hat{f}(l)u_l, \sum^{N-1}_{l'=0}\hat{g}({l'})u_{l'} \rangle \\
	&amp;amp;= \sum_{l, l'} \hat{f}(l)\hat{g}({l'})\langle u_l, u_{l'} \rangle \\
	&amp;amp;= \sum_{l} \hat{f}(l)\hat{g}(l) = \langle \hat{f}, \hat{g} \rangle
\end{align}\]

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;David K. Hammond, Pierre Vandergheynst, and Remi Gribonval. &lt;a href=&quot;https://arxiv.org/pdf/0912.3848.pdf&quot;&gt;Wavelets on graphs via spectral
graph theory. Applied and Computational Harmonic Analysis&lt;/a&gt;, 30(2):129–150, 2011.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. &lt;a href=&quot;https://arxiv.org/pdf/1606.09375.pdf&quot;&gt;Convolutional neural networks on
graphs with fast localized spectral filtering&lt;/a&gt;. In Advances in neural information processing systems
(NIPS), 2016.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Terence Tao, &lt;a href=&quot;https://www.math.ucla.edu/~tao/preprints/fourier.pdf&quot;&gt;Fourier Trasnform&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jaehyeong Jo</name><email>harryjo97@kaist.ac.kr</email></author><category term="[&quot;theory&quot;]" /><category term="GCN" /><summary type="html">Graph Convolutional Network 이해하기 : (2) Graph Fourier transform</summary></entry></feed>